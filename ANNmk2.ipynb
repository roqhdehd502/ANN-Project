{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동차 이미지들을 학습 및 테스트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터는 각 240개씩 총 960개로 구성되어있고,\n",
    "시험 데이터는 각 60개씩 총 240개로 구성되어있습니다.\n",
    "\n",
    "차종은 세단, 스포츠카, SUV, 픽업트럭으로 네가지를 구분하였고 종류별 숫자 코드는 다음과 같습니다.\n",
    "0 = sedan, 1 = sportscar, 2 = suv, 3 = truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Namin Neural Network\n",
    "    -----------------\n",
    "    conv1 - relu1 - pool1 - bn1\n",
    "    conv2 - relu2 - pool2 - bn2\n",
    "    conv3 - relu3 - pool3 - bn3\n",
    "    affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Input Layer(Input Size): 100*100*3\n",
    "    First Layer: Conv1(3EA, 3*3*3, Strides=3, Padding=1) - ReLU1\n",
    "                - 34*34*3 - Pool1(2*2, Strides=1) - 33*33*3 - Bn1\n",
    "    Second Layer: Conv2(6EA, 6*6*3, Strides=1, Padding=VALID) - ReLU2\n",
    "                - 28*28*6 - Pool2(2*2, Strides=2) - 14*14*6 - Bn2\n",
    "    Third Layer: Conv3(9EA, 3*3*6, Strides=3, Padding=2) - ReLU3\n",
    "                - 6*6*9 - Pool3(2*2, Strides=2) - 3*3*9 - Bn3\n",
    "    Output Layer: Affine(W=3*3*9, B=9) - Output Nodes = 4(Sedan, Coupe, SUV, PickupTruck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to Sedan, Coupe, SUV, PickupTruck Images\n",
    "trainlist, testlist = [], []\n",
    "with open('train.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        trainlist.append([tmp[0], tmp[1]])\n",
    "        \n",
    "with open('test.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        testlist.append([tmp[0], tmp[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "IMG_H = 100\n",
    "IMG_W = 100\n",
    "IMG_C = 3\n",
    "\n",
    "def readimg(path):\n",
    "    img = plt.imread(path)\n",
    "    return img\n",
    "\n",
    "def batch(path, batch_size):\n",
    "    img, label, paths = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        img.append(readimg(path[0][0]))\n",
    "        label.append(int(path[0][1]))\n",
    "        path.append(path.pop(0))\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-8c90a81a7ef2>:10: conv2d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-8c90a81a7ef2>:11: max_pooling2d (from tensorflow.python.keras.legacy_tf_layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-8c90a81a7ef2>:12: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-4-8c90a81a7ef2>:22: flatten (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-8c90a81a7ef2>:24: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "num_class = 4 # Sedan, Coupe, SUV, PickupTruck\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C]) # Input Layer, X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C])\n",
    "    Y = tf.placeholder(tf.int32, [None]) # Y = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    with tf.variable_scope('CNN'):\n",
    "        # 1st Layer(Conv1 - relu1 - maxpool1 - bn1) = 33*33*3\n",
    "        conv1 = tf.layers.conv2d(X, 3, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, 2, (1, 1), padding='VALID')\n",
    "        bn1 = tf.compat.v1.layers.batch_normalization(pool1, training=True)\n",
    "        # 2nd Layer(Conv2 - relu2 - maxpool2 - bn2) = 14*14*6\n",
    "        conv2 = tf.layers.conv2d(bn1, 6, 6, (1, 1), padding='VALID', activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, 2, (2, 2), padding='VALID')\n",
    "        bn2 = tf.compat.v1.layers.batch_normalization(pool2, training=True)\n",
    "        # 3rd Layer(Conv3 - relu3 - maxpool3 - bn3) = 3*3*9\n",
    "        conv3 = tf.layers.conv2d(bn2, 9, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling2d(conv3, 2, (2, 2), padding='VALID')\n",
    "        bn3 = tf.compat.v1.layers.batch_normalization(pool3, training=True)\n",
    "        # Fully Connected Layer(Affine)\n",
    "        affine1 = tf.layers.flatten(bn3)\n",
    "        # Output Layer\n",
    "        output = tf.layers.dense(affine1, num_class)\n",
    "        \n",
    "    # Softmax with Loss\n",
    "    with tf.variable_scope('Loss'):\n",
    "        Loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= Y, logits=output))\n",
    "    \n",
    "    # Training with Adam    \n",
    "    train_step = tf.train.AdamOptimizer(0.001).minimize(Loss) \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar('Epoch-Loss', Loss)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size\n",
    "np.sum([np.product(var.shape) for var in g.get_collection('trainable_variables')]).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.0545862\n",
      "Epoch: 2 Loss: 2.105048\n",
      "Epoch: 3 Loss: 1.9529419\n",
      "Epoch: 4 Loss: 1.9840019\n",
      "Epoch: 5 Loss: 1.8869127\n",
      "Epoch: 6 Loss: 1.8841138\n",
      "Epoch: 7 Loss: 1.8211415\n",
      "Epoch: 8 Loss: 1.80188\n",
      "Epoch: 9 Loss: 1.7704322\n",
      "Epoch: 10 Loss: 1.7298973\n",
      "Epoch: 11 Loss: 1.7282999\n",
      "Epoch: 12 Loss: 1.6682756\n",
      "Epoch: 13 Loss: 1.6615964\n",
      "Epoch: 14 Loss: 1.6272343\n",
      "Epoch: 15 Loss: 1.6111176\n",
      "Epoch: 16 Loss: 1.5887657\n",
      "Epoch: 17 Loss: 1.5664787\n",
      "Epoch: 18 Loss: 1.5501535\n",
      "Epoch: 19 Loss: 1.5202152\n",
      "Epoch: 20 Loss: 1.527372\n",
      "Epoch: 21 Loss: 1.4822496\n",
      "Epoch: 22 Loss: 1.489616\n",
      "Epoch: 23 Loss: 1.4626489\n",
      "Epoch: 24 Loss: 1.4608938\n",
      "Epoch: 25 Loss: 1.4408169\n",
      "Epoch: 26 Loss: 1.4348143\n",
      "Epoch: 27 Loss: 1.4152055\n",
      "Epoch: 28 Loss: 1.4132502\n",
      "Epoch: 29 Loss: 1.3974974\n",
      "Epoch: 30 Loss: 1.3900427\n",
      "Epoch: 31 Loss: 1.3814248\n",
      "Epoch: 32 Loss: 1.3733743\n",
      "Epoch: 33 Loss: 1.3626596\n",
      "Epoch: 34 Loss: 1.3606967\n",
      "Epoch: 35 Loss: 1.3360015\n",
      "Epoch: 36 Loss: 1.3371056\n",
      "Epoch: 37 Loss: 1.3250521\n",
      "Epoch: 38 Loss: 1.3174871\n",
      "Epoch: 39 Loss: 1.3089204\n",
      "Epoch: 40 Loss: 1.3024355\n",
      "Epoch: 41 Loss: 1.2911514\n",
      "Epoch: 42 Loss: 1.281278\n",
      "Epoch: 43 Loss: 1.2828459\n",
      "Epoch: 44 Loss: 1.2654192\n",
      "Epoch: 45 Loss: 1.2707598\n",
      "Epoch: 46 Loss: 1.2506616\n",
      "Epoch: 47 Loss: 1.2592157\n",
      "Epoch: 48 Loss: 1.2385335\n",
      "Epoch: 49 Loss: 1.2439301\n",
      "Epoch: 50 Loss: 1.2240397\n",
      "Epoch: 51 Loss: 1.2320619\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 52 Loss: 1.2107658\n",
      "Epoch: 53 Loss: 1.2216101\n",
      "Epoch: 54 Loss: 1.2003486\n",
      "Epoch: 55 Loss: 1.2138329\n",
      "Epoch: 56 Loss: 1.1865988\n",
      "Epoch: 57 Loss: 1.2051872\n",
      "Epoch: 58 Loss: 1.166213\n",
      "Epoch: 59 Loss: 1.1921878\n",
      "Epoch: 60 Loss: 1.164268\n",
      "Epoch: 61 Loss: 1.1734475\n",
      "Epoch: 62 Loss: 1.1600397\n",
      "Epoch: 63 Loss: 1.1588295\n",
      "Epoch: 64 Loss: 1.154883\n",
      "Epoch: 65 Loss: 1.1428467\n",
      "Epoch: 66 Loss: 1.1527095\n",
      "Epoch: 67 Loss: 1.1324769\n",
      "Epoch: 68 Loss: 1.1487681\n",
      "Epoch: 69 Loss: 1.1227273\n",
      "Epoch: 70 Loss: 1.1396226\n",
      "Epoch: 71 Loss: 1.115004\n",
      "Epoch: 72 Loss: 1.1327783\n",
      "Epoch: 73 Loss: 1.1051085\n",
      "Epoch: 74 Loss: 1.1245202\n",
      "Epoch: 75 Loss: 1.094351\n",
      "Epoch: 76 Loss: 1.1212978\n",
      "Epoch: 77 Loss: 1.0875543\n",
      "Epoch: 78 Loss: 1.1196393\n",
      "Epoch: 79 Loss: 1.0746083\n",
      "Epoch: 80 Loss: 1.1127347\n",
      "Epoch: 81 Loss: 1.064013\n",
      "Epoch: 82 Loss: 1.1015633\n",
      "Epoch: 83 Loss: 1.065338\n",
      "Epoch: 84 Loss: 1.0870414\n",
      "Epoch: 85 Loss: 1.0642555\n",
      "Epoch: 86 Loss: 1.074032\n",
      "Epoch: 87 Loss: 1.063782\n",
      "Epoch: 88 Loss: 1.0594227\n",
      "Epoch: 89 Loss: 1.0672823\n",
      "Epoch: 90 Loss: 1.0485051\n",
      "Epoch: 91 Loss: 1.0651307\n",
      "Epoch: 92 Loss: 1.039749\n",
      "Epoch: 93 Loss: 1.0589823\n",
      "Epoch: 94 Loss: 1.0314525\n",
      "Epoch: 95 Loss: 1.053541\n",
      "Epoch: 96 Loss: 1.0231369\n",
      "Epoch: 97 Loss: 1.0470544\n",
      "Epoch: 98 Loss: 1.0151793\n",
      "Epoch: 99 Loss: 1.04436\n",
      "Epoch: 100 Loss: 1.006045\n",
      "Epoch: 101 Loss: 1.0438292\n",
      "Epoch: 102 Loss: 0.9934254\n",
      "Epoch: 103 Loss: 1.0371263\n",
      "Epoch: 104 Loss: 0.98677933\n",
      "Epoch: 105 Loss: 1.0235606\n",
      "Epoch: 106 Loss: 0.9907434\n",
      "Epoch: 107 Loss: 1.0082619\n",
      "Epoch: 108 Loss: 0.98973525\n",
      "Epoch: 109 Loss: 0.9963922\n",
      "Epoch: 110 Loss: 0.9894219\n",
      "Epoch: 111 Loss: 0.9823204\n",
      "Epoch: 112 Loss: 0.99621415\n",
      "Epoch: 113 Loss: 0.9699068\n",
      "Epoch: 114 Loss: 0.9928272\n",
      "Epoch: 115 Loss: 0.9651857\n",
      "Epoch: 116 Loss: 0.9864205\n",
      "Epoch: 117 Loss: 0.95570844\n",
      "Epoch: 118 Loss: 0.9797832\n",
      "Epoch: 119 Loss: 0.94975764\n",
      "Epoch: 120 Loss: 0.9739061\n",
      "Epoch: 121 Loss: 0.94131374\n",
      "Epoch: 122 Loss: 0.97158384\n",
      "Epoch: 123 Loss: 0.93451816\n",
      "Epoch: 124 Loss: 0.97110534\n",
      "Epoch: 125 Loss: 0.92141706\n",
      "Epoch: 126 Loss: 0.963032\n",
      "Epoch: 127 Loss: 0.9180056\n",
      "Epoch: 128 Loss: 0.94856924\n",
      "Epoch: 129 Loss: 0.922632\n",
      "Epoch: 130 Loss: 0.9343243\n",
      "Epoch: 131 Loss: 0.9210288\n",
      "Epoch: 132 Loss: 0.9240028\n",
      "Epoch: 133 Loss: 0.9204794\n",
      "Epoch: 134 Loss: 0.9097623\n",
      "Epoch: 135 Loss: 0.9267118\n",
      "Epoch: 136 Loss: 0.898688\n",
      "Epoch: 137 Loss: 0.92643625\n",
      "Epoch: 138 Loss: 0.89231926\n",
      "Epoch: 139 Loss: 0.91720736\n",
      "Epoch: 140 Loss: 0.88654095\n",
      "Epoch: 141 Loss: 0.9101591\n",
      "Epoch: 142 Loss: 0.8795101\n",
      "Epoch: 143 Loss: 0.906215\n",
      "Epoch: 144 Loss: 0.87147206\n",
      "Epoch: 145 Loss: 0.9059849\n",
      "Epoch: 146 Loss: 0.8653653\n",
      "Epoch: 147 Loss: 0.9029071\n",
      "Epoch: 148 Loss: 0.85506284\n",
      "Epoch: 149 Loss: 0.8939893\n",
      "Epoch: 150 Loss: 0.8538623\n",
      "Epoch: 151 Loss: 0.88045764\n",
      "Epoch: 152 Loss: 0.8584636\n",
      "Epoch: 153 Loss: 0.86689246\n",
      "Epoch: 154 Loss: 0.8573518\n",
      "Epoch: 155 Loss: 0.8560807\n",
      "Epoch: 156 Loss: 0.8565628\n",
      "Epoch: 157 Loss: 0.84505785\n",
      "Epoch: 158 Loss: 0.8614013\n",
      "Epoch: 159 Loss: 0.8355497\n",
      "Epoch: 160 Loss: 0.86273867\n",
      "Epoch: 161 Loss: 0.82780373\n",
      "Epoch: 162 Loss: 0.8525324\n",
      "Epoch: 163 Loss: 0.82407635\n",
      "Epoch: 164 Loss: 0.8464098\n",
      "Epoch: 165 Loss: 0.815563\n",
      "Epoch: 166 Loss: 0.843593\n",
      "Epoch: 167 Loss: 0.807459\n",
      "Epoch: 168 Loss: 0.8426102\n",
      "Epoch: 169 Loss: 0.8031241\n",
      "Epoch: 170 Loss: 0.83799475\n",
      "Epoch: 171 Loss: 0.79331315\n",
      "Epoch: 172 Loss: 0.8304989\n",
      "Epoch: 173 Loss: 0.79240125\n",
      "Epoch: 174 Loss: 0.8174034\n",
      "Epoch: 175 Loss: 0.79823697\n",
      "Epoch: 176 Loss: 0.80412143\n",
      "Epoch: 177 Loss: 0.79550797\n",
      "Epoch: 178 Loss: 0.7953711\n",
      "Epoch: 179 Loss: 0.7946007\n",
      "Epoch: 180 Loss: 0.78570384\n",
      "Epoch: 181 Loss: 0.80123454\n",
      "Epoch: 182 Loss: 0.77535903\n",
      "Epoch: 183 Loss: 0.802416\n",
      "Epoch: 184 Loss: 0.76976305\n",
      "Epoch: 185 Loss: 0.7923909\n",
      "Epoch: 186 Loss: 0.76602775\n",
      "Epoch: 187 Loss: 0.78635347\n",
      "Epoch: 188 Loss: 0.7594555\n",
      "Epoch: 189 Loss: 0.7837473\n",
      "Epoch: 190 Loss: 0.74924433\n",
      "Epoch: 191 Loss: 0.78442734\n",
      "Epoch: 192 Loss: 0.74701124\n",
      "Epoch: 193 Loss: 0.77955157\n",
      "Epoch: 194 Loss: 0.7375022\n",
      "Epoch: 195 Loss: 0.7729696\n",
      "Epoch: 196 Loss: 0.73783743\n",
      "Epoch: 197 Loss: 0.7609977\n",
      "Epoch: 198 Loss: 0.74067396\n",
      "Epoch: 199 Loss: 0.7492965\n",
      "Epoch: 200 Loss: 0.7394604\n",
      "Epoch: 201 Loss: 0.7415413\n",
      "Epoch: 202 Loss: 0.738918\n",
      "Epoch: 203 Loss: 0.7323379\n",
      "Epoch: 204 Loss: 0.74453807\n",
      "Epoch: 205 Loss: 0.72507536\n",
      "Epoch: 206 Loss: 0.74457383\n",
      "Epoch: 207 Loss: 0.7193065\n",
      "Epoch: 208 Loss: 0.73633575\n",
      "Epoch: 209 Loss: 0.71537584\n",
      "Epoch: 210 Loss: 0.7311923\n",
      "Epoch: 211 Loss: 0.7064708\n",
      "Epoch: 212 Loss: 0.7311476\n",
      "Epoch: 213 Loss: 0.69604266\n",
      "Epoch: 214 Loss: 0.73256516\n",
      "Epoch: 215 Loss: 0.69507784\n",
      "Epoch: 216 Loss: 0.7289494\n",
      "Epoch: 217 Loss: 0.68649983\n",
      "Epoch: 218 Loss: 0.7207817\n",
      "Epoch: 219 Loss: 0.6881912\n",
      "Epoch: 220 Loss: 0.70928425\n",
      "Epoch: 221 Loss: 0.69194466\n",
      "Epoch: 222 Loss: 0.6988626\n",
      "Epoch: 223 Loss: 0.6897244\n",
      "Epoch: 224 Loss: 0.6914648\n",
      "Epoch: 225 Loss: 0.6905625\n",
      "Epoch: 226 Loss: 0.68167394\n",
      "Epoch: 227 Loss: 0.6970178\n",
      "Epoch: 228 Loss: 0.6756038\n",
      "Epoch: 229 Loss: 0.6967004\n",
      "Epoch: 230 Loss: 0.6716921\n",
      "Epoch: 231 Loss: 0.6892232\n",
      "Epoch: 232 Loss: 0.66669047\n",
      "Epoch: 233 Loss: 0.68432456\n",
      "Epoch: 234 Loss: 0.6584577\n",
      "Epoch: 235 Loss: 0.6863538\n",
      "Epoch: 236 Loss: 0.6471633\n",
      "Epoch: 237 Loss: 0.6868464\n",
      "Epoch: 238 Loss: 0.64809203\n",
      "Epoch: 239 Loss: 0.6815091\n",
      "Epoch: 240 Loss: 0.6425165\n",
      "Epoch: 241 Loss: 0.6728392\n",
      "Epoch: 242 Loss: 0.6437739\n",
      "Epoch: 243 Loss: 0.6623378\n",
      "Epoch: 244 Loss: 0.64673316\n",
      "Epoch: 245 Loss: 0.6509062\n",
      "Epoch: 246 Loss: 0.6464393\n",
      "Epoch: 247 Loss: 0.6452178\n",
      "Epoch: 248 Loss: 0.6462416\n",
      "Epoch: 249 Loss: 0.63465375\n",
      "Epoch: 250 Loss: 0.6523627\n",
      "Epoch: 251 Loss: 0.630423\n",
      "Epoch: 252 Loss: 0.650798\n",
      "Epoch: 253 Loss: 0.62801385\n",
      "Epoch: 254 Loss: 0.6419803\n",
      "Epoch: 255 Loss: 0.6211667\n",
      "Epoch: 256 Loss: 0.6383756\n",
      "Epoch: 257 Loss: 0.6139664\n",
      "Epoch: 258 Loss: 0.6397405\n",
      "Epoch: 259 Loss: 0.60326165\n",
      "Epoch: 260 Loss: 0.6374863\n",
      "Epoch: 261 Loss: 0.6051957\n",
      "Epoch: 262 Loss: 0.6324124\n",
      "Epoch: 263 Loss: 0.5985833\n",
      "Epoch: 264 Loss: 0.62373364\n",
      "Epoch: 265 Loss: 0.60049665\n",
      "Epoch: 266 Loss: 0.6132436\n",
      "Epoch: 267 Loss: 0.6006572\n",
      "Epoch: 268 Loss: 0.6040983\n",
      "Epoch: 269 Loss: 0.6002786\n",
      "Epoch: 270 Loss: 0.5971348\n",
      "Epoch: 271 Loss: 0.5996411\n",
      "Epoch: 272 Loss: 0.58872384\n",
      "Epoch: 273 Loss: 0.60402703\n",
      "Epoch: 274 Loss: 0.5844916\n",
      "Epoch: 275 Loss: 0.60399836\n",
      "Epoch: 276 Loss: 0.58038086\n",
      "Epoch: 277 Loss: 0.5951663\n",
      "Epoch: 278 Loss: 0.5734482\n",
      "Epoch: 279 Loss: 0.5907399\n",
      "Epoch: 280 Loss: 0.5663105\n",
      "Epoch: 281 Loss: 0.59051895\n",
      "Epoch: 282 Loss: 0.55862945\n",
      "Epoch: 283 Loss: 0.586953\n",
      "Epoch: 284 Loss: 0.5582534\n",
      "Epoch: 285 Loss: 0.5832832\n",
      "Epoch: 286 Loss: 0.55303097\n",
      "Epoch: 287 Loss: 0.5743597\n",
      "Epoch: 288 Loss: 0.55495906\n",
      "Epoch: 289 Loss: 0.5656901\n",
      "Epoch: 290 Loss: 0.5523379\n",
      "Epoch: 291 Loss: 0.55732816\n",
      "Epoch: 292 Loss: 0.55367076\n",
      "Epoch: 293 Loss: 0.5503312\n",
      "Epoch: 294 Loss: 0.5550495\n",
      "Epoch: 295 Loss: 0.54105604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 296 Loss: 0.5576919\n",
      "Epoch: 297 Loss: 0.5387074\n",
      "Epoch: 298 Loss: 0.55566114\n",
      "Epoch: 299 Loss: 0.5368554\n",
      "Epoch: 300 Loss: 0.54685545\n",
      "Epoch: 301 Loss: 0.52805346\n",
      "Epoch: 302 Loss: 0.54403114\n",
      "Epoch: 303 Loss: 0.52084434\n",
      "Epoch: 304 Loss: 0.5459362\n",
      "Epoch: 305 Loss: 0.5126323\n",
      "Epoch: 306 Loss: 0.5414714\n",
      "Epoch: 307 Loss: 0.51358044\n",
      "Epoch: 308 Loss: 0.53859466\n",
      "Epoch: 309 Loss: 0.50913787\n",
      "Epoch: 310 Loss: 0.5296135\n",
      "Epoch: 311 Loss: 0.5120751\n",
      "Epoch: 312 Loss: 0.5213061\n",
      "Epoch: 313 Loss: 0.5087022\n",
      "Epoch: 314 Loss: 0.5142618\n",
      "Epoch: 315 Loss: 0.5104329\n",
      "Epoch: 316 Loss: 0.5086236\n",
      "Epoch: 317 Loss: 0.51157427\n",
      "Epoch: 318 Loss: 0.50020814\n",
      "Epoch: 319 Loss: 0.51356524\n",
      "Epoch: 320 Loss: 0.49776554\n",
      "Epoch: 321 Loss: 0.5126197\n",
      "Epoch: 322 Loss: 0.4950677\n",
      "Epoch: 323 Loss: 0.50605845\n",
      "Epoch: 324 Loss: 0.48579085\n",
      "Epoch: 325 Loss: 0.5030857\n",
      "Epoch: 326 Loss: 0.47789574\n",
      "Epoch: 327 Loss: 0.50495976\n",
      "Epoch: 328 Loss: 0.47140902\n",
      "Epoch: 329 Loss: 0.5022693\n",
      "Epoch: 330 Loss: 0.47246888\n",
      "Epoch: 331 Loss: 0.49542332\n",
      "Epoch: 332 Loss: 0.4697754\n",
      "Epoch: 333 Loss: 0.48863423\n",
      "Epoch: 334 Loss: 0.47121498\n",
      "Epoch: 335 Loss: 0.48103452\n",
      "Epoch: 336 Loss: 0.46988833\n",
      "Epoch: 337 Loss: 0.4734337\n",
      "Epoch: 338 Loss: 0.4707437\n",
      "Epoch: 339 Loss: 0.468824\n",
      "Epoch: 340 Loss: 0.47214502\n",
      "Epoch: 341 Loss: 0.4612898\n",
      "Epoch: 342 Loss: 0.47382155\n",
      "Epoch: 343 Loss: 0.4593932\n",
      "Epoch: 344 Loss: 0.473352\n",
      "Epoch: 345 Loss: 0.45490822\n",
      "Epoch: 346 Loss: 0.46765503\n",
      "Epoch: 347 Loss: 0.4460779\n",
      "Epoch: 348 Loss: 0.4653232\n",
      "Epoch: 349 Loss: 0.43866205\n",
      "Epoch: 350 Loss: 0.46704018\n",
      "Epoch: 351 Loss: 0.4334273\n",
      "Epoch: 352 Loss: 0.46242982\n",
      "Epoch: 353 Loss: 0.4358555\n",
      "Epoch: 354 Loss: 0.45668697\n",
      "Epoch: 355 Loss: 0.43280822\n",
      "Epoch: 356 Loss: 0.44942224\n",
      "Epoch: 357 Loss: 0.4340699\n",
      "Epoch: 358 Loss: 0.44241506\n",
      "Epoch: 359 Loss: 0.4330914\n",
      "Epoch: 360 Loss: 0.4353283\n",
      "Epoch: 361 Loss: 0.43403015\n",
      "Epoch: 362 Loss: 0.43004248\n",
      "Epoch: 363 Loss: 0.43545133\n",
      "Epoch: 364 Loss: 0.42430446\n",
      "Epoch: 365 Loss: 0.4373265\n",
      "Epoch: 366 Loss: 0.42144597\n",
      "Epoch: 367 Loss: 0.43584532\n",
      "Epoch: 368 Loss: 0.41688606\n",
      "Epoch: 369 Loss: 0.43028966\n",
      "Epoch: 370 Loss: 0.4090126\n",
      "Epoch: 371 Loss: 0.42848012\n",
      "Epoch: 372 Loss: 0.40244138\n",
      "Epoch: 373 Loss: 0.42768124\n",
      "Epoch: 374 Loss: 0.39908507\n",
      "Epoch: 375 Loss: 0.4243551\n",
      "Epoch: 376 Loss: 0.39992297\n",
      "Epoch: 377 Loss: 0.41901094\n",
      "Epoch: 378 Loss: 0.39855212\n",
      "Epoch: 379 Loss: 0.41191575\n",
      "Epoch: 380 Loss: 0.39979532\n",
      "Epoch: 381 Loss: 0.40545955\n",
      "Epoch: 382 Loss: 0.39805672\n",
      "Epoch: 383 Loss: 0.3990619\n",
      "Epoch: 384 Loss: 0.39987326\n",
      "Epoch: 385 Loss: 0.39419466\n",
      "Epoch: 386 Loss: 0.40092227\n",
      "Epoch: 387 Loss: 0.38994017\n",
      "Epoch: 388 Loss: 0.40273917\n",
      "Epoch: 389 Loss: 0.38718104\n",
      "Epoch: 390 Loss: 0.3984712\n",
      "Epoch: 391 Loss: 0.38270488\n",
      "Epoch: 392 Loss: 0.396313\n",
      "Epoch: 393 Loss: 0.37458408\n",
      "Epoch: 394 Loss: 0.39549017\n",
      "Epoch: 395 Loss: 0.36837226\n",
      "Epoch: 396 Loss: 0.39215514\n",
      "Epoch: 397 Loss: 0.3665726\n",
      "Epoch: 398 Loss: 0.39019924\n",
      "Epoch: 399 Loss: 0.36775577\n",
      "Epoch: 400 Loss: 0.38407397\n",
      "Epoch: 401 Loss: 0.36730897\n",
      "Epoch: 402 Loss: 0.37844446\n",
      "Epoch: 403 Loss: 0.3677162\n",
      "Epoch: 404 Loss: 0.37153885\n",
      "Epoch: 405 Loss: 0.36568272\n",
      "Epoch: 406 Loss: 0.3677858\n",
      "Epoch: 407 Loss: 0.36564517\n",
      "Epoch: 408 Loss: 0.3626583\n",
      "Epoch: 409 Loss: 0.36852074\n",
      "Epoch: 410 Loss: 0.35844025\n",
      "Epoch: 411 Loss: 0.36950913\n",
      "Epoch: 412 Loss: 0.35658202\n",
      "Epoch: 413 Loss: 0.36577478\n",
      "Epoch: 414 Loss: 0.34941962\n",
      "Epoch: 415 Loss: 0.36443934\n",
      "Epoch: 416 Loss: 0.3431441\n",
      "Epoch: 417 Loss: 0.3629276\n",
      "Epoch: 418 Loss: 0.33842266\n",
      "Epoch: 419 Loss: 0.3596862\n",
      "Epoch: 420 Loss: 0.3374764\n",
      "Epoch: 421 Loss: 0.35734937\n",
      "Epoch: 422 Loss: 0.33740065\n",
      "Epoch: 423 Loss: 0.35114732\n",
      "Epoch: 424 Loss: 0.33796066\n",
      "Epoch: 425 Loss: 0.34561434\n",
      "Epoch: 426 Loss: 0.33885473\n",
      "Epoch: 427 Loss: 0.34011135\n",
      "Epoch: 428 Loss: 0.33697045\n",
      "Epoch: 429 Loss: 0.33637804\n",
      "Epoch: 430 Loss: 0.33657965\n",
      "Epoch: 431 Loss: 0.3326643\n",
      "Epoch: 432 Loss: 0.3395099\n",
      "Epoch: 433 Loss: 0.3296047\n",
      "Epoch: 434 Loss: 0.33915445\n",
      "Epoch: 435 Loss: 0.32839334\n",
      "Epoch: 436 Loss: 0.33620495\n",
      "Epoch: 437 Loss: 0.32126653\n",
      "Epoch: 438 Loss: 0.33381465\n",
      "Epoch: 439 Loss: 0.31531048\n",
      "Epoch: 440 Loss: 0.33316854\n",
      "Epoch: 441 Loss: 0.31132346\n",
      "Epoch: 442 Loss: 0.3297125\n",
      "Epoch: 443 Loss: 0.3105485\n",
      "Epoch: 444 Loss: 0.32747516\n",
      "Epoch: 445 Loss: 0.31031454\n",
      "Epoch: 446 Loss: 0.32302016\n",
      "Epoch: 447 Loss: 0.31046277\n",
      "Epoch: 448 Loss: 0.31771672\n",
      "Epoch: 449 Loss: 0.31095678\n",
      "Epoch: 450 Loss: 0.31219378\n",
      "Epoch: 451 Loss: 0.31018963\n",
      "Epoch: 452 Loss: 0.308247\n",
      "Epoch: 453 Loss: 0.3086461\n",
      "Epoch: 454 Loss: 0.30570823\n",
      "Epoch: 455 Loss: 0.31129375\n",
      "Epoch: 456 Loss: 0.30281323\n",
      "Epoch: 457 Loss: 0.310759\n",
      "Epoch: 458 Loss: 0.30171007\n",
      "Epoch: 459 Loss: 0.30737302\n",
      "Epoch: 460 Loss: 0.29529592\n",
      "Epoch: 461 Loss: 0.30530614\n",
      "Epoch: 462 Loss: 0.28906634\n",
      "Epoch: 463 Loss: 0.30446687\n",
      "Epoch: 464 Loss: 0.28514576\n",
      "Epoch: 465 Loss: 0.30227253\n",
      "Epoch: 466 Loss: 0.2845199\n",
      "Epoch: 467 Loss: 0.299865\n",
      "Epoch: 468 Loss: 0.2846918\n",
      "Epoch: 469 Loss: 0.2966902\n",
      "Epoch: 470 Loss: 0.2837149\n",
      "Epoch: 471 Loss: 0.2917044\n",
      "Epoch: 472 Loss: 0.28519812\n",
      "Epoch: 473 Loss: 0.2866161\n",
      "Epoch: 474 Loss: 0.2831328\n",
      "Epoch: 475 Loss: 0.2826462\n",
      "Epoch: 476 Loss: 0.28290015\n",
      "Epoch: 477 Loss: 0.28032914\n",
      "Epoch: 478 Loss: 0.28436092\n",
      "Epoch: 479 Loss: 0.27818736\n",
      "Epoch: 480 Loss: 0.2844164\n",
      "Epoch: 481 Loss: 0.27681196\n",
      "Epoch: 482 Loss: 0.28082803\n",
      "Epoch: 483 Loss: 0.27055162\n",
      "Epoch: 484 Loss: 0.2794997\n",
      "Epoch: 485 Loss: 0.26356086\n",
      "Epoch: 486 Loss: 0.27833128\n",
      "Epoch: 487 Loss: 0.26146126\n",
      "Epoch: 488 Loss: 0.27534908\n",
      "Epoch: 489 Loss: 0.2610039\n",
      "Epoch: 490 Loss: 0.2735963\n",
      "Epoch: 491 Loss: 0.2607941\n",
      "Epoch: 492 Loss: 0.27133322\n",
      "Epoch: 493 Loss: 0.2595214\n",
      "Epoch: 494 Loss: 0.26615313\n",
      "Epoch: 495 Loss: 0.26144728\n",
      "Epoch: 496 Loss: 0.26077175\n",
      "Epoch: 497 Loss: 0.25934342\n",
      "Epoch: 498 Loss: 0.25774962\n",
      "Epoch: 499 Loss: 0.25883922\n",
      "Epoch: 500 Loss: 0.25530007\n",
      "Epoch: 501 Loss: 0.2604677\n",
      "Epoch: 502 Loss: 0.25479913\n",
      "Epoch: 503 Loss: 0.25930068\n",
      "Epoch: 504 Loss: 0.2535863\n",
      "Epoch: 505 Loss: 0.25594148\n",
      "Epoch: 506 Loss: 0.2473122\n",
      "Epoch: 507 Loss: 0.2539773\n",
      "Epoch: 508 Loss: 0.24205956\n",
      "Epoch: 509 Loss: 0.25325164\n",
      "Epoch: 510 Loss: 0.23853798\n",
      "Epoch: 511 Loss: 0.25063053\n",
      "Epoch: 512 Loss: 0.23888016\n",
      "Epoch: 513 Loss: 0.24856244\n",
      "Epoch: 514 Loss: 0.23989701\n",
      "Epoch: 515 Loss: 0.24577358\n",
      "Epoch: 516 Loss: 0.23925112\n",
      "Epoch: 517 Loss: 0.24093601\n",
      "Epoch: 518 Loss: 0.2389241\n",
      "Epoch: 519 Loss: 0.23767005\n",
      "Epoch: 520 Loss: 0.23693165\n",
      "Epoch: 521 Loss: 0.2353609\n",
      "Epoch: 522 Loss: 0.23584141\n",
      "Epoch: 523 Loss: 0.23273095\n",
      "Epoch: 524 Loss: 0.23728973\n",
      "Epoch: 525 Loss: 0.23303789\n",
      "Epoch: 526 Loss: 0.23559831\n",
      "Epoch: 527 Loss: 0.23243795\n",
      "Epoch: 528 Loss: 0.23325178\n",
      "Epoch: 529 Loss: 0.22460808\n",
      "Epoch: 530 Loss: 0.2310723\n",
      "Epoch: 531 Loss: 0.21996185\n",
      "Epoch: 532 Loss: 0.2296366\n",
      "Epoch: 533 Loss: 0.21775198\n",
      "Epoch: 534 Loss: 0.22718565\n",
      "Epoch: 535 Loss: 0.21813112\n",
      "Epoch: 536 Loss: 0.2252411\n",
      "Epoch: 537 Loss: 0.21818076\n",
      "Epoch: 538 Loss: 0.22293387\n",
      "Epoch: 539 Loss: 0.21720615\n",
      "Epoch: 540 Loss: 0.21819071\n",
      "Epoch: 541 Loss: 0.21698628\n",
      "Epoch: 542 Loss: 0.2157367\n",
      "Epoch: 543 Loss: 0.21426792\n",
      "Epoch: 544 Loss: 0.21376777\n",
      "Epoch: 545 Loss: 0.2134898\n",
      "Epoch: 546 Loss: 0.21076173\n",
      "Epoch: 547 Loss: 0.21474788\n",
      "Epoch: 548 Loss: 0.21167763\n",
      "Epoch: 549 Loss: 0.21367005\n",
      "Epoch: 550 Loss: 0.2095234\n",
      "Epoch: 551 Loss: 0.2104877\n",
      "Epoch: 552 Loss: 0.2035719\n",
      "Epoch: 553 Loss: 0.20825298\n",
      "Epoch: 554 Loss: 0.19965318\n",
      "Epoch: 555 Loss: 0.20686547\n",
      "Epoch: 556 Loss: 0.19705918\n",
      "Epoch: 557 Loss: 0.20490816\n",
      "Epoch: 558 Loss: 0.19745389\n",
      "Epoch: 559 Loss: 0.20360215\n",
      "Epoch: 560 Loss: 0.19697677\n",
      "Epoch: 561 Loss: 0.2015748\n",
      "Epoch: 562 Loss: 0.19682139\n",
      "Epoch: 563 Loss: 0.19809563\n",
      "Epoch: 564 Loss: 0.19612385\n",
      "Epoch: 565 Loss: 0.19567886\n",
      "Epoch: 566 Loss: 0.19441304\n",
      "Epoch: 567 Loss: 0.19397093\n",
      "Epoch: 568 Loss: 0.19426706\n",
      "Epoch: 569 Loss: 0.19091107\n",
      "Epoch: 570 Loss: 0.19545278\n",
      "Epoch: 571 Loss: 0.19289018\n",
      "Epoch: 572 Loss: 0.19395407\n",
      "Epoch: 573 Loss: 0.19116615\n",
      "Epoch: 574 Loss: 0.19036958\n",
      "Epoch: 575 Loss: 0.1855736\n",
      "Epoch: 576 Loss: 0.18922795\n",
      "Epoch: 577 Loss: 0.1813279\n",
      "Epoch: 578 Loss: 0.18828775\n",
      "Epoch: 579 Loss: 0.17934394\n",
      "Epoch: 580 Loss: 0.18660341\n",
      "Epoch: 581 Loss: 0.17972925\n",
      "Epoch: 582 Loss: 0.18517855\n",
      "Epoch: 583 Loss: 0.17967878\n",
      "Epoch: 584 Loss: 0.18330744\n",
      "Epoch: 585 Loss: 0.17954837\n",
      "Epoch: 586 Loss: 0.1801321\n",
      "Epoch: 587 Loss: 0.17861179\n",
      "Epoch: 588 Loss: 0.17871238\n",
      "Epoch: 589 Loss: 0.17721994\n",
      "Epoch: 590 Loss: 0.1767194\n",
      "Epoch: 591 Loss: 0.17744115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 592 Loss: 0.17493868\n",
      "Epoch: 593 Loss: 0.17808208\n",
      "Epoch: 594 Loss: 0.17677662\n",
      "Epoch: 595 Loss: 0.17722285\n",
      "Epoch: 596 Loss: 0.17449537\n",
      "Epoch: 597 Loss: 0.17462862\n",
      "Epoch: 598 Loss: 0.16882235\n",
      "Epoch: 599 Loss: 0.17361887\n",
      "Epoch: 600 Loss: 0.16568232\n",
      "Epoch: 601 Loss: 0.17155634\n",
      "Epoch: 602 Loss: 0.16397777\n",
      "Epoch: 603 Loss: 0.17044844\n",
      "Epoch: 604 Loss: 0.16445577\n",
      "Epoch: 605 Loss: 0.16896445\n",
      "Epoch: 606 Loss: 0.16497312\n",
      "Epoch: 607 Loss: 0.16754942\n",
      "Epoch: 608 Loss: 0.16434968\n",
      "Epoch: 609 Loss: 0.1646232\n",
      "Epoch: 610 Loss: 0.16308656\n",
      "Epoch: 611 Loss: 0.16399896\n",
      "Epoch: 612 Loss: 0.16218852\n",
      "Epoch: 613 Loss: 0.16211264\n",
      "Epoch: 614 Loss: 0.16338794\n",
      "Epoch: 615 Loss: 0.16029269\n",
      "Epoch: 616 Loss: 0.16315033\n",
      "Epoch: 617 Loss: 0.16304418\n",
      "Epoch: 618 Loss: 0.16238347\n",
      "Epoch: 619 Loss: 0.16056737\n",
      "Epoch: 620 Loss: 0.15967633\n",
      "Epoch: 621 Loss: 0.15488005\n",
      "Epoch: 622 Loss: 0.15892062\n",
      "Epoch: 623 Loss: 0.1518391\n",
      "Epoch: 624 Loss: 0.15745565\n",
      "Epoch: 625 Loss: 0.15052338\n",
      "Epoch: 626 Loss: 0.15623373\n",
      "Epoch: 627 Loss: 0.15107669\n",
      "Epoch: 628 Loss: 0.15559426\n",
      "Epoch: 629 Loss: 0.15124185\n",
      "Epoch: 630 Loss: 0.15402748\n",
      "Epoch: 631 Loss: 0.15084793\n",
      "Epoch: 632 Loss: 0.15172653\n",
      "Epoch: 633 Loss: 0.14950997\n",
      "Epoch: 634 Loss: 0.1511417\n",
      "Epoch: 635 Loss: 0.14876336\n",
      "Epoch: 636 Loss: 0.14982164\n",
      "Epoch: 637 Loss: 0.14941233\n",
      "Epoch: 638 Loss: 0.14847554\n",
      "Epoch: 639 Loss: 0.14944398\n",
      "Epoch: 640 Loss: 0.1501892\n",
      "Epoch: 641 Loss: 0.14886288\n",
      "Epoch: 642 Loss: 0.14762644\n",
      "Epoch: 643 Loss: 0.14660385\n",
      "Epoch: 644 Loss: 0.14232846\n",
      "Epoch: 645 Loss: 0.14546701\n",
      "Epoch: 646 Loss: 0.13956268\n",
      "Epoch: 647 Loss: 0.14411452\n",
      "Epoch: 648 Loss: 0.13857757\n",
      "Epoch: 649 Loss: 0.14339483\n",
      "Epoch: 650 Loss: 0.13893594\n",
      "Epoch: 651 Loss: 0.14257993\n",
      "Epoch: 652 Loss: 0.13934454\n",
      "Epoch: 653 Loss: 0.14128533\n",
      "Epoch: 654 Loss: 0.13799307\n",
      "Epoch: 655 Loss: 0.13912609\n",
      "Epoch: 656 Loss: 0.1378152\n",
      "Epoch: 657 Loss: 0.13831154\n",
      "Epoch: 658 Loss: 0.13615122\n",
      "Epoch: 659 Loss: 0.13769157\n",
      "Epoch: 660 Loss: 0.13736999\n",
      "Epoch: 661 Loss: 0.13633004\n",
      "Epoch: 662 Loss: 0.1370065\n",
      "Epoch: 663 Loss: 0.13809884\n",
      "Epoch: 664 Loss: 0.13651055\n",
      "Epoch: 665 Loss: 0.13484435\n",
      "Epoch: 666 Loss: 0.13458484\n",
      "Epoch: 667 Loss: 0.13073592\n",
      "Epoch: 668 Loss: 0.13299187\n",
      "Epoch: 669 Loss: 0.12809984\n",
      "Epoch: 670 Loss: 0.13218628\n",
      "Epoch: 671 Loss: 0.12690152\n",
      "Epoch: 672 Loss: 0.13160373\n",
      "Epoch: 673 Loss: 0.12774426\n",
      "Epoch: 674 Loss: 0.13119823\n",
      "Epoch: 675 Loss: 0.12769634\n",
      "Epoch: 676 Loss: 0.12954651\n",
      "Epoch: 677 Loss: 0.12662841\n",
      "Epoch: 678 Loss: 0.12838846\n",
      "Epoch: 679 Loss: 0.12571344\n",
      "Epoch: 680 Loss: 0.12719832\n",
      "Epoch: 681 Loss: 0.12490777\n",
      "Epoch: 682 Loss: 0.12668568\n",
      "Epoch: 683 Loss: 0.12613922\n",
      "Epoch: 684 Loss: 0.12555107\n",
      "Epoch: 685 Loss: 0.12624444\n",
      "Epoch: 686 Loss: 0.126679\n",
      "Epoch: 687 Loss: 0.12571847\n",
      "Epoch: 688 Loss: 0.123882055\n",
      "Epoch: 689 Loss: 0.12305649\n",
      "Epoch: 690 Loss: 0.12040734\n",
      "Epoch: 691 Loss: 0.12211276\n",
      "Epoch: 692 Loss: 0.1174502\n",
      "Epoch: 693 Loss: 0.12122429\n",
      "Epoch: 694 Loss: 0.11726949\n",
      "Epoch: 695 Loss: 0.120914906\n",
      "Epoch: 696 Loss: 0.11722005\n",
      "Epoch: 697 Loss: 0.120286\n",
      "Epoch: 698 Loss: 0.11735139\n",
      "Epoch: 699 Loss: 0.11933425\n",
      "Epoch: 700 Loss: 0.116366155\n",
      "Epoch: 701 Loss: 0.117688276\n",
      "Epoch: 702 Loss: 0.11520511\n",
      "Epoch: 703 Loss: 0.11698644\n",
      "Epoch: 704 Loss: 0.1151336\n",
      "Epoch: 705 Loss: 0.11610925\n",
      "Epoch: 706 Loss: 0.11567152\n",
      "Epoch: 707 Loss: 0.11567214\n",
      "Epoch: 708 Loss: 0.11609648\n",
      "Epoch: 709 Loss: 0.11658299\n",
      "Epoch: 710 Loss: 0.114545345\n",
      "Epoch: 711 Loss: 0.113570146\n",
      "Epoch: 712 Loss: 0.11342386\n",
      "Epoch: 713 Loss: 0.10989227\n",
      "Epoch: 714 Loss: 0.11223674\n",
      "Epoch: 715 Loss: 0.10804753\n",
      "Epoch: 716 Loss: 0.110884406\n",
      "Epoch: 717 Loss: 0.107827134\n",
      "Epoch: 718 Loss: 0.11139958\n",
      "Epoch: 719 Loss: 0.10783483\n",
      "Epoch: 720 Loss: 0.11029305\n",
      "Epoch: 721 Loss: 0.10773417\n",
      "Epoch: 722 Loss: 0.10998829\n",
      "Epoch: 723 Loss: 0.10706049\n",
      "Epoch: 724 Loss: 0.10860187\n",
      "Epoch: 725 Loss: 0.10593848\n",
      "Epoch: 726 Loss: 0.1078897\n",
      "Epoch: 727 Loss: 0.10572917\n",
      "Epoch: 728 Loss: 0.10715837\n",
      "Epoch: 729 Loss: 0.10591147\n",
      "Epoch: 730 Loss: 0.107305914\n",
      "Epoch: 731 Loss: 0.10642252\n",
      "Epoch: 732 Loss: 0.10722803\n",
      "Epoch: 733 Loss: 0.1053568\n",
      "Epoch: 734 Loss: 0.10398664\n",
      "Epoch: 735 Loss: 0.10399554\n",
      "Epoch: 736 Loss: 0.10080786\n",
      "Epoch: 737 Loss: 0.10328839\n",
      "Epoch: 738 Loss: 0.09873726\n",
      "Epoch: 739 Loss: 0.10247372\n",
      "Epoch: 740 Loss: 0.098646484\n",
      "Epoch: 741 Loss: 0.10208055\n",
      "Epoch: 742 Loss: 0.0990348\n",
      "Epoch: 743 Loss: 0.10182024\n",
      "Epoch: 744 Loss: 0.09860271\n",
      "Epoch: 745 Loss: 0.10102013\n",
      "Epoch: 746 Loss: 0.09846754\n",
      "Epoch: 747 Loss: 0.10009187\n",
      "Epoch: 748 Loss: 0.09726487\n",
      "Epoch: 749 Loss: 0.099385686\n",
      "Epoch: 750 Loss: 0.09712066\n",
      "Epoch: 751 Loss: 0.09870716\n",
      "Epoch: 752 Loss: 0.09800265\n",
      "Epoch: 753 Loss: 0.09893833\n",
      "Epoch: 754 Loss: 0.09769219\n",
      "Epoch: 755 Loss: 0.09892814\n",
      "Epoch: 756 Loss: 0.09689256\n",
      "Epoch: 757 Loss: 0.09590341\n",
      "Epoch: 758 Loss: 0.09535379\n",
      "Epoch: 759 Loss: 0.092913575\n",
      "Epoch: 760 Loss: 0.095189296\n",
      "Epoch: 761 Loss: 0.090885475\n",
      "Epoch: 762 Loss: 0.09470015\n",
      "Epoch: 763 Loss: 0.09094571\n",
      "Epoch: 764 Loss: 0.094121404\n",
      "Epoch: 765 Loss: 0.09130745\n",
      "Epoch: 766 Loss: 0.09433796\n",
      "Epoch: 767 Loss: 0.090809785\n",
      "Epoch: 768 Loss: 0.0933725\n",
      "Epoch: 769 Loss: 0.09055475\n",
      "Epoch: 770 Loss: 0.09307686\n",
      "Epoch: 771 Loss: 0.08937731\n",
      "Epoch: 772 Loss: 0.092121154\n",
      "Epoch: 773 Loss: 0.089298755\n",
      "Epoch: 774 Loss: 0.091610305\n",
      "Epoch: 775 Loss: 0.090302624\n",
      "Epoch: 776 Loss: 0.09156443\n",
      "Epoch: 777 Loss: 0.09045207\n",
      "Epoch: 778 Loss: 0.0914721\n",
      "Epoch: 779 Loss: 0.08941247\n",
      "Epoch: 780 Loss: 0.08853009\n",
      "Epoch: 781 Loss: 0.08822352\n",
      "Epoch: 782 Loss: 0.08590036\n",
      "Epoch: 783 Loss: 0.0879101\n",
      "Epoch: 784 Loss: 0.084107\n",
      "Epoch: 785 Loss: 0.0875032\n",
      "Epoch: 786 Loss: 0.08422575\n",
      "Epoch: 787 Loss: 0.08747683\n",
      "Epoch: 788 Loss: 0.08443366\n",
      "Epoch: 789 Loss: 0.087304614\n",
      "Epoch: 790 Loss: 0.084269404\n",
      "Epoch: 791 Loss: 0.08626691\n",
      "Epoch: 792 Loss: 0.08406426\n",
      "Epoch: 793 Loss: 0.086104244\n",
      "Epoch: 794 Loss: 0.082701415\n",
      "Epoch: 795 Loss: 0.085413404\n",
      "Epoch: 796 Loss: 0.08289651\n",
      "Epoch: 797 Loss: 0.08481604\n",
      "Epoch: 798 Loss: 0.08356703\n",
      "Epoch: 799 Loss: 0.085181005\n",
      "Epoch: 800 Loss: 0.083960846\n",
      "Epoch: 801 Loss: 0.08483456\n",
      "Epoch: 802 Loss: 0.08259395\n",
      "Epoch: 803 Loss: 0.08208462\n",
      "Epoch: 804 Loss: 0.08193318\n",
      "Epoch: 805 Loss: 0.07929107\n",
      "Epoch: 806 Loss: 0.08153463\n",
      "Epoch: 807 Loss: 0.07828747\n",
      "Epoch: 808 Loss: 0.080836914\n",
      "Epoch: 809 Loss: 0.07846041\n",
      "Epoch: 810 Loss: 0.08079551\n",
      "Epoch: 811 Loss: 0.07882348\n",
      "Epoch: 812 Loss: 0.08112204\n",
      "Epoch: 813 Loss: 0.077856764\n",
      "Epoch: 814 Loss: 0.08028567\n",
      "Epoch: 815 Loss: 0.077856645\n",
      "Epoch: 816 Loss: 0.079822086\n",
      "Epoch: 817 Loss: 0.07661051\n",
      "Epoch: 818 Loss: 0.07921248\n",
      "Epoch: 819 Loss: 0.07695757\n",
      "Epoch: 820 Loss: 0.07859379\n",
      "Epoch: 821 Loss: 0.077755585\n",
      "Epoch: 822 Loss: 0.07918997\n",
      "Epoch: 823 Loss: 0.07768332\n",
      "Epoch: 824 Loss: 0.078835666\n",
      "Epoch: 825 Loss: 0.07637902\n",
      "Epoch: 826 Loss: 0.07603313\n",
      "Epoch: 827 Loss: 0.07561728\n",
      "Epoch: 828 Loss: 0.073537156\n",
      "Epoch: 829 Loss: 0.07547837\n",
      "Epoch: 830 Loss: 0.072523564\n",
      "Epoch: 831 Loss: 0.07506541\n",
      "Epoch: 832 Loss: 0.07255949\n",
      "Epoch: 833 Loss: 0.07503497\n",
      "Epoch: 834 Loss: 0.073102\n",
      "Epoch: 835 Loss: 0.07540737\n",
      "Epoch: 836 Loss: 0.07227307\n",
      "Epoch: 837 Loss: 0.074241556\n",
      "Epoch: 838 Loss: 0.07210427\n",
      "Epoch: 839 Loss: 0.07421175\n",
      "Epoch: 840 Loss: 0.07090575\n",
      "Epoch: 841 Loss: 0.07375378\n",
      "Epoch: 842 Loss: 0.0712903\n",
      "Epoch: 843 Loss: 0.07303074\n",
      "Epoch: 844 Loss: 0.07227607\n",
      "Epoch: 845 Loss: 0.07375539\n",
      "Epoch: 846 Loss: 0.072379984\n",
      "Epoch: 847 Loss: 0.073076025\n",
      "Epoch: 848 Loss: 0.07136231\n",
      "Epoch: 849 Loss: 0.070436135\n",
      "Epoch: 850 Loss: 0.07038287\n",
      "Epoch: 851 Loss: 0.06822579\n",
      "Epoch: 852 Loss: 0.07021206\n",
      "Epoch: 853 Loss: 0.06752657\n",
      "Epoch: 854 Loss: 0.06942971\n",
      "Epoch: 855 Loss: 0.06785338\n",
      "Epoch: 856 Loss: 0.07003162\n",
      "Epoch: 857 Loss: 0.067791805\n",
      "Epoch: 858 Loss: 0.07006921\n",
      "Epoch: 859 Loss: 0.067294374\n",
      "Epoch: 860 Loss: 0.068986334\n",
      "Epoch: 861 Loss: 0.067109875\n",
      "Epoch: 862 Loss: 0.069132276\n",
      "Epoch: 863 Loss: 0.06609187\n",
      "Epoch: 864 Loss: 0.068564974\n",
      "Epoch: 865 Loss: 0.06643899\n",
      "Epoch: 866 Loss: 0.0680527\n",
      "Epoch: 867 Loss: 0.06739244\n",
      "Epoch: 868 Loss: 0.0687802\n",
      "Epoch: 869 Loss: 0.06756334\n",
      "Epoch: 870 Loss: 0.06820667\n",
      "Epoch: 871 Loss: 0.0664403\n",
      "Epoch: 872 Loss: 0.06533239\n",
      "Epoch: 873 Loss: 0.06590467\n",
      "Epoch: 874 Loss: 0.06360158\n",
      "Epoch: 875 Loss: 0.06535844\n",
      "Epoch: 876 Loss: 0.0629857\n",
      "Epoch: 877 Loss: 0.064978406\n",
      "Epoch: 878 Loss: 0.063152246\n",
      "Epoch: 879 Loss: 0.065415315\n",
      "Epoch: 880 Loss: 0.06330541\n",
      "Epoch: 881 Loss: 0.065611556\n",
      "Epoch: 882 Loss: 0.062998004\n",
      "Epoch: 883 Loss: 0.06471639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 884 Loss: 0.062434673\n",
      "Epoch: 885 Loss: 0.0645641\n",
      "Epoch: 886 Loss: 0.061973143\n",
      "Epoch: 887 Loss: 0.06400395\n",
      "Epoch: 888 Loss: 0.062483482\n",
      "Epoch: 889 Loss: 0.063645825\n",
      "Epoch: 890 Loss: 0.06314191\n",
      "Epoch: 891 Loss: 0.06471543\n",
      "Epoch: 892 Loss: 0.063322276\n",
      "Epoch: 893 Loss: 0.06369587\n",
      "Epoch: 894 Loss: 0.062103145\n",
      "Epoch: 895 Loss: 0.06119199\n",
      "Epoch: 896 Loss: 0.061565664\n",
      "Epoch: 897 Loss: 0.05966002\n",
      "Epoch: 898 Loss: 0.0611718\n",
      "Epoch: 899 Loss: 0.05896361\n",
      "Epoch: 900 Loss: 0.060859315\n",
      "Epoch: 901 Loss: 0.059194706\n",
      "Epoch: 902 Loss: 0.061015792\n",
      "Epoch: 903 Loss: 0.05934117\n",
      "Epoch: 904 Loss: 0.06148715\n",
      "Epoch: 905 Loss: 0.059059653\n",
      "Epoch: 906 Loss: 0.060504086\n",
      "Epoch: 907 Loss: 0.058295775\n",
      "Epoch: 908 Loss: 0.06062986\n",
      "Epoch: 909 Loss: 0.058095243\n",
      "Epoch: 910 Loss: 0.059893563\n",
      "Epoch: 911 Loss: 0.058651317\n",
      "Epoch: 912 Loss: 0.05976746\n",
      "Epoch: 913 Loss: 0.058925554\n",
      "Epoch: 914 Loss: 0.060856745\n",
      "Epoch: 915 Loss: 0.059271857\n",
      "Epoch: 916 Loss: 0.059774555\n",
      "Epoch: 917 Loss: 0.058174644\n",
      "Epoch: 918 Loss: 0.05728114\n",
      "Epoch: 919 Loss: 0.058015347\n",
      "Epoch: 920 Loss: 0.05564522\n",
      "Epoch: 921 Loss: 0.05738057\n",
      "Epoch: 922 Loss: 0.055217735\n",
      "Epoch: 923 Loss: 0.05720733\n",
      "Epoch: 924 Loss: 0.055454083\n",
      "Epoch: 925 Loss: 0.057249498\n",
      "Epoch: 926 Loss: 0.05578468\n",
      "Epoch: 927 Loss: 0.057741944\n",
      "Epoch: 928 Loss: 0.055271283\n",
      "Epoch: 929 Loss: 0.056782585\n",
      "Epoch: 930 Loss: 0.054732896\n",
      "Epoch: 931 Loss: 0.0569308\n",
      "Epoch: 932 Loss: 0.054473042\n",
      "Epoch: 933 Loss: 0.056410044\n",
      "Epoch: 934 Loss: 0.055103738\n",
      "Epoch: 935 Loss: 0.056190908\n",
      "Epoch: 936 Loss: 0.055466946\n",
      "Epoch: 937 Loss: 0.057597093\n",
      "Epoch: 938 Loss: 0.05570721\n",
      "Epoch: 939 Loss: 0.05628763\n",
      "Epoch: 940 Loss: 0.0546735\n",
      "Epoch: 941 Loss: 0.053589545\n",
      "Epoch: 942 Loss: 0.054431308\n",
      "Epoch: 943 Loss: 0.05217191\n",
      "Epoch: 944 Loss: 0.05408813\n",
      "Epoch: 945 Loss: 0.051652927\n",
      "Epoch: 946 Loss: 0.053664822\n",
      "Epoch: 947 Loss: 0.052100226\n",
      "Epoch: 948 Loss: 0.054072767\n",
      "Epoch: 949 Loss: 0.0523944\n",
      "Epoch: 950 Loss: 0.054171667\n",
      "Epoch: 951 Loss: 0.052081916\n",
      "Epoch: 952 Loss: 0.05340436\n",
      "Epoch: 953 Loss: 0.05135455\n",
      "Epoch: 954 Loss: 0.053337954\n",
      "Epoch: 955 Loss: 0.051268034\n",
      "Epoch: 956 Loss: 0.05312041\n",
      "Epoch: 957 Loss: 0.05193099\n",
      "Epoch: 958 Loss: 0.053097814\n",
      "Epoch: 959 Loss: 0.05227083\n",
      "Epoch: 960 Loss: 0.054073196\n",
      "Epoch: 961 Loss: 0.05254432\n",
      "Epoch: 962 Loss: 0.052781295\n",
      "Epoch: 963 Loss: 0.05158214\n",
      "Epoch: 964 Loss: 0.05017568\n",
      "Epoch: 965 Loss: 0.05154094\n",
      "Epoch: 966 Loss: 0.048925955\n",
      "Epoch: 967 Loss: 0.050870635\n",
      "Epoch: 968 Loss: 0.048901804\n",
      "Epoch: 969 Loss: 0.05068871\n",
      "Epoch: 970 Loss: 0.049038716\n",
      "Epoch: 971 Loss: 0.051079083\n",
      "Epoch: 972 Loss: 0.04939964\n",
      "Epoch: 973 Loss: 0.050994523\n",
      "Epoch: 974 Loss: 0.049011283\n",
      "Epoch: 975 Loss: 0.050425407\n",
      "Epoch: 976 Loss: 0.048364893\n",
      "Epoch: 977 Loss: 0.05033928\n",
      "Epoch: 978 Loss: 0.04819808\n",
      "Epoch: 979 Loss: 0.050165247\n",
      "Epoch: 980 Loss: 0.048969142\n",
      "Epoch: 981 Loss: 0.050152067\n",
      "Epoch: 982 Loss: 0.049242955\n",
      "Epoch: 983 Loss: 0.05108268\n",
      "Epoch: 984 Loss: 0.049452607\n",
      "Epoch: 985 Loss: 0.04948373\n",
      "Epoch: 986 Loss: 0.048595835\n",
      "Epoch: 987 Loss: 0.04738556\n",
      "Epoch: 988 Loss: 0.04839807\n",
      "Epoch: 989 Loss: 0.04622813\n",
      "Epoch: 990 Loss: 0.04794161\n",
      "Epoch: 991 Loss: 0.045959197\n",
      "Epoch: 992 Loss: 0.047775228\n",
      "Epoch: 993 Loss: 0.046528094\n",
      "Epoch: 994 Loss: 0.04811008\n",
      "Epoch: 995 Loss: 0.046577815\n",
      "Epoch: 996 Loss: 0.048125517\n",
      "Epoch: 997 Loss: 0.046271823\n",
      "Epoch: 998 Loss: 0.047530234\n",
      "Epoch: 999 Loss: 0.045598242\n",
      "Epoch: 1000 Loss: 0.047405913\n"
     ]
    }
   ],
   "source": [
    "# Setting Batch with Training\n",
    "batch_size = 1461\n",
    "epoch = 1000\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('tf_board', sess.graph)\n",
    "    for i in range(epoch):\n",
    "        batch_data, batch_label = batch(trainlist, batch_size)     \n",
    "        _, loss, summary = sess.run([train_step, Loss, merged], feed_dict = {X: batch_data, Y: batch_label})\n",
    "        print(\"Epoch:\",i+1,\"Loss:\",loss)\n",
    "        if i % 10 == 0:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)\n",
    "        elif i+1 == epoch:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs\\model.ckpt-1000\n",
      "[-0.12030726 -3.281833   -4.6532474   1.4550184 ] 0\n",
      "[ -2.4777465   7.056954  -10.055142    2.309625 ] 0\n",
      "[-1.2164823   8.837171   -0.66297483 -2.0606356 ] 0\n",
      "[ 1.9857156 -5.870653   6.470988  -0.2034377] 0\n",
      "[-1.8764936 -1.1147162 -5.9599423  7.5368237] 0\n",
      "[-3.5484948  -7.1042027   0.22709276  7.767265  ] 0\n",
      "[-5.098679  -4.30971    3.3988812  1.3152031] 0\n",
      "[-0.05911385  3.4939508  -5.558946   -1.4235957 ] 0\n",
      "[-0.36215955  3.6515214   5.703395   -3.472001  ] 0\n",
      "[-7.8600597  6.2443237  7.1569676 -3.6156983] 0\n",
      "[ 0.64147097 -1.5988799   4.9654655  -4.648388  ] 0\n",
      "[ 1.9114863  4.4253507  5.9227247 -7.335736 ] 0\n",
      "[ 4.9575505 -8.860134   6.7882423 -4.9683924] 0\n",
      "[-2.877397    0.32375586  1.5115387  -0.21094956] 0\n",
      "[-0.10113918  8.202822   -1.7576274  -2.8392599 ] 0\n",
      "[-0.4883746  6.618279  -7.9138927  0.839384 ] 0\n",
      "[-1.3141445  1.5968447  3.6116095 -8.722089 ] 0\n",
      "[ 1.3349209  6.1164327  3.4531128 -6.0038385] 0\n",
      "[ 3.5587907  0.5471276  8.966492  -6.0080404] 0\n",
      "[ 3.306456  -4.5238557  7.417804  -5.699491 ] 0\n",
      "[ 0.05609602 -2.4667413  -0.23808055  0.5530322 ] 0\n",
      "[ 2.088395   4.5862637 -2.3700244 -5.165488 ] 0\n",
      "[-0.30951452 -6.205733    1.6493486   3.4687667 ] 0\n",
      "[-1.82379    4.6071653 -1.5232856 -1.5386275] 0\n",
      "[-1.0294614   2.3729298   2.3071465  -0.06913368] 0\n",
      "[-6.3032646   3.1057732  -2.112279   -0.25851178] 0\n",
      "[-7.544635   -0.8075264   6.0915694   0.29773426] 0\n",
      "[-1.4864788 -2.7553923  3.7796443  4.8996115] 0\n",
      "[ -0.36659372   0.20941782 -11.458148     4.681447  ] 0\n",
      "[-4.4323688  4.294419  11.519872  -3.543586 ] 0\n",
      "[ 0.9949909  2.6984437 -2.0753658 -5.155268 ] 0\n",
      "[-1.2792426 -3.650794  11.937281  -7.37704  ] 0\n",
      "[ 0.30782992 -0.5056436   3.6675575  -2.7436328 ] 0\n",
      "[-0.7864145  1.7001569  0.5575782 -5.029589 ] 0\n",
      "[-1.6427181 -3.3859642  4.6547427  2.0066214] 0\n",
      "[-3.009132   2.457703   2.7423055 -1.7504109] 0\n",
      "[-0.93222904  4.5664616  -5.118164   -4.5152144 ] 0\n",
      "[-1.9524621   7.8343377  -6.333515   -0.24387187] 0\n",
      "[  0.09662367   9.2412       3.5920546  -10.1491375 ] 0\n",
      "[ 0.4896415  -0.64191425 -6.601955    1.4750746 ] 0\n",
      "[-6.3816357  -1.9550782   2.2676604  -0.03533222] 0\n",
      "[-1.5562478  1.150353   6.20419   -6.950134 ] 0\n",
      "[ 1.1073655  -7.7695622   2.1850843   0.20363052] 0\n",
      "[-2.5663743  3.5414932  7.976491  -6.2949862] 0\n",
      "[-1.528589  -5.4308     9.621397  -0.8866654] 0\n",
      "[-2.484105   -1.6210833   0.79088485  1.8703619 ] 0\n",
      "[-1.0944684 -7.8124056  2.5401905  5.162793 ] 0\n",
      "[-1.653219   4.4749503 -5.3843255  0.7736149] 0\n",
      "[-0.8429841 -3.272866  -4.859053   4.0596843] 0\n",
      "[  0.13330479 -10.385934    -1.3877895    0.3610872 ] 0\n",
      "[ 4.016245   -1.6006869  -1.1554576   0.09326009] 1\n",
      "[ 5.1057396  4.659585  -2.398341  -1.1164405] 1\n",
      "[ 8.394281  -7.4960327  5.981462  -3.2325263] 1\n",
      "[ 5.361246  -0.9473674 -1.7448275 -2.9583783] 1\n",
      "[ 3.4826121  -6.4698553   0.07384945 -4.35815   ] 1\n",
      "[-5.3480473 -0.90408    1.0356475  4.3430634] 1\n",
      "[  0.45535964 -12.731844    -4.609386     7.3816414 ] 1\n",
      "[  4.739037   -11.022957     4.2777877   -0.50008273] 1\n",
      "[-3.8010192  1.0629643  4.5073347 -3.8109813] 1\n",
      "[-2.997223  -2.9548676 -2.276161   8.596872 ] 1\n",
      "[-6.3494625  1.2773526 -9.630153   4.9363346] 1\n",
      "[ 2.9301417 -5.433043  -7.897702  10.739983 ] 1\n",
      "[ 6.0529485  4.502951  -5.176706  -7.4157434] 1\n",
      "[ 2.0222945  -3.678046    5.4295397  -0.59971184] 1\n",
      "[-3.5663824   4.0678306   8.413051   -0.76841295] 1\n",
      "[ 7.1427803 -4.081121  -4.9766498  2.4656963] 1\n",
      "[-0.4897924  -4.7201705   0.23483624  5.6766276 ] 1\n",
      "[-1.1725224 -3.1443574  3.1899347  2.2692308] 1\n",
      "[-0.7774853 -4.443466   7.6705184  1.1110903] 1\n",
      "[ 1.2717831 -1.2155699 -1.7155288 -5.803806 ] 1\n",
      "[ 6.2566233  2.9803987  3.4622874 -3.279188 ] 1\n",
      "[ 1.1449     -0.91070044  0.59679997 -1.3710964 ] 1\n",
      "[-5.565983  -4.234793   7.229093   0.7008736] 1\n",
      "[ 2.7020712 -4.4879217  3.7587242 -2.526165 ] 1\n",
      "[ 6.3785324 -6.367581  -3.8170054 -1.0820973] 1\n",
      "[-1.6609176 -1.8575133  4.776476  -1.4800355] 1\n",
      "[ 3.0107064  0.4028282  1.0119098 -3.5711374] 1\n",
      "[ 6.3243814  0.7976173  1.0583141 -9.854869 ] 1\n",
      "[ 5.360518   3.3294747 -2.369802  -3.817265 ] 1\n",
      "[ 0.2090222 -1.4953392 -2.9213026  4.9679565] 1\n",
      "[ 1.3326473 -7.746481  -6.2767997  5.016086 ] 1\n",
      "[ 0.21155624 -4.648974    2.9776118  -3.9324963 ] 1\n",
      "[-7.7799726 -4.5883393  1.1820384  6.9121523] 1\n",
      "[ 1.5539075 -1.1634642  1.0371224 -4.420959 ] 1\n",
      "[-1.5105191  -0.43449718 -2.0872352   4.762229  ] 1\n",
      "[ 2.824996  -6.954357   3.1663485  1.3425173] 1\n",
      "[ 1.7185075 -1.9481413 -5.0769134  4.2093897] 1\n",
      "[ 5.0868163   0.90536344 -3.1837032  -0.02288929] 1\n",
      "[-3.7223434  -0.18338086  4.6111455   3.6081314 ] 1\n",
      "[ 1.5601499 -3.8976254  5.0063043 -6.6132216] 1\n",
      "[-0.48852426 -1.2285022  -2.262468    2.8370616 ] 1\n",
      "[-4.0904984  3.9948316  2.5495048  1.2790618] 2\n",
      "[ 0.32215673  2.2123718  -3.2874815  -1.725191  ] 2\n",
      "[ 5.808434  -1.9016596 -0.5362279  2.7364762] 2\n",
      "[ 6.4477615   1.7289668   0.34841704 -6.8533745 ] 2\n",
      "[ 9.395473  -2.712742  -1.8403922 -6.1751094] 2\n",
      "[-2.0072153  5.2498612 -1.5383409 -8.553907 ] 2\n",
      "[-7.746893  8.44242  -5.41493   3.581968] 2\n",
      "[-6.150943   8.497137   1.9712179 -2.4582186] 2\n",
      "[ 9.086367   0.7892917  4.0871973 -2.795314 ] 2\n",
      "[ 3.1872978  4.807151  -1.0596467 -5.8260565] 2\n",
      "[-5.025985   4.855427  -1.692778  -2.2747927] 2\n",
      "[ 3.299304  -1.479278   3.0226712 -5.4738274] 2\n",
      "[ 3.6044521  4.912098  -3.8456028 -8.018428 ] 2\n",
      "[ 6.087141  5.616426 -4.407223  0.179783] 2\n",
      "[1.9720329  0.58916235 0.9095644  0.81747615] 2\n",
      "[  1.4092449   4.7973285 -10.856689    0.0355721] 2\n",
      "[-12.690371    2.241941    2.2498987   4.601971 ] 2\n",
      "[-1.1638563 -3.6860652 -1.5239685  3.832573 ] 2\n",
      "[-4.8965793  6.5335245  2.0265346 -5.625626 ] 2\n",
      "[ 1.4552203  3.5303948 -6.471446   1.231872 ] 2\n",
      "[ 0.12443919 -1.3461008  -3.5951679   1.1151654 ] 2\n",
      "[ 2.5195827  -4.7007184  -0.09588204  5.92481   ] 2\n",
      "[  0.5611178    3.6427543  -11.584021     0.65779585] 2\n",
      "[ 8.015924  -9.156895   3.9360962 -3.7109299] 2\n",
      "[-8.620569   5.8417253  2.4613996 -1.9596664] 2\n",
      "[ 3.6972046 -2.8371115 -2.5805357 -2.10817  ] 2\n",
      "[-3.8641315 -2.1778755 -4.1755786  4.488126 ] 2\n",
      "[-6.160153    1.7026407   0.15509704  5.852039  ] 2\n",
      "[-4.761262   4.6205435 -9.387204   7.7563434] 2\n",
      "[  3.935494  -11.172446   -3.7896197   8.846663 ] 2\n",
      "[-2.9093688 11.512423  -3.4350162 -4.2051725] 2\n",
      "[-6.7774105 -3.0375051 -1.8916799  5.032181 ] 2\n",
      "[ 2.539479  -1.8895446 -0.172531  -3.4957585] 2\n",
      "[-2.6180875  6.312088   2.7265923 -0.557479 ] 2\n",
      "[-3.6503227e+00 -2.3902457e+00 -3.6396869e-03  4.9610138e+00] 2\n",
      "[ 9.547969   0.5768946  1.8425052 -8.29045  ] 2\n",
      "[-3.1511676   4.3384333  -1.0474817   0.38794422] 2\n",
      "[-0.8078101 -0.5550048 -6.2390847  4.2658987] 2\n",
      "[ 8.494929    1.2904308  -0.06795739 -4.261293  ] 2\n",
      "[ 6.7484283  2.3568017  3.018434  -6.7999716] 2\n",
      "[ 2.0877867  1.0754347 -8.828888   6.1407466] 2\n",
      "[-1.4495596   0.70765185  0.44859064 -3.0952053 ] 3\n",
      "[  2.6031737  -0.5090692   3.001828  -11.314607 ] 3\n",
      "[ 4.063696  -1.3423594  4.2053914 -1.4243985] 3\n",
      "[ 0.4024256  1.3287225 -4.902367  -3.4022725] 3\n",
      "[ 3.8842726  -1.2541449  -0.01378368 -0.71105313] 3\n",
      "[ 6.673805  -0.7122617  1.031832  -7.1185594] 3\n",
      "[-1.735819  -6.222226   9.61815    2.0646384] 3\n",
      "[ 3.033312   -0.17544305 -0.8739121  -7.2874093 ] 3\n",
      "[-2.5501866 -1.5314913  2.791176  -1.5355932] 3\n",
      "[-5.3981423  2.0623608 -0.806646   1.6498644] 3\n",
      "[ 3.9152422  0.7198893 -3.1146421 -3.0315273] 3\n",
      "[ -1.5736605   3.6250231 -10.675264    3.2578263] 3\n",
      "[-6.786134   4.7770357  8.874844  -2.9598153] 3\n",
      "[  2.5095983 -11.194925    0.7013784   1.8461641] 3\n",
      "[-7.106468   9.036775  -1.5830766  1.4839134] 3\n",
      "[-3.5355866  -2.0342813   0.6862024   0.00517138] 3\n",
      "[ 0.52470934  3.8492293  -1.7755862   2.853734  ] 3\n",
      "[-0.43417814 -3.0490496   9.235547   -8.871111  ] 3\n",
      "[-3.351759   2.574696  -0.9890233 -0.4934681] 3\n",
      "[ 4.0262094   2.2385695   0.53278154 -6.1176867 ] 3\n",
      "[-0.45198947  7.9151716  -1.8007997  -3.9320588 ] 3\n",
      "[ 9.35735    2.8997738 -8.316615  -4.0545044] 3\n",
      "[ 1.8861058 -1.3788108 -1.5707678 -5.1570206] 3\n",
      "[-3.7780643  4.335405   1.0651915  2.676573 ] 3\n",
      "[ 2.856922    0.23364417 -7.7828255  -2.693777  ] 3\n",
      "[ 6.350477   5.7921767 -7.7076817 -4.7469225] 3\n",
      "[-2.073622  -5.403006   7.4496675  3.8174064] 3\n",
      "[ 1.1880807 -7.573378   7.7285457 -2.7926717] 3\n",
      "[ 1.5062109  4.571004   3.1418784 -7.8964925] 3\n",
      "[-0.8350697  8.580845  -2.23746   -1.0604113] 3\n",
      "[ 2.5513344  0.9807818 -4.1181364 -2.0882652] 3\n",
      "[ 3.6341114 -4.4833975 -4.0572    -1.411344 ] 3\n",
      "[-1.9066443 -1.7545615  5.159122  -0.7789393] 3\n",
      "[ 7.1587377  7.462984  -1.8504239 -6.3624554] 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1286565   6.931317   -4.6676955  -0.20109098] 3\n",
      "[ 5.0763054 -8.138518   1.6620226 -1.8426087] 3\n",
      "[ 8.53099    7.6264663 -1.6522506 -8.886767 ] 3\n",
      "[-5.594768   7.33219   -5.418709   3.5177734] 3\n",
      "[ 1.975495    2.7416673   0.65969265 -6.9638367 ] 3\n",
      "[ 4.843412    0.94500667  1.5853206  -3.1576502 ] 3\n",
      "Accuracy: 0.2833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Print an Accuracy\n",
    "acc = 0\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint('logs')\n",
    "    if checkpoint:\n",
    "        saver.restore(sess, checkpoint)\n",
    "    for i in range(len(testlist)):\n",
    "        batch_data, batch_label = batch(testlist, 1)\n",
    "        logit = sess.run(output, feed_dict = {X:batch_data})\n",
    "        if np.argmax(logit[0]) == batch_label[0]:\n",
    "            acc += 1\n",
    "        else:\n",
    "            print(logit[0], batch_label[0])\n",
    "            \n",
    "    print(\"Accuracy:\", acc/len(testlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
