{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동차 이미지들을 학습 및 테스트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터는 각 480개씩 총 1920개로 구성되어있고,\n",
    "시험 데이터는 각 160개씩 총 480개로 구성되어있습니다.\n",
    "\n",
    "차종은 세단, 쿠페, SUV, 픽업트럭으로 네가지를 구분하였고 종류별 숫자 코드는 다음과 같습니다.\n",
    "0 = Sedan, 1 = Coupe, 2 = SUV, 3 = Pickup-Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Namin Neural Network\n",
    "    ----------\n",
    "    Input Layer: 100*100*3(Input Size)\n",
    "    \n",
    "    1st Layer: Conv1(3EA, 3*3*3, Strides=3, Padding=1),\n",
    "               ReLU1(34*34*3),\n",
    "               Pool1(2*2, Strides=1),\n",
    "               Bn1(33*33*3)\n",
    "    2nd Layer: Conv2(6EA, 6*6*3, Strides=1, Padding=VALID),\n",
    "               ReLU2(28*28*6),\n",
    "               Pool2(2*2, Strides=2),\n",
    "               Bn2(14*14*6)\n",
    "    3rd Layer: Conv3(9EA, 3*3*6, Strides=3, Padding=2),\n",
    "               ReLU3(6*6*9),\n",
    "               Pool3(2*2, Strides=2),\n",
    "               Bn3(3*3*9)\n",
    "    4th Layer: Flatten,\n",
    "               Affine(W=3*3*9, B=9)\n",
    "                \n",
    "    Output Layer: Softmax Cross Entropy(Sedan, Coupe, SUV, Pickup-Truck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to Sedan, Coupe, SUV, PickupTruck Images\n",
    "trainlist, testlist = [], []\n",
    "with open('train.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        trainlist.append([tmp[0], tmp[1]])\n",
    "        \n",
    "with open('test.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        testlist.append([tmp[0], tmp[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "IMG_H = 100\n",
    "IMG_W = 100\n",
    "IMG_C = 3\n",
    "\n",
    "def readimg(path):\n",
    "    img = plt.imread(path)\n",
    "    return img\n",
    "\n",
    "def batch(path, batch_size):\n",
    "    img, label, paths = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        img.append(readimg(path[0][0]))\n",
    "        label.append(int(path[0][1]))\n",
    "        path.append(path.pop(0))\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "num_class = 4 # Sedan, Coupe, SUV, PickupTruck\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C]) # Input Layer, X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C])\n",
    "    Y = tf.placeholder(tf.int32, [None]) # Y = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    with tf.variable_scope('CNN'):\n",
    "        # 1st Layer(Conv1 - relu1 - maxpool1 - bn1) = 33*33*3\n",
    "        conv1 = tf.layers.conv2d(X, 3, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, 2, (1, 1), padding='VALID')\n",
    "        bn1 = tf.compat.v1.layers.batch_normalization(pool1, training=True)\n",
    "        # 2nd Layer(Conv2 - relu2 - maxpool2 - bn2) = 14*14*6\n",
    "        conv2 = tf.layers.conv2d(bn1, 6, 6, (1, 1), padding='VALID', activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, 2, (2, 2), padding='VALID')\n",
    "        bn2 = tf.compat.v1.layers.batch_normalization(pool2, training=True)\n",
    "        # 3rd Layer(Conv3 - relu3 - maxpool3 - bn3) = 3*3*9\n",
    "        conv3 = tf.layers.conv2d(bn2, 9, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling2d(conv3, 2, (2, 2), padding='VALID')\n",
    "        bn3 = tf.compat.v1.layers.batch_normalization(pool3, training=True)\n",
    "        # Fully Connected Layer(Affine)\n",
    "        affine1 = tf.layers.flatten(bn3)\n",
    "        # Output Layer\n",
    "        output = tf.layers.dense(affine1, num_class)\n",
    "        \n",
    "    # Softmax with Loss\n",
    "    with tf.variable_scope('Loss'):\n",
    "        Loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= Y, logits=output))\n",
    "    \n",
    "    # Training with Adam    \n",
    "    train_step = tf.train.AdamOptimizer(0.005, beta1=0.9, beta2=0.999).minimize(Loss) \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar('Epoch-Loss', Loss)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size\n",
    "np.sum([np.product(var.shape) for var in g.get_collection('trainable_variables')]).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.842403\n",
      "Epoch: 2 Loss: 1.5546517\n",
      "Epoch: 3 Loss: 1.5860039\n",
      "Epoch: 4 Loss: 1.4614025\n",
      "Epoch: 5 Loss: 1.4471499\n",
      "Epoch: 6 Loss: 1.1652559\n",
      "Epoch: 7 Loss: 1.1132253\n",
      "Epoch: 8 Loss: 0.9840789\n",
      "Epoch: 9 Loss: 1.0310172\n",
      "Epoch: 10 Loss: 0.95310575\n",
      "Epoch: 11 Loss: 2.3206503\n",
      "Epoch: 12 Loss: 2.5604267\n",
      "Epoch: 13 Loss: 2.1791894\n",
      "Epoch: 14 Loss: 2.1872263\n",
      "Epoch: 15 Loss: 2.0629036\n",
      "Epoch: 16 Loss: 1.8591622\n",
      "Epoch: 17 Loss: 1.882165\n",
      "Epoch: 18 Loss: 1.6168758\n",
      "Epoch: 19 Loss: 1.5575806\n",
      "Epoch: 20 Loss: 1.6853269\n",
      "Epoch: 21 Loss: 2.6511672\n",
      "Epoch: 22 Loss: 2.395547\n",
      "Epoch: 23 Loss: 2.1813936\n",
      "Epoch: 24 Loss: 1.8824915\n",
      "Epoch: 25 Loss: 1.7863803\n",
      "Epoch: 26 Loss: 1.7270676\n",
      "Epoch: 27 Loss: 1.5662298\n",
      "Epoch: 28 Loss: 1.575847\n",
      "Epoch: 29 Loss: 1.671554\n",
      "Epoch: 30 Loss: 1.766152\n",
      "Epoch: 31 Loss: 2.5231733\n",
      "Epoch: 32 Loss: 2.5118606\n",
      "Epoch: 33 Loss: 2.4094818\n",
      "Epoch: 34 Loss: 2.4296525\n",
      "Epoch: 35 Loss: 2.0789087\n",
      "Epoch: 36 Loss: 2.0049827\n",
      "Epoch: 37 Loss: 1.9325482\n",
      "Epoch: 38 Loss: 1.782046\n",
      "Epoch: 39 Loss: 1.6473004\n",
      "Epoch: 40 Loss: 1.5726703\n",
      "Epoch: 41 Loss: 1.7831076\n",
      "Epoch: 42 Loss: 1.7825533\n",
      "Epoch: 43 Loss: 1.6968626\n",
      "Epoch: 44 Loss: 1.9649194\n",
      "Epoch: 45 Loss: 1.7581409\n",
      "Epoch: 46 Loss: 2.0671895\n",
      "Epoch: 47 Loss: 1.8286546\n",
      "Epoch: 48 Loss: 1.7237066\n",
      "Epoch: 49 Loss: 1.6303797\n",
      "Epoch: 50 Loss: 1.4653391\n",
      "Epoch: 51 Loss: 1.7042702\n",
      "Epoch: 52 Loss: 1.5884253\n",
      "Epoch: 53 Loss: 1.6641618\n",
      "Epoch: 54 Loss: 1.5332586\n",
      "Epoch: 55 Loss: 1.4428148\n",
      "Epoch: 56 Loss: 1.5529817\n",
      "Epoch: 57 Loss: 1.4224542\n",
      "Epoch: 58 Loss: 1.4981979\n",
      "Epoch: 59 Loss: 1.4640608\n",
      "Epoch: 60 Loss: 1.5031285\n",
      "Epoch: 61 Loss: 1.986888\n",
      "Epoch: 62 Loss: 1.9340271\n",
      "Epoch: 63 Loss: 1.8671423\n",
      "Epoch: 64 Loss: 1.6625541\n",
      "Epoch: 65 Loss: 1.722952\n",
      "Epoch: 66 Loss: 1.7766796\n",
      "Epoch: 67 Loss: 1.7178392\n",
      "Epoch: 68 Loss: 1.6184233\n",
      "Epoch: 69 Loss: 1.6032352\n",
      "Epoch: 70 Loss: 1.6547483\n",
      "Epoch: 71 Loss: 1.5333582\n",
      "Epoch: 72 Loss: 1.5952363\n",
      "Epoch: 73 Loss: 1.6180458\n",
      "Epoch: 74 Loss: 1.7422212\n",
      "Epoch: 75 Loss: 1.5235523\n",
      "Epoch: 76 Loss: 1.4900271\n",
      "Epoch: 77 Loss: 1.395496\n",
      "Epoch: 78 Loss: 1.3696008\n",
      "Epoch: 79 Loss: 1.2964717\n",
      "Epoch: 80 Loss: 1.2947878\n",
      "Epoch: 81 Loss: 1.5414568\n",
      "Epoch: 82 Loss: 1.5253035\n",
      "Epoch: 83 Loss: 1.4794116\n",
      "Epoch: 84 Loss: 1.5908422\n",
      "Epoch: 85 Loss: 1.5364999\n",
      "Epoch: 86 Loss: 1.7206055\n",
      "Epoch: 87 Loss: 1.5578855\n",
      "Epoch: 88 Loss: 1.497598\n",
      "Epoch: 89 Loss: 1.5019914\n",
      "Epoch: 90 Loss: 1.4413061\n",
      "Epoch: 91 Loss: 1.5519556\n",
      "Epoch: 92 Loss: 1.4535443\n",
      "Epoch: 93 Loss: 1.5110726\n",
      "Epoch: 94 Loss: 1.4261469\n",
      "Epoch: 95 Loss: 1.3136182\n",
      "Epoch: 96 Loss: 1.471666\n",
      "Epoch: 97 Loss: 1.378037\n",
      "Epoch: 98 Loss: 1.4355222\n",
      "Epoch: 99 Loss: 1.3873483\n",
      "Epoch: 100 Loss: 1.3659805\n",
      "Epoch: 101 Loss: 1.6402278\n",
      "Epoch: 102 Loss: 1.599836\n",
      "Epoch: 103 Loss: 1.5806932\n",
      "Epoch: 104 Loss: 1.4977751\n",
      "Epoch: 105 Loss: 1.4779888\n",
      "Epoch: 106 Loss: 1.5382477\n",
      "Epoch: 107 Loss: 1.5188814\n",
      "Epoch: 108 Loss: 1.4939874\n",
      "Epoch: 109 Loss: 1.4900112\n",
      "Epoch: 110 Loss: 1.4957567\n",
      "Epoch: 111 Loss: 1.3470899\n",
      "Epoch: 112 Loss: 1.4574007\n",
      "Epoch: 113 Loss: 1.4401083\n",
      "Epoch: 114 Loss: 1.5583769\n",
      "Epoch: 115 Loss: 1.4061747\n",
      "Epoch: 116 Loss: 1.4194771\n",
      "Epoch: 117 Loss: 1.3268201\n",
      "Epoch: 118 Loss: 1.3325754\n",
      "Epoch: 119 Loss: 1.264265\n",
      "Epoch: 120 Loss: 1.2861581\n",
      "Epoch: 121 Loss: 1.4681352\n",
      "Epoch: 122 Loss: 1.4806852\n",
      "Epoch: 123 Loss: 1.4396394\n",
      "Epoch: 124 Loss: 1.5420758\n",
      "Epoch: 125 Loss: 1.4737043\n",
      "Epoch: 126 Loss: 1.6241107\n",
      "Epoch: 127 Loss: 1.49579\n",
      "Epoch: 128 Loss: 1.4239575\n",
      "Epoch: 129 Loss: 1.4604038\n",
      "Epoch: 130 Loss: 1.4235181\n",
      "Epoch: 131 Loss: 1.4634094\n",
      "Epoch: 132 Loss: 1.3967568\n",
      "Epoch: 133 Loss: 1.4537143\n",
      "Epoch: 134 Loss: 1.3703235\n",
      "Epoch: 135 Loss: 1.3087125\n",
      "Epoch: 136 Loss: 1.426885\n",
      "Epoch: 137 Loss: 1.3634915\n",
      "Epoch: 138 Loss: 1.3902379\n",
      "Epoch: 139 Loss: 1.3482116\n",
      "Epoch: 140 Loss: 1.3275281\n",
      "Epoch: 141 Loss: 1.5649554\n",
      "Epoch: 142 Loss: 1.5466763\n",
      "Epoch: 143 Loss: 1.5471973\n",
      "Epoch: 144 Loss: 1.4676403\n",
      "Epoch: 145 Loss: 1.4413947\n",
      "Epoch: 146 Loss: 1.4921767\n",
      "Epoch: 147 Loss: 1.4705118\n",
      "Epoch: 148 Loss: 1.4668471\n",
      "Epoch: 149 Loss: 1.4335722\n",
      "Epoch: 150 Loss: 1.4612383\n",
      "Epoch: 151 Loss: 1.2978901\n",
      "Epoch: 152 Loss: 1.4164509\n",
      "Epoch: 153 Loss: 1.3771706\n",
      "Epoch: 154 Loss: 1.4984199\n",
      "Epoch: 155 Loss: 1.3777971\n",
      "Epoch: 156 Loss: 1.3803114\n",
      "Epoch: 157 Loss: 1.2936789\n",
      "Epoch: 158 Loss: 1.31122\n",
      "Epoch: 159 Loss: 1.2299279\n",
      "Epoch: 160 Loss: 1.2743493\n",
      "Epoch: 161 Loss: 1.4448627\n",
      "Epoch: 162 Loss: 1.476272\n",
      "Epoch: 163 Loss: 1.4372824\n",
      "Epoch: 164 Loss: 1.5273056\n",
      "Epoch: 165 Loss: 1.4504846\n",
      "Epoch: 166 Loss: 1.5818733\n",
      "Epoch: 167 Loss: 1.4826412\n",
      "Epoch: 168 Loss: 1.423633\n",
      "Epoch: 169 Loss: 1.4414057\n",
      "Epoch: 170 Loss: 1.4131536\n",
      "Epoch: 171 Loss: 1.4213578\n",
      "Epoch: 172 Loss: 1.3526373\n",
      "Epoch: 173 Loss: 1.436617\n",
      "Epoch: 174 Loss: 1.3376116\n",
      "Epoch: 175 Loss: 1.3088477\n",
      "Epoch: 176 Loss: 1.4281746\n",
      "Epoch: 177 Loss: 1.3646237\n",
      "Epoch: 178 Loss: 1.3435639\n",
      "Epoch: 179 Loss: 1.3422543\n",
      "Epoch: 180 Loss: 1.3228129\n",
      "Epoch: 181 Loss: 1.5295349\n",
      "Epoch: 182 Loss: 1.5195643\n",
      "Epoch: 183 Loss: 1.4957503\n",
      "Epoch: 184 Loss: 1.4732808\n",
      "Epoch: 185 Loss: 1.432334\n",
      "Epoch: 186 Loss: 1.4268061\n",
      "Epoch: 187 Loss: 1.4274659\n",
      "Epoch: 188 Loss: 1.4701267\n",
      "Epoch: 189 Loss: 1.4204718\n",
      "Epoch: 190 Loss: 1.3899298\n",
      "Epoch: 191 Loss: 1.327264\n",
      "Epoch: 192 Loss: 1.4471103\n",
      "Epoch: 193 Loss: 1.3582859\n",
      "Epoch: 194 Loss: 1.4589001\n",
      "Epoch: 195 Loss: 1.3651689\n",
      "Epoch: 196 Loss: 1.3828942\n",
      "Epoch: 197 Loss: 1.2873417\n",
      "Epoch: 198 Loss: 1.3170694\n",
      "Epoch: 199 Loss: 1.2033871\n",
      "Epoch: 200 Loss: 1.2791264\n",
      "Epoch: 201 Loss: 1.4103737\n",
      "Epoch: 202 Loss: 1.4507154\n",
      "Epoch: 203 Loss: 1.4232984\n",
      "Epoch: 204 Loss: 1.4739051\n",
      "Epoch: 205 Loss: 1.3905277\n",
      "Epoch: 206 Loss: 1.5263513\n",
      "Epoch: 207 Loss: 1.4449118\n",
      "Epoch: 208 Loss: 1.3844404\n",
      "Epoch: 209 Loss: 1.3805633\n",
      "Epoch: 210 Loss: 1.3559672\n",
      "Epoch: 211 Loss: 1.4390522\n",
      "Epoch: 212 Loss: 1.4004115\n",
      "Epoch: 213 Loss: 1.4338974\n",
      "Epoch: 214 Loss: 1.3460827\n",
      "Epoch: 215 Loss: 1.3351107\n",
      "Epoch: 216 Loss: 1.3670641\n",
      "Epoch: 217 Loss: 1.3770968\n",
      "Epoch: 218 Loss: 1.3538638\n",
      "Epoch: 219 Loss: 1.3237604\n",
      "Epoch: 220 Loss: 1.3266703\n",
      "Epoch: 221 Loss: 1.6072801\n",
      "Epoch: 222 Loss: 1.589757\n",
      "Epoch: 223 Loss: 1.557621\n",
      "Epoch: 224 Loss: 1.4729733\n",
      "Epoch: 225 Loss: 1.4480338\n",
      "Epoch: 226 Loss: 1.4973431\n",
      "Epoch: 227 Loss: 1.4591564\n",
      "Epoch: 228 Loss: 1.4433345\n",
      "Epoch: 229 Loss: 1.3669869\n",
      "Epoch: 230 Loss: 1.4376673\n",
      "Epoch: 231 Loss: 1.4176598\n",
      "Epoch: 232 Loss: 1.4823608\n",
      "Epoch: 233 Loss: 1.4518181\n",
      "Epoch: 234 Loss: 1.5690438\n",
      "Epoch: 235 Loss: 1.4341551\n",
      "Epoch: 236 Loss: 1.3850783\n",
      "Epoch: 237 Loss: 1.2561858\n",
      "Epoch: 238 Loss: 1.3288645\n",
      "Epoch: 239 Loss: 1.2268249\n",
      "Epoch: 240 Loss: 1.2773012\n",
      "Epoch: 241 Loss: 1.4756111\n",
      "Epoch: 242 Loss: 1.4916378\n",
      "Epoch: 243 Loss: 1.4454638\n",
      "Epoch: 244 Loss: 1.5303396\n",
      "Epoch: 245 Loss: 1.4383802\n",
      "Epoch: 246 Loss: 1.5797586\n",
      "Epoch: 247 Loss: 1.4696003\n",
      "Epoch: 248 Loss: 1.4762412\n",
      "Epoch: 249 Loss: 1.4539896\n",
      "Epoch: 250 Loss: 1.4189523\n",
      "Epoch: 251 Loss: 1.4111933\n",
      "Epoch: 252 Loss: 1.3306074\n",
      "Epoch: 253 Loss: 1.4115883\n",
      "Epoch: 254 Loss: 1.3212366\n",
      "Epoch: 255 Loss: 1.3100187\n",
      "Epoch: 256 Loss: 1.3682737\n",
      "Epoch: 257 Loss: 1.35616\n",
      "Epoch: 258 Loss: 1.3108377\n",
      "Epoch: 259 Loss: 1.2991825\n",
      "Epoch: 260 Loss: 1.2973927\n",
      "Epoch: 261 Loss: 1.4854708\n",
      "Epoch: 262 Loss: 1.4950806\n",
      "Epoch: 263 Loss: 1.4773735\n",
      "Epoch: 264 Loss: 1.467853\n",
      "Epoch: 265 Loss: 1.435977\n",
      "Epoch: 266 Loss: 1.3616123\n",
      "Epoch: 267 Loss: 1.4069897\n",
      "Epoch: 268 Loss: 1.456884\n",
      "Epoch: 269 Loss: 1.3989486\n",
      "Epoch: 270 Loss: 1.3752476\n",
      "Epoch: 271 Loss: 1.4570912\n",
      "Epoch: 272 Loss: 1.510918\n",
      "Epoch: 273 Loss: 1.3670887\n",
      "Epoch: 274 Loss: 1.4880289\n",
      "Epoch: 275 Loss: 1.4261208\n",
      "Epoch: 276 Loss: 1.416433\n",
      "Epoch: 277 Loss: 1.312966\n",
      "Epoch: 278 Loss: 1.3327557\n",
      "Epoch: 279 Loss: 1.2357978\n",
      "Epoch: 280 Loss: 1.2873712\n",
      "Epoch: 281 Loss: 1.4374441\n",
      "Epoch: 282 Loss: 1.4411459\n",
      "Epoch: 283 Loss: 1.4280338\n",
      "Epoch: 284 Loss: 1.4278854\n",
      "Epoch: 285 Loss: 1.3849572\n",
      "Epoch: 286 Loss: 1.4429146\n",
      "Epoch: 287 Loss: 1.4167233\n",
      "Epoch: 288 Loss: 1.3724316\n",
      "Epoch: 289 Loss: 1.3698076\n",
      "Epoch: 290 Loss: 1.3366504\n",
      "Epoch: 291 Loss: 1.430329\n",
      "Epoch: 292 Loss: 1.3939229\n",
      "Epoch: 293 Loss: 1.4348364\n",
      "Epoch: 294 Loss: 1.3363062\n",
      "Epoch: 295 Loss: 1.369361\n",
      "Epoch: 296 Loss: 1.3732849\n",
      "Epoch: 297 Loss: 1.374828\n",
      "Epoch: 298 Loss: 1.3894271\n",
      "Epoch: 299 Loss: 1.317665\n",
      "Epoch: 300 Loss: 1.3350598\n",
      "Epoch: 301 Loss: 1.5884908\n",
      "Epoch: 302 Loss: 1.5524702\n",
      "Epoch: 303 Loss: 1.5554962\n",
      "Epoch: 304 Loss: 1.4453541\n",
      "Epoch: 305 Loss: 1.4449233\n",
      "Epoch: 306 Loss: 1.468133\n",
      "Epoch: 307 Loss: 1.4530636\n",
      "Epoch: 308 Loss: 1.4323622\n",
      "Epoch: 309 Loss: 1.3486782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 310 Loss: 1.4336394\n",
      "Epoch: 311 Loss: 1.3938466\n",
      "Epoch: 312 Loss: 1.4446888\n",
      "Epoch: 313 Loss: 1.4007746\n",
      "Epoch: 314 Loss: 1.5125831\n",
      "Epoch: 315 Loss: 1.3854259\n",
      "Epoch: 316 Loss: 1.3761324\n",
      "Epoch: 317 Loss: 1.262642\n",
      "Epoch: 318 Loss: 1.3508339\n",
      "Epoch: 319 Loss: 1.2268624\n",
      "Epoch: 320 Loss: 1.2774855\n",
      "Epoch: 321 Loss: 1.4461813\n",
      "Epoch: 322 Loss: 1.4722306\n",
      "Epoch: 323 Loss: 1.4223171\n",
      "Epoch: 324 Loss: 1.4990724\n",
      "Epoch: 325 Loss: 1.4077059\n",
      "Epoch: 326 Loss: 1.5642058\n",
      "Epoch: 327 Loss: 1.4745747\n",
      "Epoch: 328 Loss: 1.4726442\n",
      "Epoch: 329 Loss: 1.4534904\n",
      "Epoch: 330 Loss: 1.4098631\n",
      "Epoch: 331 Loss: 1.3960013\n",
      "Epoch: 332 Loss: 1.339235\n",
      "Epoch: 333 Loss: 1.4161547\n",
      "Epoch: 334 Loss: 1.2997531\n",
      "Epoch: 335 Loss: 1.3385109\n",
      "Epoch: 336 Loss: 1.3523113\n",
      "Epoch: 337 Loss: 1.3570557\n",
      "Epoch: 338 Loss: 1.3331851\n",
      "Epoch: 339 Loss: 1.3085216\n",
      "Epoch: 340 Loss: 1.300516\n",
      "Epoch: 341 Loss: 1.4825948\n",
      "Epoch: 342 Loss: 1.4937544\n",
      "Epoch: 343 Loss: 1.5103111\n",
      "Epoch: 344 Loss: 1.4784724\n",
      "Epoch: 345 Loss: 1.440381\n",
      "Epoch: 346 Loss: 1.4094623\n",
      "Epoch: 347 Loss: 1.4097433\n",
      "Epoch: 348 Loss: 1.454165\n",
      "Epoch: 349 Loss: 1.3589125\n",
      "Epoch: 350 Loss: 1.3787416\n",
      "Epoch: 351 Loss: 1.3031197\n",
      "Epoch: 352 Loss: 1.3829818\n",
      "Epoch: 353 Loss: 1.2752094\n",
      "Epoch: 354 Loss: 1.3883401\n",
      "Epoch: 355 Loss: 1.300964\n",
      "Epoch: 356 Loss: 1.3041543\n",
      "Epoch: 357 Loss: 1.2156452\n",
      "Epoch: 358 Loss: 1.2421473\n",
      "Epoch: 359 Loss: 1.1654097\n",
      "Epoch: 360 Loss: 1.2269487\n",
      "Epoch: 361 Loss: 1.4163448\n",
      "Epoch: 362 Loss: 1.4832735\n",
      "Epoch: 363 Loss: 1.4442134\n",
      "Epoch: 364 Loss: 1.4744507\n",
      "Epoch: 365 Loss: 1.4017166\n",
      "Epoch: 366 Loss: 1.4994869\n",
      "Epoch: 367 Loss: 1.4353275\n",
      "Epoch: 368 Loss: 1.4100555\n",
      "Epoch: 369 Loss: 1.4113779\n",
      "Epoch: 370 Loss: 1.3748407\n",
      "Epoch: 371 Loss: 1.3923856\n",
      "Epoch: 372 Loss: 1.3584019\n",
      "Epoch: 373 Loss: 1.4538511\n",
      "Epoch: 374 Loss: 1.3080918\n",
      "Epoch: 375 Loss: 1.3747692\n",
      "Epoch: 376 Loss: 1.3614779\n",
      "Epoch: 377 Loss: 1.3699538\n",
      "Epoch: 378 Loss: 1.3571757\n",
      "Epoch: 379 Loss: 1.3202819\n",
      "Epoch: 380 Loss: 1.3275143\n",
      "Epoch: 381 Loss: 1.5072991\n",
      "Epoch: 382 Loss: 1.4576868\n",
      "Epoch: 383 Loss: 1.5090586\n",
      "Epoch: 384 Loss: 1.4093596\n",
      "Epoch: 385 Loss: 1.3825212\n",
      "Epoch: 386 Loss: 1.4190503\n",
      "Epoch: 387 Loss: 1.3979001\n",
      "Epoch: 388 Loss: 1.4294758\n",
      "Epoch: 389 Loss: 1.3137801\n",
      "Epoch: 390 Loss: 1.4006529\n",
      "Epoch: 391 Loss: 1.2406646\n",
      "Epoch: 392 Loss: 1.33049\n",
      "Epoch: 393 Loss: 1.3200248\n",
      "Epoch: 394 Loss: 1.3856541\n",
      "Epoch: 395 Loss: 1.2685093\n",
      "Epoch: 396 Loss: 1.3181896\n",
      "Epoch: 397 Loss: 1.2136813\n",
      "Epoch: 398 Loss: 1.2804407\n",
      "Epoch: 399 Loss: 1.1960536\n",
      "Epoch: 400 Loss: 1.2298802\n",
      "Epoch: 401 Loss: 1.4308802\n",
      "Epoch: 402 Loss: 1.4459707\n",
      "Epoch: 403 Loss: 1.4240093\n",
      "Epoch: 404 Loss: 1.4627243\n",
      "Epoch: 405 Loss: 1.4066248\n",
      "Epoch: 406 Loss: 1.5111904\n",
      "Epoch: 407 Loss: 1.4276733\n",
      "Epoch: 408 Loss: 1.4383564\n",
      "Epoch: 409 Loss: 1.4331245\n",
      "Epoch: 410 Loss: 1.4082007\n",
      "Epoch: 411 Loss: 1.3500413\n",
      "Epoch: 412 Loss: 1.3016849\n",
      "Epoch: 413 Loss: 1.4051005\n",
      "Epoch: 414 Loss: 1.2632354\n",
      "Epoch: 415 Loss: 1.2836818\n",
      "Epoch: 416 Loss: 1.3161966\n",
      "Epoch: 417 Loss: 1.3371919\n",
      "Epoch: 418 Loss: 1.3122785\n",
      "Epoch: 419 Loss: 1.2731901\n",
      "Epoch: 420 Loss: 1.2949308\n",
      "Epoch: 421 Loss: 1.4600549\n",
      "Epoch: 422 Loss: 1.4448477\n",
      "Epoch: 423 Loss: 1.48021\n",
      "Epoch: 424 Loss: 1.4117862\n",
      "Epoch: 425 Loss: 1.3767413\n",
      "Epoch: 426 Loss: 1.4227381\n",
      "Epoch: 427 Loss: 1.3920912\n",
      "Epoch: 428 Loss: 1.452425\n",
      "Epoch: 429 Loss: 1.3159882\n",
      "Epoch: 430 Loss: 1.395513\n",
      "Epoch: 431 Loss: 1.1397648\n",
      "Epoch: 432 Loss: 1.3209157\n",
      "Epoch: 433 Loss: 1.2621037\n",
      "Epoch: 434 Loss: 1.3062478\n",
      "Epoch: 435 Loss: 1.2756389\n",
      "Epoch: 436 Loss: 1.3247386\n",
      "Epoch: 437 Loss: 1.1735321\n",
      "Epoch: 438 Loss: 1.2485342\n",
      "Epoch: 439 Loss: 1.1574675\n",
      "Epoch: 440 Loss: 1.2243216\n",
      "Epoch: 441 Loss: 1.4017206\n",
      "Epoch: 442 Loss: 1.4600824\n",
      "Epoch: 443 Loss: 1.4160951\n",
      "Epoch: 444 Loss: 1.4270114\n",
      "Epoch: 445 Loss: 1.3897558\n",
      "Epoch: 446 Loss: 1.4715718\n",
      "Epoch: 447 Loss: 1.3969064\n",
      "Epoch: 448 Loss: 1.4013222\n",
      "Epoch: 449 Loss: 1.409606\n",
      "Epoch: 450 Loss: 1.4102335\n",
      "Epoch: 451 Loss: 1.3950933\n",
      "Epoch: 452 Loss: 1.3331624\n",
      "Epoch: 453 Loss: 1.4441048\n",
      "Epoch: 454 Loss: 1.2730361\n",
      "Epoch: 455 Loss: 1.3201836\n",
      "Epoch: 456 Loss: 1.3478311\n",
      "Epoch: 457 Loss: 1.3507048\n",
      "Epoch: 458 Loss: 1.3443128\n",
      "Epoch: 459 Loss: 1.2828679\n",
      "Epoch: 460 Loss: 1.3045021\n",
      "Epoch: 461 Loss: 1.4731792\n",
      "Epoch: 462 Loss: 1.4262909\n",
      "Epoch: 463 Loss: 1.4592413\n",
      "Epoch: 464 Loss: 1.3860782\n",
      "Epoch: 465 Loss: 1.3666004\n",
      "Epoch: 466 Loss: 1.3824544\n",
      "Epoch: 467 Loss: 1.3804792\n",
      "Epoch: 468 Loss: 1.4223728\n",
      "Epoch: 469 Loss: 1.2887914\n",
      "Epoch: 470 Loss: 1.3724519\n",
      "Epoch: 471 Loss: 1.1726297\n",
      "Epoch: 472 Loss: 1.2896962\n",
      "Epoch: 473 Loss: 1.2838248\n",
      "Epoch: 474 Loss: 1.3232414\n",
      "Epoch: 475 Loss: 1.2361722\n",
      "Epoch: 476 Loss: 1.3027562\n",
      "Epoch: 477 Loss: 1.1693524\n",
      "Epoch: 478 Loss: 1.2683539\n",
      "Epoch: 479 Loss: 1.1484487\n",
      "Epoch: 480 Loss: 1.1917682\n",
      "Epoch: 481 Loss: 1.4527925\n",
      "Epoch: 482 Loss: 1.4392081\n",
      "Epoch: 483 Loss: 1.4309384\n",
      "Epoch: 484 Loss: 1.4682493\n",
      "Epoch: 485 Loss: 1.4261342\n",
      "Epoch: 486 Loss: 1.525876\n",
      "Epoch: 487 Loss: 1.4396138\n",
      "Epoch: 488 Loss: 1.4444689\n",
      "Epoch: 489 Loss: 1.4408954\n",
      "Epoch: 490 Loss: 1.4193454\n",
      "Epoch: 491 Loss: 1.3197871\n",
      "Epoch: 492 Loss: 1.310404\n",
      "Epoch: 493 Loss: 1.4089404\n",
      "Epoch: 494 Loss: 1.269893\n",
      "Epoch: 495 Loss: 1.2557334\n",
      "Epoch: 496 Loss: 1.3352928\n",
      "Epoch: 497 Loss: 1.3004801\n",
      "Epoch: 498 Loss: 1.2782905\n",
      "Epoch: 499 Loss: 1.2490722\n",
      "Epoch: 500 Loss: 1.2696934\n",
      "Epoch: 501 Loss: 1.4704212\n",
      "Epoch: 502 Loss: 1.4196234\n",
      "Epoch: 503 Loss: 1.4535357\n",
      "Epoch: 504 Loss: 1.3821068\n",
      "Epoch: 505 Loss: 1.3474278\n",
      "Epoch: 506 Loss: 1.3949203\n",
      "Epoch: 507 Loss: 1.3801064\n",
      "Epoch: 508 Loss: 1.4023104\n",
      "Epoch: 509 Loss: 1.3122939\n",
      "Epoch: 510 Loss: 1.377214\n",
      "Epoch: 511 Loss: 1.1418273\n",
      "Epoch: 512 Loss: 1.3141998\n",
      "Epoch: 513 Loss: 1.2649622\n",
      "Epoch: 514 Loss: 1.2741901\n",
      "Epoch: 515 Loss: 1.2589444\n",
      "Epoch: 516 Loss: 1.3147901\n",
      "Epoch: 517 Loss: 1.1544471\n",
      "Epoch: 518 Loss: 1.2617798\n",
      "Epoch: 519 Loss: 1.1790656\n",
      "Epoch: 520 Loss: 1.1964881\n",
      "Epoch: 521 Loss: 1.3907903\n",
      "Epoch: 522 Loss: 1.4376801\n",
      "Epoch: 523 Loss: 1.4061679\n",
      "Epoch: 524 Loss: 1.3975321\n",
      "Epoch: 525 Loss: 1.3813051\n",
      "Epoch: 526 Loss: 1.4413813\n",
      "Epoch: 527 Loss: 1.3762039\n",
      "Epoch: 528 Loss: 1.3967613\n",
      "Epoch: 529 Loss: 1.4127232\n",
      "Epoch: 530 Loss: 1.3913121\n",
      "Epoch: 531 Loss: 1.3814778\n",
      "Epoch: 532 Loss: 1.3165082\n",
      "Epoch: 533 Loss: 1.4378649\n",
      "Epoch: 534 Loss: 1.2555848\n",
      "Epoch: 535 Loss: 1.301501\n",
      "Epoch: 536 Loss: 1.3138489\n",
      "Epoch: 537 Loss: 1.3579054\n",
      "Epoch: 538 Loss: 1.286523\n",
      "Epoch: 539 Loss: 1.2209195\n",
      "Epoch: 540 Loss: 1.2775187\n",
      "Epoch: 541 Loss: 1.4665604\n",
      "Epoch: 542 Loss: 1.3948298\n",
      "Epoch: 543 Loss: 1.4496855\n",
      "Epoch: 544 Loss: 1.3863395\n",
      "Epoch: 545 Loss: 1.2987996\n",
      "Epoch: 546 Loss: 1.3662561\n",
      "Epoch: 547 Loss: 1.3661394\n",
      "Epoch: 548 Loss: 1.4258461\n",
      "Epoch: 549 Loss: 1.2818986\n",
      "Epoch: 550 Loss: 1.3693882\n",
      "Epoch: 551 Loss: 1.1280013\n",
      "Epoch: 552 Loss: 1.3403488\n",
      "Epoch: 553 Loss: 1.2840177\n",
      "Epoch: 554 Loss: 1.2726661\n",
      "Epoch: 555 Loss: 1.2687105\n",
      "Epoch: 556 Loss: 1.3340153\n",
      "Epoch: 557 Loss: 1.1571811\n",
      "Epoch: 558 Loss: 1.2702979\n",
      "Epoch: 559 Loss: 1.1298748\n",
      "Epoch: 560 Loss: 1.2065085\n",
      "Epoch: 561 Loss: 1.4402561\n",
      "Epoch: 562 Loss: 1.4240574\n",
      "Epoch: 563 Loss: 1.4144506\n",
      "Epoch: 564 Loss: 1.4427191\n",
      "Epoch: 565 Loss: 1.4051286\n",
      "Epoch: 566 Loss: 1.468001\n",
      "Epoch: 567 Loss: 1.4091362\n",
      "Epoch: 568 Loss: 1.4183499\n",
      "Epoch: 569 Loss: 1.4019505\n",
      "Epoch: 570 Loss: 1.3795387\n",
      "Epoch: 571 Loss: 1.3120471\n",
      "Epoch: 572 Loss: 1.2568151\n",
      "Epoch: 573 Loss: 1.3926042\n",
      "Epoch: 574 Loss: 1.2270175\n",
      "Epoch: 575 Loss: 1.242975\n",
      "Epoch: 576 Loss: 1.299672\n",
      "Epoch: 577 Loss: 1.3134032\n",
      "Epoch: 578 Loss: 1.2518582\n",
      "Epoch: 579 Loss: 1.2632645\n",
      "Epoch: 580 Loss: 1.2450061\n",
      "Epoch: 581 Loss: 1.4771472\n",
      "Epoch: 582 Loss: 1.4237772\n",
      "Epoch: 583 Loss: 1.4296087\n",
      "Epoch: 584 Loss: 1.3895441\n",
      "Epoch: 585 Loss: 1.3651425\n",
      "Epoch: 586 Loss: 1.3581495\n",
      "Epoch: 587 Loss: 1.3775271\n",
      "Epoch: 588 Loss: 1.3852507\n",
      "Epoch: 589 Loss: 1.308511\n",
      "Epoch: 590 Loss: 1.3685609\n",
      "Epoch: 591 Loss: 1.2196847\n",
      "Epoch: 592 Loss: 1.3880271\n",
      "Epoch: 593 Loss: 1.3314239\n",
      "Epoch: 594 Loss: 1.2854123\n",
      "Epoch: 595 Loss: 1.2551988\n",
      "Epoch: 596 Loss: 1.294542\n",
      "Epoch: 597 Loss: 1.1626269\n",
      "Epoch: 598 Loss: 1.2884539\n",
      "Epoch: 599 Loss: 1.1645393\n",
      "Epoch: 600 Loss: 1.2155474\n",
      "Epoch: 601 Loss: 1.3538021\n",
      "Epoch: 602 Loss: 1.4638543\n",
      "Epoch: 603 Loss: 1.411035\n",
      "Epoch: 604 Loss: 1.3929701\n",
      "Epoch: 605 Loss: 1.3823367\n",
      "Epoch: 606 Loss: 1.4262117\n",
      "Epoch: 607 Loss: 1.3701162\n",
      "Epoch: 608 Loss: 1.3795806\n",
      "Epoch: 609 Loss: 1.3915113\n",
      "Epoch: 610 Loss: 1.3465637\n",
      "Epoch: 611 Loss: 1.3535558\n",
      "Epoch: 612 Loss: 1.307391\n",
      "Epoch: 613 Loss: 1.4311951\n",
      "Epoch: 614 Loss: 1.2505693\n",
      "Epoch: 615 Loss: 1.2815809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 616 Loss: 1.3226149\n",
      "Epoch: 617 Loss: 1.3576604\n",
      "Epoch: 618 Loss: 1.2742294\n",
      "Epoch: 619 Loss: 1.2304271\n",
      "Epoch: 620 Loss: 1.2896446\n",
      "Epoch: 621 Loss: 1.5077451\n",
      "Epoch: 622 Loss: 1.3989416\n",
      "Epoch: 623 Loss: 1.4130138\n",
      "Epoch: 624 Loss: 1.3779248\n",
      "Epoch: 625 Loss: 1.3087617\n",
      "Epoch: 626 Loss: 1.3747567\n",
      "Epoch: 627 Loss: 1.3434759\n",
      "Epoch: 628 Loss: 1.3690033\n",
      "Epoch: 629 Loss: 1.2837454\n",
      "Epoch: 630 Loss: 1.3572639\n",
      "Epoch: 631 Loss: 1.13877\n",
      "Epoch: 632 Loss: 1.2873412\n",
      "Epoch: 633 Loss: 1.2347803\n",
      "Epoch: 634 Loss: 1.2556905\n",
      "Epoch: 635 Loss: 1.2140104\n",
      "Epoch: 636 Loss: 1.2804316\n",
      "Epoch: 637 Loss: 1.1283599\n",
      "Epoch: 638 Loss: 1.2590669\n",
      "Epoch: 639 Loss: 1.1057976\n",
      "Epoch: 640 Loss: 1.1547545\n",
      "Epoch: 641 Loss: 1.3802239\n",
      "Epoch: 642 Loss: 1.4510784\n",
      "Epoch: 643 Loss: 1.3676715\n",
      "Epoch: 644 Loss: 1.425223\n",
      "Epoch: 645 Loss: 1.380006\n",
      "Epoch: 646 Loss: 1.4575762\n",
      "Epoch: 647 Loss: 1.4023453\n",
      "Epoch: 648 Loss: 1.3938264\n",
      "Epoch: 649 Loss: 1.402716\n",
      "Epoch: 650 Loss: 1.3882693\n",
      "Epoch: 651 Loss: 1.3074849\n",
      "Epoch: 652 Loss: 1.235497\n",
      "Epoch: 653 Loss: 1.3896586\n",
      "Epoch: 654 Loss: 1.2176226\n",
      "Epoch: 655 Loss: 1.21324\n",
      "Epoch: 656 Loss: 1.2463151\n",
      "Epoch: 657 Loss: 1.2902842\n",
      "Epoch: 658 Loss: 1.2110347\n",
      "Epoch: 659 Loss: 1.1816311\n",
      "Epoch: 660 Loss: 1.2591368\n",
      "Epoch: 661 Loss: 1.4498605\n",
      "Epoch: 662 Loss: 1.3598003\n",
      "Epoch: 663 Loss: 1.3870646\n",
      "Epoch: 664 Loss: 1.3544031\n",
      "Epoch: 665 Loss: 1.296007\n",
      "Epoch: 666 Loss: 1.3560416\n",
      "Epoch: 667 Loss: 1.3366541\n",
      "Epoch: 668 Loss: 1.377852\n",
      "Epoch: 669 Loss: 1.2595001\n",
      "Epoch: 670 Loss: 1.3149865\n",
      "Epoch: 671 Loss: 1.1582779\n",
      "Epoch: 672 Loss: 1.3259552\n",
      "Epoch: 673 Loss: 1.3057442\n",
      "Epoch: 674 Loss: 1.2242271\n",
      "Epoch: 675 Loss: 1.2943106\n",
      "Epoch: 676 Loss: 1.2793745\n",
      "Epoch: 677 Loss: 1.1300443\n",
      "Epoch: 678 Loss: 1.2380387\n",
      "Epoch: 679 Loss: 1.0866979\n",
      "Epoch: 680 Loss: 1.160004\n",
      "Epoch: 681 Loss: 1.3974754\n",
      "Epoch: 682 Loss: 1.416552\n",
      "Epoch: 683 Loss: 1.3883947\n",
      "Epoch: 684 Loss: 1.466767\n",
      "Epoch: 685 Loss: 1.3669809\n",
      "Epoch: 686 Loss: 1.4384557\n",
      "Epoch: 687 Loss: 1.38172\n",
      "Epoch: 688 Loss: 1.4231831\n",
      "Epoch: 689 Loss: 1.3834606\n",
      "Epoch: 690 Loss: 1.3650967\n",
      "Epoch: 691 Loss: 1.3110491\n",
      "Epoch: 692 Loss: 1.293762\n",
      "Epoch: 693 Loss: 1.4350783\n",
      "Epoch: 694 Loss: 1.2581831\n",
      "Epoch: 695 Loss: 1.2731184\n",
      "Epoch: 696 Loss: 1.3296486\n",
      "Epoch: 697 Loss: 1.3412527\n",
      "Epoch: 698 Loss: 1.2479315\n",
      "Epoch: 699 Loss: 1.2507519\n",
      "Epoch: 700 Loss: 1.2536755\n",
      "Epoch: 701 Loss: 1.5035721\n",
      "Epoch: 702 Loss: 1.3760867\n",
      "Epoch: 703 Loss: 1.3827912\n",
      "Epoch: 704 Loss: 1.3736573\n",
      "Epoch: 705 Loss: 1.3002411\n",
      "Epoch: 706 Loss: 1.3564411\n",
      "Epoch: 707 Loss: 1.3391534\n",
      "Epoch: 708 Loss: 1.3531231\n",
      "Epoch: 709 Loss: 1.2586789\n",
      "Epoch: 710 Loss: 1.3277394\n",
      "Epoch: 711 Loss: 1.292606\n",
      "Epoch: 712 Loss: 1.4287472\n",
      "Epoch: 713 Loss: 1.3744177\n",
      "Epoch: 714 Loss: 1.3161153\n",
      "Epoch: 715 Loss: 1.2431717\n",
      "Epoch: 716 Loss: 1.3140037\n",
      "Epoch: 717 Loss: 1.1549908\n",
      "Epoch: 718 Loss: 1.289548\n",
      "Epoch: 719 Loss: 1.114911\n",
      "Epoch: 720 Loss: 1.211197\n",
      "Epoch: 721 Loss: 1.3804115\n",
      "Epoch: 722 Loss: 1.4786159\n",
      "Epoch: 723 Loss: 1.4274272\n",
      "Epoch: 724 Loss: 1.4181948\n",
      "Epoch: 725 Loss: 1.3965197\n",
      "Epoch: 726 Loss: 1.4099423\n",
      "Epoch: 727 Loss: 1.3438705\n",
      "Epoch: 728 Loss: 1.3805088\n",
      "Epoch: 729 Loss: 1.3821454\n",
      "Epoch: 730 Loss: 1.3871769\n",
      "Epoch: 731 Loss: 1.3259562\n",
      "Epoch: 732 Loss: 1.3507334\n",
      "Epoch: 733 Loss: 1.4570681\n",
      "Epoch: 734 Loss: 1.2356975\n",
      "Epoch: 735 Loss: 1.2374314\n",
      "Epoch: 736 Loss: 1.3411747\n",
      "Epoch: 737 Loss: 1.4022989\n",
      "Epoch: 738 Loss: 1.2782282\n",
      "Epoch: 739 Loss: 1.2362612\n",
      "Epoch: 740 Loss: 1.2667369\n",
      "Epoch: 741 Loss: 1.4710168\n",
      "Epoch: 742 Loss: 1.4382731\n",
      "Epoch: 743 Loss: 1.3808312\n",
      "Epoch: 744 Loss: 1.3235852\n",
      "Epoch: 745 Loss: 1.3180867\n",
      "Epoch: 746 Loss: 1.3986567\n",
      "Epoch: 747 Loss: 1.3669113\n",
      "Epoch: 748 Loss: 1.3572855\n",
      "Epoch: 749 Loss: 1.2812737\n",
      "Epoch: 750 Loss: 1.3197187\n",
      "Epoch: 751 Loss: 1.1651478\n",
      "Epoch: 752 Loss: 1.3371853\n",
      "Epoch: 753 Loss: 1.273094\n",
      "Epoch: 754 Loss: 1.2343512\n",
      "Epoch: 755 Loss: 1.2300946\n",
      "Epoch: 756 Loss: 1.3054279\n",
      "Epoch: 757 Loss: 1.1361771\n",
      "Epoch: 758 Loss: 1.2784148\n",
      "Epoch: 759 Loss: 1.1341847\n",
      "Epoch: 760 Loss: 1.1259942\n",
      "Epoch: 761 Loss: 1.3534431\n",
      "Epoch: 762 Loss: 1.5233804\n",
      "Epoch: 763 Loss: 1.4054645\n",
      "Epoch: 764 Loss: 1.444899\n",
      "Epoch: 765 Loss: 1.403521\n",
      "Epoch: 766 Loss: 1.5546079\n",
      "Epoch: 767 Loss: 1.3986667\n",
      "Epoch: 768 Loss: 1.3453783\n",
      "Epoch: 769 Loss: 1.4042472\n",
      "Epoch: 770 Loss: 1.3649565\n",
      "Epoch: 771 Loss: 1.3133268\n",
      "Epoch: 772 Loss: 1.2454782\n",
      "Epoch: 773 Loss: 1.3967406\n",
      "Epoch: 774 Loss: 1.2418402\n",
      "Epoch: 775 Loss: 1.2178789\n",
      "Epoch: 776 Loss: 1.2791268\n",
      "Epoch: 777 Loss: 1.2931336\n",
      "Epoch: 778 Loss: 1.2249092\n",
      "Epoch: 779 Loss: 1.1733427\n",
      "Epoch: 780 Loss: 1.2841201\n",
      "Epoch: 781 Loss: 1.4642887\n",
      "Epoch: 782 Loss: 1.3092402\n",
      "Epoch: 783 Loss: 1.360199\n",
      "Epoch: 784 Loss: 1.3399057\n",
      "Epoch: 785 Loss: 1.260938\n",
      "Epoch: 786 Loss: 1.3177146\n",
      "Epoch: 787 Loss: 1.3077756\n",
      "Epoch: 788 Loss: 1.368605\n",
      "Epoch: 789 Loss: 1.2301422\n",
      "Epoch: 790 Loss: 1.3101228\n",
      "Epoch: 791 Loss: 1.1250285\n",
      "Epoch: 792 Loss: 1.3068973\n",
      "Epoch: 793 Loss: 1.2467109\n",
      "Epoch: 794 Loss: 1.2565404\n",
      "Epoch: 795 Loss: 1.247938\n",
      "Epoch: 796 Loss: 1.2721907\n",
      "Epoch: 797 Loss: 1.1434811\n",
      "Epoch: 798 Loss: 1.2602097\n",
      "Epoch: 799 Loss: 1.1018814\n",
      "Epoch: 800 Loss: 1.2170638\n",
      "Epoch: 801 Loss: 1.4441592\n",
      "Epoch: 802 Loss: 1.4403458\n",
      "Epoch: 803 Loss: 1.4110187\n",
      "Epoch: 804 Loss: 1.4185454\n",
      "Epoch: 805 Loss: 1.3962299\n",
      "Epoch: 806 Loss: 1.3820273\n",
      "Epoch: 807 Loss: 1.3524647\n",
      "Epoch: 808 Loss: 1.3766346\n",
      "Epoch: 809 Loss: 1.3810915\n",
      "Epoch: 810 Loss: 1.3439876\n",
      "Epoch: 811 Loss: 1.2622871\n",
      "Epoch: 812 Loss: 1.2524146\n",
      "Epoch: 813 Loss: 1.3384506\n",
      "Epoch: 814 Loss: 1.192799\n",
      "Epoch: 815 Loss: 1.225201\n",
      "Epoch: 816 Loss: 1.3123727\n",
      "Epoch: 817 Loss: 1.3434461\n",
      "Epoch: 818 Loss: 1.2006615\n",
      "Epoch: 819 Loss: 1.2255493\n",
      "Epoch: 820 Loss: 1.2028109\n",
      "Epoch: 821 Loss: 1.4626251\n",
      "Epoch: 822 Loss: 1.3583888\n",
      "Epoch: 823 Loss: 1.378901\n",
      "Epoch: 824 Loss: 1.337038\n",
      "Epoch: 825 Loss: 1.3228477\n",
      "Epoch: 826 Loss: 1.3666285\n",
      "Epoch: 827 Loss: 1.3369733\n",
      "Epoch: 828 Loss: 1.328059\n",
      "Epoch: 829 Loss: 1.241784\n",
      "Epoch: 830 Loss: 1.3253261\n",
      "Epoch: 831 Loss: 1.1625148\n",
      "Epoch: 832 Loss: 1.3888625\n",
      "Epoch: 833 Loss: 1.2971537\n",
      "Epoch: 834 Loss: 1.2047616\n",
      "Epoch: 835 Loss: 1.2350295\n",
      "Epoch: 836 Loss: 1.2920753\n",
      "Epoch: 837 Loss: 1.1395818\n",
      "Epoch: 838 Loss: 1.2777058\n",
      "Epoch: 839 Loss: 1.093387\n",
      "Epoch: 840 Loss: 1.1846122\n",
      "Epoch: 841 Loss: 1.3091491\n",
      "Epoch: 842 Loss: 1.527887\n",
      "Epoch: 843 Loss: 1.3915949\n",
      "Epoch: 844 Loss: 1.3964639\n",
      "Epoch: 845 Loss: 1.3779702\n",
      "Epoch: 846 Loss: 1.459178\n",
      "Epoch: 847 Loss: 1.2985779\n",
      "Epoch: 848 Loss: 1.3624258\n",
      "Epoch: 849 Loss: 1.3599874\n",
      "Epoch: 850 Loss: 1.3333306\n",
      "Epoch: 851 Loss: 1.2780907\n",
      "Epoch: 852 Loss: 1.2407554\n",
      "Epoch: 853 Loss: 1.3777448\n",
      "Epoch: 854 Loss: 1.2260083\n",
      "Epoch: 855 Loss: 1.2635881\n",
      "Epoch: 856 Loss: 1.3120754\n",
      "Epoch: 857 Loss: 1.3416777\n",
      "Epoch: 858 Loss: 1.2521144\n",
      "Epoch: 859 Loss: 1.2165335\n",
      "Epoch: 860 Loss: 1.2742544\n",
      "Epoch: 861 Loss: 1.4712614\n",
      "Epoch: 862 Loss: 1.3473163\n",
      "Epoch: 863 Loss: 1.3296579\n",
      "Epoch: 864 Loss: 1.3489981\n",
      "Epoch: 865 Loss: 1.2625946\n",
      "Epoch: 866 Loss: 1.3405544\n",
      "Epoch: 867 Loss: 1.3180984\n",
      "Epoch: 868 Loss: 1.383102\n",
      "Epoch: 869 Loss: 1.2184831\n",
      "Epoch: 870 Loss: 1.3026994\n",
      "Epoch: 871 Loss: 1.0404046\n",
      "Epoch: 872 Loss: 1.2524506\n",
      "Epoch: 873 Loss: 1.1926783\n",
      "Epoch: 874 Loss: 1.1545094\n",
      "Epoch: 875 Loss: 1.1324791\n",
      "Epoch: 876 Loss: 1.2484674\n",
      "Epoch: 877 Loss: 1.0968332\n",
      "Epoch: 878 Loss: 1.1848431\n",
      "Epoch: 879 Loss: 1.0492792\n",
      "Epoch: 880 Loss: 1.1644043\n",
      "Epoch: 881 Loss: 1.3301072\n",
      "Epoch: 882 Loss: 1.4702829\n",
      "Epoch: 883 Loss: 1.3924717\n",
      "Epoch: 884 Loss: 1.3888226\n",
      "Epoch: 885 Loss: 1.3908329\n",
      "Epoch: 886 Loss: 1.4202899\n",
      "Epoch: 887 Loss: 1.3473539\n",
      "Epoch: 888 Loss: 1.3381624\n",
      "Epoch: 889 Loss: 1.3662776\n",
      "Epoch: 890 Loss: 1.3635699\n",
      "Epoch: 891 Loss: 1.2426548\n",
      "Epoch: 892 Loss: 1.212685\n",
      "Epoch: 893 Loss: 1.2775229\n",
      "Epoch: 894 Loss: 1.1646506\n",
      "Epoch: 895 Loss: 1.135888\n",
      "Epoch: 896 Loss: 1.2370003\n",
      "Epoch: 897 Loss: 1.2739038\n",
      "Epoch: 898 Loss: 1.1611428\n",
      "Epoch: 899 Loss: 1.1435299\n",
      "Epoch: 900 Loss: 1.2139353\n",
      "Epoch: 901 Loss: 1.4000078\n",
      "Epoch: 902 Loss: 1.2891648\n",
      "Epoch: 903 Loss: 1.2814137\n",
      "Epoch: 904 Loss: 1.310423\n",
      "Epoch: 905 Loss: 1.235444\n",
      "Epoch: 906 Loss: 1.2844592\n",
      "Epoch: 907 Loss: 1.268803\n",
      "Epoch: 908 Loss: 1.3370304\n",
      "Epoch: 909 Loss: 1.1980186\n",
      "Epoch: 910 Loss: 1.2830153\n",
      "Epoch: 911 Loss: 1.0225767\n",
      "Epoch: 912 Loss: 1.2637769\n",
      "Epoch: 913 Loss: 1.1867427\n",
      "Epoch: 914 Loss: 1.1248416\n",
      "Epoch: 915 Loss: 1.1301923\n",
      "Epoch: 916 Loss: 1.2138764\n",
      "Epoch: 917 Loss: 1.0781714\n",
      "Epoch: 918 Loss: 1.1406044\n",
      "Epoch: 919 Loss: 1.0054532\n",
      "Epoch: 920 Loss: 1.1477824\n",
      "Epoch: 921 Loss: 1.3582917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 922 Loss: 1.4446793\n",
      "Epoch: 923 Loss: 1.367886\n",
      "Epoch: 924 Loss: 1.3973771\n",
      "Epoch: 925 Loss: 1.376578\n",
      "Epoch: 926 Loss: 1.3759985\n",
      "Epoch: 927 Loss: 1.3392767\n",
      "Epoch: 928 Loss: 1.3446951\n",
      "Epoch: 929 Loss: 1.3332747\n",
      "Epoch: 930 Loss: 1.3331488\n",
      "Epoch: 931 Loss: 1.2406961\n",
      "Epoch: 932 Loss: 1.1870726\n",
      "Epoch: 933 Loss: 1.2694283\n",
      "Epoch: 934 Loss: 1.1398345\n",
      "Epoch: 935 Loss: 1.1658357\n",
      "Epoch: 936 Loss: 1.2553042\n",
      "Epoch: 937 Loss: 1.2879639\n",
      "Epoch: 938 Loss: 1.148333\n",
      "Epoch: 939 Loss: 1.1609797\n",
      "Epoch: 940 Loss: 1.1967245\n",
      "Epoch: 941 Loss: 1.4417938\n",
      "Epoch: 942 Loss: 1.3122683\n",
      "Epoch: 943 Loss: 1.3197153\n",
      "Epoch: 944 Loss: 1.2993752\n",
      "Epoch: 945 Loss: 1.3003615\n",
      "Epoch: 946 Loss: 1.3240132\n",
      "Epoch: 947 Loss: 1.2966242\n",
      "Epoch: 948 Loss: 1.2969718\n",
      "Epoch: 949 Loss: 1.2173263\n",
      "Epoch: 950 Loss: 1.2738047\n",
      "Epoch: 951 Loss: 1.0547949\n",
      "Epoch: 952 Loss: 1.3322738\n",
      "Epoch: 953 Loss: 1.2231416\n",
      "Epoch: 954 Loss: 1.1523916\n",
      "Epoch: 955 Loss: 1.1437947\n",
      "Epoch: 956 Loss: 1.2130675\n",
      "Epoch: 957 Loss: 1.083025\n",
      "Epoch: 958 Loss: 1.1299263\n",
      "Epoch: 959 Loss: 0.9941872\n",
      "Epoch: 960 Loss: 1.1764872\n",
      "Epoch: 961 Loss: 1.2748226\n",
      "Epoch: 962 Loss: 1.4496552\n",
      "Epoch: 963 Loss: 1.3502617\n",
      "Epoch: 964 Loss: 1.3457648\n",
      "Epoch: 965 Loss: 1.3591532\n",
      "Epoch: 966 Loss: 1.3630542\n",
      "Epoch: 967 Loss: 1.3166083\n",
      "Epoch: 968 Loss: 1.3371493\n",
      "Epoch: 969 Loss: 1.3503577\n",
      "Epoch: 970 Loss: 1.3549374\n",
      "Epoch: 971 Loss: 1.2305708\n",
      "Epoch: 972 Loss: 1.1834288\n",
      "Epoch: 973 Loss: 1.3317875\n",
      "Epoch: 974 Loss: 1.2043072\n",
      "Epoch: 975 Loss: 1.2061764\n",
      "Epoch: 976 Loss: 1.2805324\n",
      "Epoch: 977 Loss: 1.2425066\n",
      "Epoch: 978 Loss: 1.1934268\n",
      "Epoch: 979 Loss: 1.1158389\n",
      "Epoch: 980 Loss: 1.1840644\n",
      "Epoch: 981 Loss: 1.4177927\n",
      "Epoch: 982 Loss: 1.3100734\n",
      "Epoch: 983 Loss: 1.2982951\n",
      "Epoch: 984 Loss: 1.2752222\n",
      "Epoch: 985 Loss: 1.2384106\n",
      "Epoch: 986 Loss: 1.3147069\n",
      "Epoch: 987 Loss: 1.2326608\n",
      "Epoch: 988 Loss: 1.3296624\n",
      "Epoch: 989 Loss: 1.2265602\n",
      "Epoch: 990 Loss: 1.3045675\n",
      "Epoch: 991 Loss: 1.0673842\n",
      "Epoch: 992 Loss: 1.3433743\n",
      "Epoch: 993 Loss: 1.2302275\n",
      "Epoch: 994 Loss: 1.1298909\n",
      "Epoch: 995 Loss: 1.0790871\n",
      "Epoch: 996 Loss: 1.2569212\n",
      "Epoch: 997 Loss: 1.0700392\n",
      "Epoch: 998 Loss: 1.1823251\n",
      "Epoch: 999 Loss: 1.0450839\n",
      "Epoch: 1000 Loss: 1.1953743\n"
     ]
    }
   ],
   "source": [
    "# Setting Batch with Training\n",
    "batch_size = 48\n",
    "epoch = 1000\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('tf_board', sess.graph)\n",
    "    for i in range(epoch):\n",
    "        batch_data, batch_label = batch(trainlist, batch_size)     \n",
    "        _, loss, summary = sess.run([train_step, Loss, merged], feed_dict = {X: batch_data, Y: batch_label})\n",
    "        print(\"Epoch:\",i+1,\"Loss:\",loss)\n",
    "        if i % 10 == 0:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)\n",
    "        elif i+1 == epoch:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs\\model.ckpt-1000\n",
      "[-0.28698385 -0.321227    0.6881922  -1.2158612 ] 0\n",
      "[0.38163096 0.57300365 1.2239884  1.7631562 ] 0\n",
      "[-0.38732666  0.11303855 -0.66456974 -0.4872252 ] 0\n",
      "[ 0.27771845 -0.92172414  0.8993701   1.9032156 ] 0\n",
      "[0.41678143 0.6598618  0.32812378 1.4228878 ] 0\n",
      "[-0.8118152  3.362328  -1.3685751 -2.3973255] 0\n",
      "[-0.8321893  -1.9414852   0.10708249  0.16838491] 0\n",
      "[-0.34275898  0.9142803  -0.5197569  -3.1513398 ] 0\n",
      "[-1.0745963  -2.239171    0.06340072  0.24595931] 0\n",
      "[-0.58498126  1.5430027   0.45462608 -0.2618784 ] 0\n",
      "[-1.0393459  -1.7565814  -0.00444157  0.5721166 ] 0\n",
      "[-0.97900563 -0.98945546  0.4478366  -2.2484117 ] 0\n",
      "[-8.1724495e-01 -1.9470155e-03 -2.6080048e-01 -2.7088356e+00] 0\n",
      "[-0.45428878  1.4320624   0.43006253 -2.1133294 ] 0\n",
      "[-1.0395371  -0.7818316  -0.41779605 -1.6435171 ] 0\n",
      "[ 0.11837521 -1.2139616   1.4864583   0.9747925 ] 0\n",
      "[-0.04425101 -1.1167481   1.6856391   1.2936469 ] 0\n",
      "[-0.24951127 -1.5698161   1.9690194   2.0473669 ] 0\n",
      "[-0.00163024  0.03983807 -0.81603545 -0.3611617 ] 0\n",
      "[ 0.01192349 -0.84967154  0.5304139   0.47380885] 0\n",
      "[-0.6395875  -0.16716771 -1.1658126  -1.1856517 ] 0\n",
      "[ 1.7284666 -0.5785543  0.6000104  1.9666698] 0\n",
      "[ 0.9955675  -0.6341301   0.28684416  1.8982649 ] 0\n",
      "[-0.89770246 -1.4272989  -0.33198678 -0.77092767] 0\n",
      "[ 0.36412567 -0.02375558  0.740499    1.6398402 ] 0\n",
      "[ 0.3142566  -1.2322066   0.41013125  0.43458214] 0\n",
      "[0.24446887 0.5090447  1.0792032  0.2668937 ] 0\n",
      "[ 0.5914673  -1.9476776   0.11362881  0.67936146] 0\n",
      "[ 0.8692912  -1.4208565  -0.16575056  1.2339072 ] 0\n",
      "[ 1.2105387  -0.38454872  1.0726014   1.5292267 ] 0\n",
      "[-0.00564489  0.22321829  1.1621699   1.0388559 ] 0\n",
      "[-0.68289536 -0.8637557  -0.3111189  -2.055579  ] 0\n",
      "[-0.04609097 -0.30826336  1.4765644  -0.07669441] 0\n",
      "[-0.29046774  0.32954267 -0.5285614   1.8964262 ] 0\n",
      "[ 0.78891665 -0.57301795 -0.46401086  1.2026578 ] 0\n",
      "[ 1.4824233 -1.2514971  1.0748267  1.731733 ] 0\n",
      "[-0.11638811 -0.63256335  0.21972528 -0.22078213] 0\n",
      "[-0.26574153 -0.0279459   0.8749678  -0.14965744] 0\n",
      "[-0.98346174  0.08822438 -0.95209074 -3.245192  ] 0\n",
      "[-0.86981016 -1.9332209  -0.59880036 -0.48040047] 0\n",
      "[-1.6158713  -0.16716664 -0.12355472 -0.63615894] 0\n",
      "[-0.11296561  0.45590958  0.4802623  -0.7733636 ] 0\n",
      "[ 0.40552416  0.72404194 -0.6596342   0.2597722 ] 0\n",
      "[-0.05086084  0.33109915  0.4413444   0.39550582] 0\n",
      "[ 0.7580226  -1.5937628  -0.97940105  1.374732  ] 0\n",
      "[-0.4825108  -1.9030075  -0.2464521   0.75083137] 0\n",
      "[-0.9532947   2.3853705  -0.09070217 -0.20191169] 0\n",
      "[ 0.34836742  0.14360233 -0.66816545  1.7726907 ] 0\n",
      "[-0.6538907  -0.30690405  0.82280105 -1.6339649 ] 0\n",
      "[-0.4954711  -0.42894554  1.5911034   1.9639685 ] 0\n",
      "[ 0.43776554 -0.42015186 -0.5289301   0.9577302 ] 0\n",
      "[0.8552057  1.2529317  0.17359388 1.324089  ] 0\n",
      "[ 0.41043252 -1.1394901  -1.396008    0.54313326] 0\n",
      "[-0.38832873 -2.3634927  -0.44843554 -0.3751722 ] 0\n",
      "[ 0.1152257  -1.0388312   0.7934724   0.11619067] 0\n",
      "[-0.7292246   0.26495123  2.0514245  -0.96575725] 0\n",
      "[ 0.22560392 -0.50095224 -0.35736737  1.5054655 ] 0\n",
      "[-0.89327544 -1.1882465   0.63875383  0.19287303] 0\n",
      "[-0.60627866  0.28244847 -1.0155351  -3.035635  ] 0\n",
      "[-0.2593664   0.25305435  0.63580376 -0.23335952] 0\n",
      "[-0.66050726 -1.4970245   0.7960504  -0.6515541 ] 0\n",
      "[ 0.23052162 -0.00547726  0.55363846 -0.6333399 ] 0\n",
      "[ 1.1632723 -0.46684    2.353326   1.9280076] 0\n",
      "[ 0.80860966 -2.2469401   0.46173963  1.0449212 ] 0\n",
      "[-1.0627506  -0.64337003 -0.10616642 -2.0492811 ] 0\n",
      "[-0.6001436  -0.39766833  0.17482379 -1.3123243 ] 0\n",
      "[0.4196629  0.33720806 0.6902308  1.5900404 ] 0\n",
      "[-0.58332056  0.92218614  1.0457395  -1.1139431 ] 0\n",
      "[-0.32339716  0.27466083  0.5340553   2.1235716 ] 0\n",
      "[-0.886391   0.9830694 -0.5623437 -1.4735657] 0\n",
      "[ 0.2592648   0.9262292  -0.21899644 -0.27508295] 0\n",
      "[ 0.8286971   1.0036608  -0.57846063 -0.79635155] 0\n",
      "[ 0.31384295 -0.12636733 -0.46575546  1.0213324 ] 0\n",
      "[-1.4023379  -0.2124394  -0.47661057  0.01120435] 0\n",
      "[-0.20130219  0.5246453  -0.51886964  0.11351901] 0\n",
      "[-0.14189377  0.46617597  1.083931   -0.71192575] 0\n",
      "[-1.6973588  -0.81493115 -0.66958606 -2.2796378 ] 0\n",
      "[-0.30825222 -0.5568632  -0.84752035  0.5411304 ] 0\n",
      "[ 0.13672844 -1.5041579  -0.48673686  0.78138256] 0\n",
      "[ 0.6546979  2.369247  -1.1655567 -1.4297284] 0\n",
      "[-0.62858146 -2.0966117   0.5956095  -0.9740939 ] 0\n",
      "[ 0.81788087 -1.1657342   0.8413686   1.2746632 ] 0\n",
      "[-0.02037572  0.5434347   1.3032721   1.2558515 ] 0\n",
      "[ 0.0205383   0.62366325  0.710321   -0.0321086 ] 0\n",
      "[ 1.65556     1.8339593   0.23413518 -0.76636744] 0\n",
      "[ 0.5445022   0.2915497  -0.17863628  1.3995577 ] 0\n",
      "[-0.05673945  1.9486101  -0.4683259   0.5856987 ] 0\n",
      "[ 0.02629479  1.7161549  -0.6679382  -2.0760946 ] 0\n",
      "[-0.39565384 -0.3430266   0.24368656 -0.9861287 ] 0\n",
      "[-0.25913107 -0.81984264  0.8141334  -0.03343125] 0\n",
      "[-1.3813574  -0.67546517  0.87430084  0.5555266 ] 0\n",
      "[ 0.20261893 -0.97444475  1.1285584  -0.09244055] 0\n",
      "[ 0.03402963 -0.9644455  -0.12124559  1.1200591 ] 0\n",
      "[-0.20488846  0.13244215  0.6570014   1.9086993 ] 0\n",
      "[-0.61691326  0.51892227  1.837099   -1.1296914 ] 0\n",
      "[ 0.43373016  0.5251119  -0.03786505 -0.89411354] 0\n",
      "[-0.20449428 -0.35658044 -0.10248422 -0.1044222 ] 0\n",
      "[-0.8686932 -1.440948  -1.1656363  0.8800926] 0\n",
      "[ 0.12872139 -0.84350085 -0.38510066  1.0048972 ] 0\n",
      "[-0.007395   -3.1423469  -0.46822587  0.62756205] 0\n",
      "[0.96048075 0.3823513  0.77899075 1.918194  ] 0\n",
      "[-0.3223644 -0.8574465  0.6183889  0.6472124] 0\n",
      "[ 0.43200347 -0.18966965 -0.9782369   0.7058462 ] 0\n",
      "[ 1.0119895  -0.4400024  -0.15715256  2.7269177 ] 0\n",
      "[-0.39468116  1.2684087  -0.05681159 -3.8042467 ] 0\n",
      "[ 0.00428371 -0.48587665  0.6264927   1.2333928 ] 0\n",
      "[ 0.55158824  0.31755573 -0.6354977   1.2439123 ] 0\n",
      "[-1.4665133  -1.718128   -0.29198995  0.6539819 ] 0\n",
      "[ 0.00645901  0.06473461  1.416233   -0.01756871] 0\n",
      "[-0.3717757  1.1504239 -0.5987999 -2.552962 ] 0\n",
      "[0.31878006 1.0128473  0.9006971  0.54979813] 0\n",
      "[-0.208947   0.1083228 -0.7286643 -0.8728757] 0\n",
      "[-0.10079943  1.4108006  -0.6834519   0.07281157] 0\n",
      "[1.2381629  0.21239085 0.83767647 1.3140202 ] 0\n",
      "[-0.45642984  1.6309468   0.6235848   1.5408541 ] 0\n",
      "[-1.3541963  -0.6689103   0.43540037 -2.8766541 ] 0\n",
      "[-0.07646553  0.09671405  0.9297489   1.0415891 ] 0\n",
      "[0.8217749 1.2513709 0.6379788 0.7658551] 0\n",
      "[0.50645053 0.85618716 0.58589995 1.6369736 ] 0\n",
      "[-0.31877086  0.17189181 -0.36811113  1.9735572 ] 0\n",
      "[-0.41165787 -1.7941573  -0.32022122 -1.4852598 ] 0\n",
      "[-0.12038566 -1.0721817   1.34924     0.973873  ] 0\n",
      "[-2.0450997   0.0545937  -0.29933116 -2.611241  ] 0\n",
      "[ 0.20137231  0.42402416  0.42187488 -0.00468123] 0\n",
      "[-0.36404043 -0.29360747 -0.7707181  -2.4358742 ] 0\n",
      "[-0.7561802  -0.06349564 -0.31328303 -2.59181   ] 0\n",
      "[-0.6922719  -1.5923632  -0.7172781   0.05335143] 0\n",
      "[-1.2445939  -0.76288426 -0.60486656 -0.6568575 ] 0\n",
      "[-0.49218193 -0.63473433  0.7689652   1.6037931 ] 0\n",
      "[ 0.33586243  1.6409003  -0.18382534 -2.1646993 ] 0\n",
      "[ 0.3275908 -1.5345819  0.8637835  0.9031757] 0\n",
      "[ 0.3680823   1.0760404   0.07727375 -0.26203257] 0\n",
      "[ 0.6105367  -0.07721242 -0.10061249  1.2791678 ] 0\n",
      "[-0.6491993 -1.206551  -0.5272356 -2.3401449] 0\n",
      "[ 0.9211005   1.0243345  -0.22220233  0.7837641 ] 0\n",
      "[-0.01229824 -0.13498662  0.10391443 -0.6631621 ] 0\n",
      "[ 0.46843138 -0.767507    1.0130076   1.678231  ] 0\n",
      "[-0.43979096 -0.45965618  0.01272255  1.4854134 ] 0\n",
      "[ 0.6208283   0.22467615 -0.1109164   0.7870848 ] 0\n",
      "[-1.5220935  -0.5649161   0.5274888  -0.03318657] 0\n",
      "[0.1831307 1.4022167 1.2867806 1.8914008] 0\n",
      "[-0.25725493 -0.19511953 -0.6186096   1.1872256 ] 0\n",
      "[-0.87783694 -0.661293    0.68061477 -0.38291675] 1\n",
      "[-0.3987703  -0.10316254 -0.67013985  1.1923376 ] 1\n",
      "[ 0.21709402 -0.0890667   1.1461835   2.4046333 ] 1\n",
      "[-0.8363833  -0.4225805  -0.14153951  1.0016903 ] 1\n",
      "[-0.33991876 -2.0980732   0.3734773   0.37538746] 1\n",
      "[ 0.9376713  -0.42035046 -0.26864648  2.6448443 ] 1\n",
      "[-0.77983236  0.1326192   0.2023553  -1.6041267 ] 1\n",
      "[-0.15097083  0.8575539   1.4556175   0.3087174 ] 1\n",
      "[ 0.49451947 -1.0654738  -0.6787985   0.37781748] 1\n",
      "[ 0.47891846 -0.41928264 -0.20175767  0.7226099 ] 1\n",
      "[ 0.07774523 -0.2804721   0.7478882   1.5800601 ] 1\n",
      "[0.8457995  0.4300873  0.19581562 2.2956216 ] 1\n",
      "[-1.6465404 -1.2250646  0.7403064 -1.4477023] 1\n",
      "[ 1.2852914   0.9087901  -0.8758867   0.40470573] 1\n",
      "[-0.01574201 -0.11263558  0.54682374  0.43077603] 1\n",
      "[ 0.00935435 -1.0962455  -0.69593704  0.8630277 ] 1\n",
      "[ 0.36672905 -0.14100796  1.4768778   2.5337691 ] 1\n",
      "[ 0.18126051 -1.8278074  -0.01713272  0.55208695] 1\n",
      "[ 0.07983191 -0.45579192 -0.51629806 -0.5621692 ] 1\n",
      "[ 0.5766773 -1.069959   1.0191115  2.5057554] 1\n",
      "[-0.30006343 -0.54505396  1.4442561   0.1283884 ] 1\n",
      "[ 1.7584469  -0.46244547 -0.14702037  2.4368935 ] 1\n",
      "[-0.33165267 -0.82022065  0.28783873  0.0099729 ] 1\n",
      "[-0.7980139   0.02476936  1.7478219  -1.948081  ] 1\n",
      "[ 1.1874653  -1.1219972  -0.07140783 -0.40072107] 1\n",
      "[ 0.693496   -0.83753514  0.5647102  -0.06694126] 1\n",
      "[-0.46771774 -0.61489034  0.4912168  -0.7474065 ] 1\n",
      "[ 0.84537125 -0.90110856 -0.5877492   1.7871709 ] 1\n",
      "[0.45402196 0.57872355 0.51145947 1.4959854 ] 1\n",
      "[-0.03723358 -0.12657517  0.19197872  1.0363371 ] 1\n",
      "[1.1381367  0.14899024 1.8786515  1.7350833 ] 1\n",
      "[-0.6464799  -0.35622412  0.25670555 -0.6195408 ] 1\n",
      "[ 0.2769965 -0.9538991 -0.2955237 -1.8139813] 1\n",
      "[ 0.16925868 -1.3700218   1.4921634   0.79654145] 1\n",
      "[ 0.18895051 -1.4371959   0.9754952   0.3318033 ] 1\n",
      "[0.2117801 0.753469  0.3214167 1.3526238] 1\n",
      "[ 0.41878778  0.5800164   0.9180906  -1.7423378 ] 1\n",
      "[-1.2600441  -0.06637791  0.9383411  -0.3606993 ] 1\n",
      "[-0.4276181   1.795333    0.41586146  2.2916355 ] 1\n",
      "[0.25214037 0.16486432 0.5413759  0.23695666] 1\n",
      "[ 0.62970257 -0.04008004 -0.16310868  1.8142366 ] 1\n",
      "[-0.80123395 -1.532613   -0.30962738  1.0546732 ] 1\n",
      "[ 0.07685213 -0.38293433  0.7908163   0.20530543] 1\n",
      "[-0.331482   -1.0812759   1.1341395   0.95816064] 1\n",
      "[ 1.4325117  -0.11423671  0.36328682  1.666768  ] 1\n",
      "[0.48708433 0.06189999 0.35485175 0.8763169 ] 1\n",
      "[0.6075397  0.25686157 0.2321651  0.92695546] 1\n",
      "[-0.43709934  0.07351428  0.12789267 -0.9497696 ] 1\n",
      "[ 0.56320196  0.27391702 -0.06134588 -1.367638  ] 1\n",
      "[-1.0017712 -2.0434718 -0.5182043  0.9084811] 1\n",
      "[0.653104   0.31803015 0.8035655  1.2707007 ] 1\n",
      "[-0.3444244  0.501026   0.3217989  1.8650713] 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1366646  -3.0690105   0.35708657  0.00645146] 1\n",
      "[ 0.7127959   0.08411755 -1.128293   -0.16971469] 1\n",
      "[-0.49606207  0.14563558  0.94002485 -1.9787498 ] 1\n",
      "[1.5293674  0.13882717 0.36578894 1.144209  ] 1\n",
      "[0.53316456 0.2755158  0.59671736 1.2872816 ] 1\n",
      "[-0.04453268 -0.83661205 -0.5412307  -0.1924164 ] 1\n",
      "[-1.7382385  -1.2629309   0.6838681   0.37062237] 1\n",
      "[ 0.7949715  -1.3670774   0.7875375   0.80123496] 1\n",
      "[0.95726943 0.50326556 0.568724   1.1781058 ] 1\n",
      "[-0.7708544   0.1442288   0.38106808 -0.9897845 ] 1\n",
      "[ 1.1261288   0.4134846  -0.41955036  2.0258892 ] 1\n",
      "[-0.1707908  -1.9075913   1.3939202  -0.49799618] 1\n",
      "[-0.6596896  -0.01977901  0.32743683  1.5660088 ] 1\n",
      "[-0.82517    -0.75463206 -0.6573956  -1.4559885 ] 1\n",
      "[-1.2461475  -0.8743057  -0.05832357 -1.302405  ] 1\n",
      "[-1.1645391  -1.0519586   0.68321353 -1.4958282 ] 1\n",
      "[-0.8020095  -1.0423573   0.00338017 -1.5179584 ] 1\n",
      "[-0.3620492 -1.2691157  1.053124   1.3596843] 1\n",
      "[-1.9050999  -0.9422445  -0.4442162  -0.81590223] 1\n",
      "[ 1.5269262  -1.1096115   0.84280354  0.15879178] 1\n",
      "[ 0.2572682  -2.0923452  -0.17628035  0.77616036] 1\n",
      "[ 1.0291953  -0.38055518  0.06643816  1.5416521 ] 1\n",
      "[ 0.9403761  -0.4425233   0.03211023  0.59191287] 1\n",
      "[ 0.10022166 -1.1722082   1.0105212   1.2176923 ] 1\n",
      "[ 0.15574464 -0.6474055   1.5884469   1.731865  ] 1\n",
      "[-0.62655544  0.08024293  0.4915236   0.9203527 ] 1\n",
      "[-0.67403054 -0.3600265   0.64384454 -0.11363942] 1\n",
      "[-0.14083713 -0.8247204   1.2231698   0.99680173] 1\n",
      "[1.348713  1.2921776 1.0335518 1.0709039] 1\n",
      "[-0.47966355 -0.8441482   0.63989574 -1.5594611 ] 1\n",
      "[-0.34303552 -0.61018914 -0.46193478 -2.3693118 ] 1\n",
      "[ 0.5848118  -1.3088771  -0.94940555  0.12920865] 1\n",
      "[-0.8541776  -0.5781255   0.75264436 -0.10591216] 1\n",
      "[-0.14728539 -0.6334687   0.06205913 -0.9560474 ] 1\n",
      "[-1.0308279  -2.1402955  -0.10420974  0.07011309] 1\n",
      "[-0.8839846  -0.70453894  0.14340174 -0.21569985] 1\n",
      "[ 1.2653749 -0.6159035 -0.162204  -0.0713523] 1\n",
      "[1.0245299 0.8681144 0.1335578 1.2436674] 1\n",
      "[0.965638  0.3493486 1.2212406 2.4350414] 1\n",
      "[-0.77915907  0.37901425  0.66490996 -1.9589194 ] 1\n",
      "[-0.4278      0.52943736 -0.72763443  1.0805007 ] 1\n",
      "[ 0.05940353 -1.6169177   1.1588472   1.8357184 ] 1\n",
      "[ 0.10619389 -1.3506335  -1.1823196   0.7946538 ] 1\n",
      "[-0.46550164 -1.1215589  -2.643825   -0.62615156] 1\n",
      "[ 0.7370642 -0.7799695 -1.0455912 -1.0490055] 1\n",
      "[ 0.29463673 -0.65807796 -0.4281507  -0.3558141 ] 1\n",
      "[ 0.30927482 -1.4316536   1.3729347   0.97455966] 1\n",
      "[-0.66214037 -1.3110058  -1.8159549   0.5912771 ] 1\n",
      "[ 0.8363753  -0.39145958 -0.35065532  1.8336368 ] 1\n",
      "[0.82040435 0.2740973  0.15654805 2.2719228 ] 1\n",
      "[-1.416299    0.08347807  0.18103078 -0.5164894 ] 1\n",
      "[ 0.08858685 -1.5442554   0.79010856  1.1177548 ] 1\n",
      "[ 0.9870732  -0.72262883  0.07059312  1.2261685 ] 1\n",
      "[-1.5859822 -1.2771858 -0.6894866 -2.3039618] 1\n",
      "[-0.9412867  -1.6887313  -0.34318268 -0.8846508 ] 1\n",
      "[0.05154324 0.32661778 0.6170559  0.9670756 ] 1\n",
      "[ 0.7744324  -0.1639784  -0.37075937  2.1586976 ] 1\n",
      "[ 0.78678215 -0.03786558 -1.0374606  -0.36879104] 1\n",
      "[-0.43388957 -0.05613162  0.93977964  0.9201608 ] 1\n",
      "[-0.24478893  0.87653756 -0.12834366 -1.0497272 ] 2\n",
      "[0.2827999 0.7697923 0.5629676 1.0211698] 2\n",
      "[-0.15782408  0.65610594 -0.0738778  -0.07827254] 2\n",
      "[-0.04340941 -0.28345352 -0.73122066 -0.19116975] 2\n",
      "[-0.56240875 -0.2147101  -0.5552188   0.7919456 ] 2\n",
      "[1.0719279 1.1598197 0.4708433 0.4921883] 2\n",
      "[-0.28671795 -1.344407   -0.288926    2.3100405 ] 2\n",
      "[ 1.2209986  -1.0998658   0.19271952  0.42000076] 2\n",
      "[-0.40186775 -0.3989373   0.73317933  1.1478827 ] 2\n",
      "[-0.02957402 -1.651927   -0.38390318  0.55648994] 2\n",
      "[1.555627   0.00436739 1.324203   0.4774182 ] 2\n",
      "[-0.6925372   0.13421068  0.7701677   0.85931003] 2\n",
      "[-1.2455342   0.39987007 -0.55976963 -2.5708442 ] 2\n",
      "[ 0.7117915  -0.84871477  0.35821706  0.826033  ] 2\n",
      "[-0.03043697 -1.401472    0.03490985  2.0244932 ] 2\n",
      "[ 0.52335376 -0.96832967  0.37831992  0.98122036] 2\n",
      "[0.20163351 0.2218306  0.16992798 0.4664012 ] 2\n",
      "[-1.6108992  -1.2424238  -0.28386778  0.41924974] 2\n",
      "[-0.7799565 -0.6807543 -0.6084116  1.6228935] 2\n",
      "[0.3975778  0.20283076 0.21076792 1.2925436 ] 2\n",
      "[-0.25850153  0.9003926   0.43106818 -0.7559173 ] 2\n",
      "[1.0101609  1.1977814  0.08334718 1.2829803 ] 2\n",
      "[-1.053709    2.3336751  -0.10322604 -0.56957954] 2\n",
      "[-0.6036756   2.350406   -0.42991295  0.22851956] 2\n",
      "[0.07097493 0.01796899 0.15793106 2.268222  ] 2\n",
      "[0.13105457 1.043869   0.40773687 1.13365   ] 2\n",
      "[ 0.08036445 -1.2077066   0.03026921  0.54862857] 2\n",
      "[ 0.6356061   0.18204924 -0.10558699 -0.25620237] 2\n",
      "[ 2.323438   -1.7535529   0.79945296  2.123159  ] 2\n",
      "[-0.44469047 -0.15809488 -1.3211553   0.05936721] 2\n",
      "[ 1.0719169  -0.00304873 -0.08798236  2.1755095 ] 2\n",
      "[ 0.2454134  -0.48405394  1.0546402   2.6263807 ] 2\n",
      "[-0.23891798  0.04932268  0.27787235  0.7571317 ] 2\n",
      "[0.24644276 0.8052374  0.34321332 2.1607132 ] 2\n",
      "[-0.73420894 -1.1547269   0.59439176  0.73310125] 2\n",
      "[-0.23683493 -0.99834025 -0.29370397  0.32432696] 2\n",
      "[-0.00841438  0.21206255  0.23157993  0.3154668 ] 2\n",
      "[ 0.13308465  1.2615811   0.33881983 -1.5934874 ] 2\n",
      "[ 0.47361922 -0.8536349   0.07586325  3.7121923 ] 2\n",
      "[0.71450967 0.5425069  0.3955653  1.3823148 ] 2\n",
      "[0.18437551 1.0598896  0.48906645 0.7889389 ] 2\n",
      "[0.68638283 1.0170577  0.6679583  0.6350521 ] 2\n",
      "[ 0.5926988   1.685053   -0.05879256  0.3530343 ] 2\n",
      "[-1.0657077  -0.73705983 -0.36433426 -0.27355033] 2\n",
      "[-0.02056027  0.48725453 -0.58190215  0.9379239 ] 2\n",
      "[ 1.0927182   0.3518896  -0.02744041  1.1952789 ] 2\n",
      "[ 0.5116524  -1.6720262   0.11254317 -1.2195344 ] 2\n",
      "[0.5427423  0.35913473 0.918917   2.2330651 ] 2\n",
      "[ 0.15641116  1.637699    1.2699733  -2.061717  ] 2\n",
      "[-0.7206615   0.5585933  -0.03284268  0.53678596] 2\n",
      "[-0.98104966 -2.3058393  -0.33032978  1.1574508 ] 2\n",
      "[ 0.05998806  0.05655773 -0.06568374  2.5263655 ] 2\n",
      "[-0.3084461  -0.6281594  -0.87663853  0.589484  ] 2\n",
      "[-0.26854807 -0.37147567  0.5096911   1.5651917 ] 2\n",
      "[-0.4425131   0.24965702  0.6539498   2.2453    ] 2\n",
      "[ 0.25051725 -0.50664634  0.9907776   2.6358595 ] 2\n",
      "[-1.4001598   0.8527199  -0.53550524 -1.5322405 ] 2\n",
      "[-0.6751309  -0.07895792 -1.2273788  -1.7668738 ] 2\n",
      "[-0.45676425 -1.387501   -0.36323512  1.3185195 ] 2\n",
      "[ 0.5402896  -1.2623978  -0.5724723   0.42699966] 2\n",
      "[-0.7372378   0.02811306 -1.5977899  -1.2471256 ] 2\n",
      "[-0.6013516  0.805499  -0.9575733 -2.3757224] 2\n",
      "[-0.2714819  1.1713493 -0.4041373 -2.4011078] 2\n",
      "[0.24387774 0.64440906 0.72045946 2.1267378 ] 2\n",
      "[-0.3813194  0.0520531 -1.0761279 -1.0118147] 2\n",
      "[ 0.71512693 -1.3879896   0.05755447  2.6502624 ] 2\n",
      "[-0.66985315  1.2240316  -0.05095539  0.8697443 ] 2\n",
      "[ 0.7927628 -1.0460005  1.5954784  1.6830974] 2\n",
      "[-0.53318125 -1.7348815  -0.6166884   2.2999864 ] 2\n",
      "[ 0.11540338 -1.1434051   0.19405079  1.4026983 ] 2\n",
      "[ 0.65671766 -0.634781   -0.7173083   1.0345159 ] 2\n",
      "[-1.039206   -1.1894891   0.79613554  1.3442351 ] 2\n",
      "[-0.12906693  0.26662242 -0.32086533  0.84609437] 2\n",
      "[0.41113126 0.04798206 1.2121142  1.7414926 ] 2\n",
      "[ 0.8887434  -0.5220802   0.73414063  2.186117  ] 2\n",
      "[ 0.33506352 -1.2144482   0.27762696  0.28284445] 2\n",
      "[ 0.20191677  0.2648286  -0.01360185  1.6053799 ] 2\n",
      "[ 0.2887856   1.1823337   0.014943   -0.05331227] 2\n",
      "[ 0.2706669  -0.57936263  0.03778204  1.3885164 ] 2\n",
      "[-0.37632096 -1.0869862  -0.03410348  0.35988703] 2\n",
      "[ 0.2876792   0.49633324 -0.45040596 -0.94029856] 2\n",
      "[ 0.1827141 -1.4383543  0.6344246  1.379841 ] 2\n",
      "[ 0.65953696  1.8149728  -0.6511479   0.40452912] 2\n",
      "[ 0.12193161 -1.1230516   1.6691101   2.4383264 ] 2\n",
      "[-1.0869122   0.40719518 -0.11634191  0.7796917 ] 2\n",
      "[ 0.12422897 -1.2762111   0.27412185  1.3912802 ] 2\n",
      "[-0.4054491  1.6775457 -0.0947048 -2.1957064] 2\n",
      "[ 0.77112746 -0.32813698  1.4195739   2.6816976 ] 2\n",
      "[-0.72868997 -1.1623513   0.8926109   1.688384  ] 2\n",
      "[ 0.7810186 -0.6212747  0.969978   1.6831956] 2\n",
      "[-0.95168066  0.34734586 -0.77123296 -2.6008563 ] 2\n",
      "[-0.05158703 -0.9396237  -0.01986166  1.6550885 ] 2\n",
      "[ 0.68717384 -0.49366125 -0.24484918 -1.1648355 ] 2\n",
      "[-0.14847031 -1.3538876  -0.6135884   0.8264512 ] 2\n",
      "[ 0.6095685  0.729141  -0.385188   0.8644347] 2\n",
      "[-0.124552    1.273697    0.5963178   0.13740715] 2\n",
      "[ 1.0615252  1.6490359 -0.4921902  1.0077981] 2\n",
      "[-0.26821235  1.575144   -0.8500041  -2.2426813 ] 2\n",
      "[-0.54362255 -1.4533008  -0.49680617  1.5823929 ] 2\n",
      "[-0.49039912 -2.4798434  -0.28214398  2.6373384 ] 2\n",
      "[ 0.13081959 -1.1193312   1.4180036   2.07197   ] 2\n",
      "[ 0.00891841 -2.0223494  -0.8123647   1.4143041 ] 2\n",
      "[-0.33564574  0.21959896 -0.7360562  -2.6565168 ] 2\n",
      "[0.26834816 0.2673212  0.8656154  1.9524641 ] 2\n",
      "[ 0.36456448  1.4802924   0.4367189  -2.291543  ] 2\n",
      "[-0.53906566 -0.97967446  0.554986    2.1779149 ] 2\n",
      "[ 0.18048654  0.31907976 -1.0856428  -2.61521   ] 2\n",
      "[ 0.57551193 -0.8141222   0.7304482   1.681889  ] 2\n",
      "[ 0.75344026 -1.5480344   0.47562304  2.5462847 ] 2\n",
      "[ 0.8334681   0.77061486 -1.1938354  -0.6816206 ] 2\n",
      "[ 1.0422806  -0.04671534  1.2604034   1.2392935 ] 3\n",
      "[ 0.6298764  -0.02788586  0.00420155 -0.03401504] 3\n",
      "[ 0.88622284 -1.3484241   0.15073767  0.8519362 ] 3\n",
      "[1.1293724  0.18853363 1.918954   1.3494003 ] 3\n",
      "[ 1.3749237  -0.9326106  -0.35284752  0.7719146 ] 3\n",
      "[ 0.5469406  -0.8641968   0.09833179  0.12747413] 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6338743  0.56043863 1.3497678  0.10598293] 3\n",
      "[ 0.02806809  2.401536   -1.333574   -1.3800466 ] 3\n",
      "[-0.11178063 -1.174769   -0.8321129  -1.5025595 ] 3\n",
      "[0.31847858 0.00793937 1.6276566  0.7149265 ] 3\n",
      "[ 1.711208   -1.6452326   0.73644906  1.467462  ] 3\n",
      "[-1.3687611   1.9563527  -0.24890503  0.00313404] 3\n",
      "[-0.40609536  0.13945712 -0.57430375 -0.55364966] 3\n",
      "[-1.4539045   0.59922427 -0.66204697 -1.831228  ] 3\n",
      "[ 1.1975547 -0.2907412  0.3282321  1.0352954] 3\n",
      "[ 0.36550868  0.08581685 -0.6377894  -0.39466226] 3\n",
      "[ 0.04727875  0.21168382  0.7442707  -0.9102658 ] 3\n",
      "[ 0.25376248  1.5480411  -0.40401822 -0.5419005 ] 3\n",
      "[ 1.3658872   0.74498373 -0.9376021   0.33338615] 3\n",
      "[-0.15298444 -0.35294682  1.580652    0.79616094] 3\n",
      "[-0.15135212 -1.060815    1.4471433   0.26550758] 3\n",
      "[-0.7118351   0.6424823  -1.1035107   0.26985162] 3\n",
      "[-1.6807731  -0.3798003   0.57140183 -1.4451029 ] 3\n",
      "[ 0.6869642   0.46116883  0.97941256 -0.04115868] 3\n",
      "[-0.3369592   0.8344749   0.7524578   0.01157093] 3\n",
      "[-0.09722108 -1.2106944   0.39931902 -0.01430042] 3\n",
      "[ 0.00554435  0.78800136  1.6330633  -0.82943654] 3\n",
      "[ 0.3412589  1.37043    0.742913  -0.5686519] 3\n",
      "[-0.8667967  -1.086456    0.04515827 -0.6523558 ] 3\n",
      "[0.97623456 0.8203799  0.8185656  0.97524345] 3\n",
      "[ 0.6801032   0.76656175 -0.64834034 -0.16808315] 3\n",
      "[-0.09828265 -1.1239003  -0.3386275  -0.8870375 ] 3\n",
      "[-0.5553593   1.2130423  -0.97137535 -0.3279955 ] 3\n",
      "[-0.42947763 -0.89890385  0.88588816 -1.398375  ] 3\n",
      "[ 0.5079337 -1.0740343 -0.651879  -1.3444471] 3\n",
      "[-0.01791044  0.74920297 -0.8907847  -2.4230142 ] 3\n",
      "[ 0.0669494  -0.85601425  1.6657224   0.6852603 ] 3\n",
      "[ 0.71261287  0.949271   -0.38513094 -1.2263837 ] 3\n",
      "[ 1.1760504  -0.01724447  0.3120975   1.0490606 ] 3\n",
      "[-0.42206407 -0.03277522 -0.23345667 -0.12620823] 3\n",
      "[-0.661477   -0.90307575  0.3875726  -0.01943851] 3\n",
      "[ 0.21128848  2.700666    1.0527439  -0.45693082] 3\n",
      "[-0.20287421 -0.94723326  0.89171886 -0.59936565] 3\n",
      "[ 0.09410682 -0.67798    -1.0704283  -1.4585214 ] 3\n",
      "[ 0.80043805 -0.3435924   0.34346768  0.73935676] 3\n",
      "[-0.0159915 -0.8232724  1.3334574  0.7857686] 3\n",
      "[-0.91992027 -0.4138459   1.5632565  -0.81578505] 3\n",
      "[ 0.39305422  0.9422871   1.3999106  -0.05427891] 3\n",
      "[ 0.10951281 -0.81285346  1.1627911  -0.34345886] 3\n",
      "[ 1.2025679  0.8983886  0.5410708 -1.4011382] 3\n",
      "[-1.0796891  -2.6331298  -0.10705794 -1.1460341 ] 3\n",
      "[ 0.3487904  -0.45321563  1.0237304  -0.05465436] 3\n",
      "[-0.9440013   1.0999146   0.520345    0.61102986] 3\n",
      "[ 0.1582264  -0.50516605 -0.49831966 -1.044833  ] 3\n",
      "[-0.6901734   2.392906   -0.06441962 -0.23433164] 3\n",
      "[-0.58155423 -2.0934765   0.9291135   0.57872677] 3\n",
      "[ 0.36256546  0.07015254 -1.0095421  -0.89297307] 3\n",
      "[ 1.2329043  -0.03790892  0.981274    0.7527182 ] 3\n",
      "[ 1.2366043   0.5097636  -0.03792092  1.0440845 ] 3\n",
      "[-0.44873235  1.5819247  -1.1809431  -1.387745  ] 3\n",
      "[ 0.8389912  -0.6413108  -0.2570513   0.70722854] 3\n",
      "[-0.57549727 -1.7970486   1.2887974   0.7128279 ] 3\n",
      "[ 0.45542553  1.7799896  -0.01076607 -0.6353177 ] 3\n",
      "[ 0.35564187 -1.2760633   0.9131714  -0.7795969 ] 3\n",
      "[ 0.2560589  -1.329467   -0.40176204 -1.8321667 ] 3\n",
      "[ 1.1898268  -0.45338222 -0.41591498  0.35939035] 3\n",
      "[ 0.39978132 -1.5631196   2.1051917   1.4300138 ] 3\n",
      "[-1.8203384  -1.4145705  -0.30041495 -1.0785093 ] 3\n",
      "[-0.6201291 -1.0993627  0.5004465 -1.6896551] 3\n",
      "[-0.2855894  -0.20239191  1.4227408   0.05213755] 3\n",
      "Accuracy: 0.3234375\n"
     ]
    }
   ],
   "source": [
    "# Print an Accuracy\n",
    "acc = 0\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint('logs')\n",
    "    if checkpoint:\n",
    "        saver.restore(sess, checkpoint)\n",
    "    for i in range(len(testlist)):\n",
    "        batch_data, batch_label = batch(testlist, 1)\n",
    "        logit = sess.run(output, feed_dict = {X:batch_data})\n",
    "        if np.argmax(logit[0]) == batch_label[0]:\n",
    "            acc += 1\n",
    "        else:\n",
    "            print(logit[0], batch_label[0])\n",
    "            \n",
    "    print(\"Accuracy:\", acc/len(testlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
