{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동차 이미지들을 학습 및 테스트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터는 각 240개씩 총 960개로 구성되어있고,\n",
    "시험 데이터는 각 60개씩 총 240개로 구성되어있습니다.\n",
    "\n",
    "차종은 세단, 스포츠카, SUV, 픽업트럭으로 네가지를 구분하였고 종류별 숫자 코드는 다음과 같습니다.\n",
    "0 = sedan, 1 = sportscar, 2 = suv, 3 = truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Namin Neural Network\n",
    "    -----------------\n",
    "    conv1 - relu1 - pool1 - bn1\n",
    "    conv2 - relu2 - pool2 - bn2\n",
    "    conv3 - relu3 - pool3 - bn3\n",
    "    affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Input Layer(Input Size): 100*100*3\n",
    "    First Layer: Conv1(3EA, 3*3*3, Strides=3, Padding=1) - ReLU1\n",
    "                - 34*34*3 - Pool1(2*2, Strides=1) - 33*33*3 - Bn1\n",
    "    Second Layer: Conv2(6EA, 6*6*3, Strides=1, Padding=VALID) - ReLU2\n",
    "                - 28*28*6 - Pool2(2*2, Strides=2) - 14*14*6 - Bn2\n",
    "    Third Layer: Conv3(9EA, 3*3*6, Strides=3, Padding=2) - ReLU3\n",
    "                - 6*6*9 - Pool3(2*2, Strides=2) - 3*3*9 - Bn3\n",
    "    Output Layer: Affine(W=3*3*9, B=9) - Output Nodes = 4(Sedan, Coupe, SUV, PickupTruck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to Sedan, Coupe, SUV, PickupTruck Images\n",
    "trainlist, testlist = [], []\n",
    "with open('train.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        trainlist.append([tmp[0], tmp[1]])\n",
    "        \n",
    "with open('test.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        testlist.append([tmp[0], tmp[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "IMG_H = 100\n",
    "IMG_W = 100\n",
    "IMG_C = 3\n",
    "\n",
    "def readimg(path):\n",
    "    img = plt.imread(path)\n",
    "    return img\n",
    "\n",
    "def batch(path, batch_size):\n",
    "    img, label, paths = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        img.append(readimg(path[0][0]))\n",
    "        label.append(int(path[0][1]))\n",
    "        path.append(path.pop(0))\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-4d6037b351e0>:10: conv2d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-4d6037b351e0>:11: max_pooling2d (from tensorflow.python.keras.legacy_tf_layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-4d6037b351e0>:12: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-4-4d6037b351e0>:22: flatten (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-4d6037b351e0>:24: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "num_class = 4 # Sedan, Coupe, SUV, PickupTruck\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C]) # Input Layer, X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C])\n",
    "    Y = tf.placeholder(tf.int32, [None]) # Y = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    with tf.variable_scope('CNN'):\n",
    "        # 1st Layer(Conv1 - relu1 - maxpool1 - bn1) = 33*33*3\n",
    "        conv1 = tf.layers.conv2d(X, 3, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, 2, (1, 1), padding='VALID')\n",
    "        bn1 = tf.compat.v1.layers.batch_normalization(pool1, training=True)\n",
    "        # 2nd Layer(Conv2 - relu2 - maxpool2 - bn2) = 14*14*6\n",
    "        conv2 = tf.layers.conv2d(bn1, 6, 6, (1, 1), padding='VALID', activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, 2, (2, 2), padding='VALID')\n",
    "        bn2 = tf.compat.v1.layers.batch_normalization(pool2, training=True)\n",
    "        # 3rd Layer(Conv3 - relu3 - maxpool3 - bn3) = 3*3*9\n",
    "        conv3 = tf.layers.conv2d(bn2, 9, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling2d(conv3, 2, (2, 2), padding='VALID')\n",
    "        bn3 = tf.compat.v1.layers.batch_normalization(pool3, training=True)\n",
    "        # Fully Connected Layer(Affine)\n",
    "        affine1 = tf.layers.flatten(bn3)\n",
    "        # Output Layer\n",
    "        output = tf.layers.dense(affine1, num_class)\n",
    "        \n",
    "    # Softmax with Loss\n",
    "    with tf.variable_scope('Loss'):\n",
    "        Loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= Y, logits=output))\n",
    "    \n",
    "    # Training with Adam    \n",
    "    train_step = tf.train.AdamOptimizer(0.005).minimize(Loss) \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar('Epoch-Loss', Loss)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size\n",
    "np.sum([np.product(var.shape) for var in g.get_collection('trainable_variables')]).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.8610898\n",
      "Epoch: 2 Loss: 1.6784959\n",
      "Epoch: 3 Loss: 1.5134746\n",
      "Epoch: 4 Loss: 1.4753015\n",
      "Epoch: 5 Loss: 1.4646184\n",
      "Epoch: 6 Loss: 1.3870704\n",
      "Epoch: 7 Loss: 1.3817285\n",
      "Epoch: 8 Loss: 1.3844335\n",
      "Epoch: 9 Loss: 1.3351525\n",
      "Epoch: 10 Loss: 1.3237802\n",
      "Epoch: 11 Loss: 1.3044883\n",
      "Epoch: 12 Loss: 1.3024364\n",
      "Epoch: 13 Loss: 1.2783118\n",
      "Epoch: 14 Loss: 1.2546879\n",
      "Epoch: 15 Loss: 1.263465\n",
      "Epoch: 16 Loss: 1.2451932\n",
      "Epoch: 17 Loss: 1.2223198\n",
      "Epoch: 18 Loss: 1.2194121\n",
      "Epoch: 19 Loss: 1.2012662\n",
      "Epoch: 20 Loss: 1.2111149\n",
      "Epoch: 21 Loss: 1.1874096\n",
      "Epoch: 22 Loss: 1.180881\n",
      "Epoch: 23 Loss: 1.1920776\n",
      "Epoch: 24 Loss: 1.163473\n",
      "Epoch: 25 Loss: 1.1649181\n",
      "Epoch: 26 Loss: 1.1727526\n",
      "Epoch: 27 Loss: 1.1476595\n",
      "Epoch: 28 Loss: 1.1455699\n",
      "Epoch: 29 Loss: 1.1407949\n",
      "Epoch: 30 Loss: 1.1289343\n",
      "Epoch: 31 Loss: 1.1253608\n",
      "Epoch: 32 Loss: 1.1213796\n",
      "Epoch: 33 Loss: 1.108736\n",
      "Epoch: 34 Loss: 1.1028564\n",
      "Epoch: 35 Loss: 1.0816852\n",
      "Epoch: 36 Loss: 1.094297\n",
      "Epoch: 37 Loss: 1.0782992\n",
      "Epoch: 38 Loss: 1.0635791\n",
      "Epoch: 39 Loss: 1.0692173\n",
      "Epoch: 40 Loss: 1.054427\n",
      "Epoch: 41 Loss: 1.0445116\n",
      "Epoch: 42 Loss: 1.0487005\n",
      "Epoch: 43 Loss: 1.0361588\n",
      "Epoch: 44 Loss: 1.0202893\n",
      "Epoch: 45 Loss: 1.0239321\n",
      "Epoch: 46 Loss: 1.0025731\n",
      "Epoch: 47 Loss: 1.0049343\n",
      "Epoch: 48 Loss: 1.001949\n",
      "Epoch: 49 Loss: 0.9768409\n",
      "Epoch: 50 Loss: 0.97953606\n",
      "Epoch: 51 Loss: 0.9707107\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 52 Loss: 0.9576621\n",
      "Epoch: 53 Loss: 0.9557391\n",
      "Epoch: 54 Loss: 0.93989366\n",
      "Epoch: 55 Loss: 0.9384772\n",
      "Epoch: 56 Loss: 0.92377496\n",
      "Epoch: 57 Loss: 0.9194916\n",
      "Epoch: 58 Loss: 0.9127536\n",
      "Epoch: 59 Loss: 0.90242535\n",
      "Epoch: 60 Loss: 0.8917269\n",
      "Epoch: 61 Loss: 0.9052924\n",
      "Epoch: 62 Loss: 0.86439466\n",
      "Epoch: 63 Loss: 0.8692827\n",
      "Epoch: 64 Loss: 0.8801771\n",
      "Epoch: 65 Loss: 0.83128643\n",
      "Epoch: 66 Loss: 0.8429233\n",
      "Epoch: 67 Loss: 0.84496623\n",
      "Epoch: 68 Loss: 0.80810523\n",
      "Epoch: 69 Loss: 0.8154196\n",
      "Epoch: 70 Loss: 0.8094153\n",
      "Epoch: 71 Loss: 0.7916955\n",
      "Epoch: 72 Loss: 0.781163\n",
      "Epoch: 73 Loss: 0.7824125\n",
      "Epoch: 74 Loss: 0.7756972\n",
      "Epoch: 75 Loss: 0.75772494\n",
      "Epoch: 76 Loss: 0.7567527\n",
      "Epoch: 77 Loss: 0.7715543\n",
      "Epoch: 78 Loss: 0.7185221\n",
      "Epoch: 79 Loss: 0.7400411\n",
      "Epoch: 80 Loss: 0.7492922\n",
      "Epoch: 81 Loss: 0.6990263\n",
      "Epoch: 82 Loss: 0.7047411\n",
      "Epoch: 83 Loss: 0.7232243\n",
      "Epoch: 84 Loss: 0.67842114\n",
      "Epoch: 85 Loss: 0.6766334\n",
      "Epoch: 86 Loss: 0.6889141\n",
      "Epoch: 87 Loss: 0.66226953\n",
      "Epoch: 88 Loss: 0.65082765\n",
      "Epoch: 89 Loss: 0.6614384\n",
      "Epoch: 90 Loss: 0.6448807\n",
      "Epoch: 91 Loss: 0.630014\n",
      "Epoch: 92 Loss: 0.63710374\n",
      "Epoch: 93 Loss: 0.6438056\n",
      "Epoch: 94 Loss: 0.6030026\n",
      "Epoch: 95 Loss: 0.6160422\n",
      "Epoch: 96 Loss: 0.63472354\n",
      "Epoch: 97 Loss: 0.57636774\n",
      "Epoch: 98 Loss: 0.5939309\n",
      "Epoch: 99 Loss: 0.6140772\n",
      "Epoch: 100 Loss: 0.56599593\n",
      "Epoch: 101 Loss: 0.5648361\n",
      "Epoch: 102 Loss: 0.5838805\n",
      "Epoch: 103 Loss: 0.55206674\n",
      "Epoch: 104 Loss: 0.542063\n",
      "Epoch: 105 Loss: 0.55820936\n",
      "Epoch: 106 Loss: 0.53546256\n",
      "Epoch: 107 Loss: 0.5234752\n",
      "Epoch: 108 Loss: 0.5323637\n",
      "Epoch: 109 Loss: 0.53369415\n",
      "Epoch: 110 Loss: 0.4990005\n",
      "Epoch: 111 Loss: 0.5095679\n",
      "Epoch: 112 Loss: 0.5386583\n",
      "Epoch: 113 Loss: 0.47305048\n",
      "Epoch: 114 Loss: 0.4978688\n",
      "Epoch: 115 Loss: 0.50898725\n",
      "Epoch: 116 Loss: 0.45768487\n",
      "Epoch: 117 Loss: 0.46016806\n",
      "Epoch: 118 Loss: 0.47120273\n",
      "Epoch: 119 Loss: 0.44494706\n",
      "Epoch: 120 Loss: 0.43928933\n",
      "Epoch: 121 Loss: 0.45083243\n",
      "Epoch: 122 Loss: 0.43633768\n",
      "Epoch: 123 Loss: 0.42251483\n",
      "Epoch: 124 Loss: 0.43553042\n",
      "Epoch: 125 Loss: 0.42178145\n",
      "Epoch: 126 Loss: 0.42109153\n",
      "Epoch: 127 Loss: 0.40608472\n",
      "Epoch: 128 Loss: 0.4454172\n",
      "Epoch: 129 Loss: 0.38035932\n",
      "Epoch: 130 Loss: 0.414758\n",
      "Epoch: 131 Loss: 0.41363853\n",
      "Epoch: 132 Loss: 0.37850705\n",
      "Epoch: 133 Loss: 0.37289846\n",
      "Epoch: 134 Loss: 0.39096105\n",
      "Epoch: 135 Loss: 0.35254467\n",
      "Epoch: 136 Loss: 0.35319573\n",
      "Epoch: 137 Loss: 0.359697\n",
      "Epoch: 138 Loss: 0.34755164\n",
      "Epoch: 139 Loss: 0.3342362\n",
      "Epoch: 140 Loss: 0.34863997\n",
      "Epoch: 141 Loss: 0.33400735\n",
      "Epoch: 142 Loss: 0.32340407\n",
      "Epoch: 143 Loss: 0.32785928\n",
      "Epoch: 144 Loss: 0.3419014\n",
      "Epoch: 145 Loss: 0.30443093\n",
      "Epoch: 146 Loss: 0.320754\n",
      "Epoch: 147 Loss: 0.33090523\n",
      "Epoch: 148 Loss: 0.29314795\n",
      "Epoch: 149 Loss: 0.2981638\n",
      "Epoch: 150 Loss: 0.3149677\n",
      "Epoch: 151 Loss: 0.28422543\n",
      "Epoch: 152 Loss: 0.27460185\n",
      "Epoch: 153 Loss: 0.28427678\n",
      "Epoch: 154 Loss: 0.27408355\n",
      "Epoch: 155 Loss: 0.26049575\n",
      "Epoch: 156 Loss: 0.27458385\n",
      "Epoch: 157 Loss: 0.26130295\n",
      "Epoch: 158 Loss: 0.256559\n",
      "Epoch: 159 Loss: 0.26104397\n",
      "Epoch: 160 Loss: 0.25945193\n",
      "Epoch: 161 Loss: 0.24825428\n",
      "Epoch: 162 Loss: 0.24475908\n",
      "Epoch: 163 Loss: 0.26312152\n",
      "Epoch: 164 Loss: 0.23276928\n",
      "Epoch: 165 Loss: 0.23763084\n",
      "Epoch: 166 Loss: 0.26946408\n",
      "Epoch: 167 Loss: 0.23109569\n",
      "Epoch: 168 Loss: 0.22660229\n",
      "Epoch: 169 Loss: 0.24196146\n",
      "Epoch: 170 Loss: 0.21741238\n",
      "Epoch: 171 Loss: 0.21309285\n",
      "Epoch: 172 Loss: 0.21014692\n",
      "Epoch: 173 Loss: 0.20581853\n",
      "Epoch: 174 Loss: 0.19675595\n",
      "Epoch: 175 Loss: 0.20092215\n",
      "Epoch: 176 Loss: 0.1981468\n",
      "Epoch: 177 Loss: 0.18722741\n",
      "Epoch: 178 Loss: 0.19689828\n",
      "Epoch: 179 Loss: 0.19383597\n",
      "Epoch: 180 Loss: 0.18321873\n",
      "Epoch: 181 Loss: 0.17984334\n",
      "Epoch: 182 Loss: 0.2041026\n",
      "Epoch: 183 Loss: 0.18418463\n",
      "Epoch: 184 Loss: 0.17042841\n",
      "Epoch: 185 Loss: 0.19725785\n",
      "Epoch: 186 Loss: 0.1790169\n",
      "Epoch: 187 Loss: 0.1639679\n",
      "Epoch: 188 Loss: 0.18513696\n",
      "Epoch: 189 Loss: 0.16149072\n",
      "Epoch: 190 Loss: 0.16805589\n",
      "Epoch: 191 Loss: 0.16332853\n",
      "Epoch: 192 Loss: 0.16819972\n",
      "Epoch: 193 Loss: 0.15359126\n",
      "Epoch: 194 Loss: 0.16434267\n",
      "Epoch: 195 Loss: 0.16230336\n",
      "Epoch: 196 Loss: 0.14245191\n",
      "Epoch: 197 Loss: 0.1541465\n",
      "Epoch: 198 Loss: 0.14959633\n",
      "Epoch: 199 Loss: 0.13451725\n",
      "Epoch: 200 Loss: 0.13348521\n",
      "Epoch: 201 Loss: 0.13979897\n",
      "Epoch: 202 Loss: 0.13666748\n",
      "Epoch: 203 Loss: 0.12789379\n",
      "Epoch: 204 Loss: 0.1395332\n",
      "Epoch: 205 Loss: 0.13118105\n",
      "Epoch: 206 Loss: 0.12695298\n",
      "Epoch: 207 Loss: 0.131027\n",
      "Epoch: 208 Loss: 0.12545207\n",
      "Epoch: 209 Loss: 0.12696306\n",
      "Epoch: 210 Loss: 0.12885201\n",
      "Epoch: 211 Loss: 0.128453\n",
      "Epoch: 212 Loss: 0.11638264\n",
      "Epoch: 213 Loss: 0.12390956\n",
      "Epoch: 214 Loss: 0.12157336\n",
      "Epoch: 215 Loss: 0.107715435\n",
      "Epoch: 216 Loss: 0.10982479\n",
      "Epoch: 217 Loss: 0.11425795\n",
      "Epoch: 218 Loss: 0.105035275\n",
      "Epoch: 219 Loss: 0.10016485\n",
      "Epoch: 220 Loss: 0.10634547\n",
      "Epoch: 221 Loss: 0.103433535\n",
      "Epoch: 222 Loss: 0.099838294\n",
      "Epoch: 223 Loss: 0.10481941\n",
      "Epoch: 224 Loss: 0.09702569\n",
      "Epoch: 225 Loss: 0.098735936\n",
      "Epoch: 226 Loss: 0.099250264\n",
      "Epoch: 227 Loss: 0.098089494\n",
      "Epoch: 228 Loss: 0.09356315\n",
      "Epoch: 229 Loss: 0.099231906\n",
      "Epoch: 230 Loss: 0.095693156\n",
      "Epoch: 231 Loss: 0.087199345\n",
      "Epoch: 232 Loss: 0.08822457\n",
      "Epoch: 233 Loss: 0.09504041\n",
      "Epoch: 234 Loss: 0.08637462\n",
      "Epoch: 235 Loss: 0.080229715\n",
      "Epoch: 236 Loss: 0.093696676\n",
      "Epoch: 237 Loss: 0.079622574\n",
      "Epoch: 238 Loss: 0.08122718\n",
      "Epoch: 239 Loss: 0.080320254\n",
      "Epoch: 240 Loss: 0.077553034\n",
      "Epoch: 241 Loss: 0.076979846\n",
      "Epoch: 242 Loss: 0.07987303\n",
      "Epoch: 243 Loss: 0.074139036\n",
      "Epoch: 244 Loss: 0.07681865\n",
      "Epoch: 245 Loss: 0.07761942\n",
      "Epoch: 246 Loss: 0.074344814\n",
      "Epoch: 247 Loss: 0.069606446\n",
      "Epoch: 248 Loss: 0.072049536\n",
      "Epoch: 249 Loss: 0.07536112\n",
      "Epoch: 250 Loss: 0.06612539\n",
      "Epoch: 251 Loss: 0.066222385\n",
      "Epoch: 252 Loss: 0.07311982\n",
      "Epoch: 253 Loss: 0.064261794\n",
      "Epoch: 254 Loss: 0.062438294\n",
      "Epoch: 255 Loss: 0.06479896\n",
      "Epoch: 256 Loss: 0.06025034\n",
      "Epoch: 257 Loss: 0.061247714\n",
      "Epoch: 258 Loss: 0.060200617\n",
      "Epoch: 259 Loss: 0.05861284\n",
      "Epoch: 260 Loss: 0.06086063\n",
      "Epoch: 261 Loss: 0.062571034\n",
      "Epoch: 262 Loss: 0.0591108\n",
      "Epoch: 263 Loss: 0.0560878\n",
      "Epoch: 264 Loss: 0.06075536\n",
      "Epoch: 265 Loss: 0.057934266\n",
      "Epoch: 266 Loss: 0.054457165\n",
      "Epoch: 267 Loss: 0.0529726\n",
      "Epoch: 268 Loss: 0.057084538\n",
      "Epoch: 269 Loss: 0.052657414\n",
      "Epoch: 270 Loss: 0.050548367\n",
      "Epoch: 271 Loss: 0.053631455\n",
      "Epoch: 272 Loss: 0.050311085\n",
      "Epoch: 273 Loss: 0.050842963\n",
      "Epoch: 274 Loss: 0.05018799\n",
      "Epoch: 275 Loss: 0.048774898\n",
      "Epoch: 276 Loss: 0.050217267\n",
      "Epoch: 277 Loss: 0.05037192\n",
      "Epoch: 278 Loss: 0.048668176\n",
      "Epoch: 279 Loss: 0.047259457\n",
      "Epoch: 280 Loss: 0.05045974\n",
      "Epoch: 281 Loss: 0.046866175\n",
      "Epoch: 282 Loss: 0.04430369\n",
      "Epoch: 283 Loss: 0.04570862\n",
      "Epoch: 284 Loss: 0.04693515\n",
      "Epoch: 285 Loss: 0.04325615\n",
      "Epoch: 286 Loss: 0.0410632\n",
      "Epoch: 287 Loss: 0.04467785\n",
      "Epoch: 288 Loss: 0.042251043\n",
      "Epoch: 289 Loss: 0.041847985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 Loss: 0.041820917\n",
      "Epoch: 291 Loss: 0.039877973\n",
      "Epoch: 292 Loss: 0.042612877\n",
      "Epoch: 293 Loss: 0.04115216\n",
      "Epoch: 294 Loss: 0.040253565\n",
      "Epoch: 295 Loss: 0.040685087\n",
      "Epoch: 296 Loss: 0.043621987\n",
      "Epoch: 297 Loss: 0.040378224\n",
      "Epoch: 298 Loss: 0.037459224\n",
      "Epoch: 299 Loss: 0.039730348\n",
      "Epoch: 300 Loss: 0.039389238\n",
      "Epoch: 301 Loss: 0.03638715\n",
      "Epoch: 302 Loss: 0.035344873\n",
      "Epoch: 303 Loss: 0.038259935\n",
      "Epoch: 304 Loss: 0.035363477\n",
      "Epoch: 305 Loss: 0.034990814\n",
      "Epoch: 306 Loss: 0.035698023\n",
      "Epoch: 307 Loss: 0.034423348\n",
      "Epoch: 308 Loss: 0.0356228\n",
      "Epoch: 309 Loss: 0.034822952\n",
      "Epoch: 310 Loss: 0.033922404\n",
      "Epoch: 311 Loss: 0.03472626\n",
      "Epoch: 312 Loss: 0.03624654\n",
      "Epoch: 313 Loss: 0.033759017\n",
      "Epoch: 314 Loss: 0.032643322\n",
      "Epoch: 315 Loss: 0.03556143\n",
      "Epoch: 316 Loss: 0.03434453\n",
      "Epoch: 317 Loss: 0.031596597\n",
      "Epoch: 318 Loss: 0.03104816\n",
      "Epoch: 319 Loss: 0.033347487\n",
      "Epoch: 320 Loss: 0.029655647\n",
      "Epoch: 321 Loss: 0.030191788\n",
      "Epoch: 322 Loss: 0.02992104\n",
      "Epoch: 323 Loss: 0.02950684\n",
      "Epoch: 324 Loss: 0.030095218\n",
      "Epoch: 325 Loss: 0.02934178\n",
      "Epoch: 326 Loss: 0.028140493\n",
      "Epoch: 327 Loss: 0.030402254\n",
      "Epoch: 328 Loss: 0.030670894\n",
      "Epoch: 329 Loss: 0.028995268\n",
      "Epoch: 330 Loss: 0.028684512\n",
      "Epoch: 331 Loss: 0.030979073\n",
      "Epoch: 332 Loss: 0.029119223\n",
      "Epoch: 333 Loss: 0.027168963\n",
      "Epoch: 334 Loss: 0.027615631\n",
      "Epoch: 335 Loss: 0.028321644\n",
      "Epoch: 336 Loss: 0.026027229\n",
      "Epoch: 337 Loss: 0.026534451\n",
      "Epoch: 338 Loss: 0.02670423\n",
      "Epoch: 339 Loss: 0.025869261\n",
      "Epoch: 340 Loss: 0.026047243\n",
      "Epoch: 341 Loss: 0.025475247\n",
      "Epoch: 342 Loss: 0.024630852\n",
      "Epoch: 343 Loss: 0.026063155\n",
      "Epoch: 344 Loss: 0.02640308\n",
      "Epoch: 345 Loss: 0.024551185\n",
      "Epoch: 346 Loss: 0.025633764\n",
      "Epoch: 347 Loss: 0.02684407\n",
      "Epoch: 348 Loss: 0.02510364\n",
      "Epoch: 349 Loss: 0.023623282\n",
      "Epoch: 350 Loss: 0.02552211\n",
      "Epoch: 351 Loss: 0.026205806\n",
      "Epoch: 352 Loss: 0.023318881\n",
      "Epoch: 353 Loss: 0.02328846\n",
      "Epoch: 354 Loss: 0.0241362\n",
      "Epoch: 355 Loss: 0.02228343\n",
      "Epoch: 356 Loss: 0.022189284\n",
      "Epoch: 357 Loss: 0.022190435\n",
      "Epoch: 358 Loss: 0.021985512\n",
      "Epoch: 359 Loss: 0.023440523\n",
      "Epoch: 360 Loss: 0.022939933\n",
      "Epoch: 361 Loss: 0.021731017\n",
      "Epoch: 362 Loss: 0.023556337\n",
      "Epoch: 363 Loss: 0.024414888\n",
      "Epoch: 364 Loss: 0.02269139\n",
      "Epoch: 365 Loss: 0.021552648\n",
      "Epoch: 366 Loss: 0.024065396\n",
      "Epoch: 367 Loss: 0.022987524\n",
      "Epoch: 368 Loss: 0.020580998\n",
      "Epoch: 369 Loss: 0.020411044\n",
      "Epoch: 370 Loss: 0.020924967\n",
      "Epoch: 371 Loss: 0.020018704\n",
      "Epoch: 372 Loss: 0.019731106\n",
      "Epoch: 373 Loss: 0.020198997\n",
      "Epoch: 374 Loss: 0.019970048\n",
      "Epoch: 375 Loss: 0.02026824\n",
      "Epoch: 376 Loss: 0.019769251\n",
      "Epoch: 377 Loss: 0.019347845\n",
      "Epoch: 378 Loss: 0.020837964\n",
      "Epoch: 379 Loss: 0.02164949\n",
      "Epoch: 380 Loss: 0.019565543\n",
      "Epoch: 381 Loss: 0.019395316\n",
      "Epoch: 382 Loss: 0.02094164\n",
      "Epoch: 383 Loss: 0.019636968\n",
      "Epoch: 384 Loss: 0.01861858\n",
      "Epoch: 385 Loss: 0.01829363\n",
      "Epoch: 386 Loss: 0.01883454\n",
      "Epoch: 387 Loss: 0.018386763\n",
      "Epoch: 388 Loss: 0.017788986\n",
      "Epoch: 389 Loss: 0.018411689\n",
      "Epoch: 390 Loss: 0.017595787\n",
      "Epoch: 391 Loss: 0.017789802\n",
      "Epoch: 392 Loss: 0.017684044\n",
      "Epoch: 393 Loss: 0.017142354\n",
      "Epoch: 394 Loss: 0.018602503\n",
      "Epoch: 395 Loss: 0.018482978\n",
      "Epoch: 396 Loss: 0.016935512\n",
      "Epoch: 397 Loss: 0.017721044\n",
      "Epoch: 398 Loss: 0.018858498\n",
      "Epoch: 399 Loss: 0.01733234\n",
      "Epoch: 400 Loss: 0.01673848\n",
      "Epoch: 401 Loss: 0.017082697\n",
      "Epoch: 402 Loss: 0.01747393\n",
      "Epoch: 403 Loss: 0.016174728\n",
      "Epoch: 404 Loss: 0.015910054\n",
      "Epoch: 405 Loss: 0.016342208\n",
      "Epoch: 406 Loss: 0.01576212\n",
      "Epoch: 407 Loss: 0.016009152\n",
      "Epoch: 408 Loss: 0.015695663\n",
      "Epoch: 409 Loss: 0.01544765\n",
      "Epoch: 410 Loss: 0.01669226\n",
      "Epoch: 411 Loss: 0.01648439\n",
      "Epoch: 412 Loss: 0.0154287685\n",
      "Epoch: 413 Loss: 0.016402151\n",
      "Epoch: 414 Loss: 0.01748479\n",
      "Epoch: 415 Loss: 0.015880005\n",
      "Epoch: 416 Loss: 0.015210318\n",
      "Epoch: 417 Loss: 0.016442204\n",
      "Epoch: 418 Loss: 0.016055003\n",
      "Epoch: 419 Loss: 0.0146470675\n",
      "Epoch: 420 Loss: 0.014153474\n",
      "Epoch: 421 Loss: 0.015153497\n",
      "Epoch: 422 Loss: 0.01445498\n",
      "Epoch: 423 Loss: 0.014174739\n",
      "Epoch: 424 Loss: 0.014429863\n",
      "Epoch: 425 Loss: 0.014134183\n",
      "Epoch: 426 Loss: 0.0151459165\n",
      "Epoch: 427 Loss: 0.0143290395\n",
      "Epoch: 428 Loss: 0.013945281\n",
      "Epoch: 429 Loss: 0.014862504\n",
      "Epoch: 430 Loss: 0.01580412\n",
      "Epoch: 431 Loss: 0.013957599\n",
      "Epoch: 432 Loss: 0.014119649\n",
      "Epoch: 433 Loss: 0.014980263\n",
      "Epoch: 434 Loss: 0.014593285\n",
      "Epoch: 435 Loss: 0.0134323705\n",
      "Epoch: 436 Loss: 0.01335661\n",
      "Epoch: 437 Loss: 0.013844285\n",
      "Epoch: 438 Loss: 0.013005565\n",
      "Epoch: 439 Loss: 0.012918578\n",
      "Epoch: 440 Loss: 0.0128931245\n",
      "Epoch: 441 Loss: 0.01284838\n",
      "Epoch: 442 Loss: 0.013118463\n",
      "Epoch: 443 Loss: 0.012901453\n",
      "Epoch: 444 Loss: 0.012412835\n",
      "Epoch: 445 Loss: 0.01355911\n",
      "Epoch: 446 Loss: 0.013795569\n",
      "Epoch: 447 Loss: 0.012583813\n",
      "Epoch: 448 Loss: 0.013148556\n",
      "Epoch: 449 Loss: 0.013916231\n",
      "Epoch: 450 Loss: 0.013028897\n",
      "Epoch: 451 Loss: 0.0122944815\n",
      "Epoch: 452 Loss: 0.012431637\n",
      "Epoch: 453 Loss: 0.012477949\n",
      "Epoch: 454 Loss: 0.012114958\n",
      "Epoch: 455 Loss: 0.011469557\n",
      "Epoch: 456 Loss: 0.012031307\n",
      "Epoch: 457 Loss: 0.011735034\n",
      "Epoch: 458 Loss: 0.011929476\n",
      "Epoch: 459 Loss: 0.0116886245\n",
      "Epoch: 460 Loss: 0.011497388\n",
      "Epoch: 461 Loss: 0.01243634\n",
      "Epoch: 462 Loss: 0.012436648\n",
      "Epoch: 463 Loss: 0.011439201\n",
      "Epoch: 464 Loss: 0.012107412\n",
      "Epoch: 465 Loss: 0.012914323\n",
      "Epoch: 466 Loss: 0.011692299\n",
      "Epoch: 467 Loss: 0.011218853\n",
      "Epoch: 468 Loss: 0.011729798\n",
      "Epoch: 469 Loss: 0.011776454\n",
      "Epoch: 470 Loss: 0.011008606\n",
      "Epoch: 471 Loss: 0.010759289\n",
      "Epoch: 472 Loss: 0.011359209\n",
      "Epoch: 473 Loss: 0.010815828\n",
      "Epoch: 474 Loss: 0.010765761\n",
      "Epoch: 475 Loss: 0.010674569\n",
      "Epoch: 476 Loss: 0.010582846\n",
      "Epoch: 477 Loss: 0.011575158\n",
      "Epoch: 478 Loss: 0.011097955\n",
      "Epoch: 479 Loss: 0.010389925\n",
      "Epoch: 480 Loss: 0.011666505\n",
      "Epoch: 481 Loss: 0.012359342\n",
      "Epoch: 482 Loss: 0.010765315\n",
      "Epoch: 483 Loss: 0.010526394\n",
      "Epoch: 484 Loss: 0.011382748\n",
      "Epoch: 485 Loss: 0.010969427\n",
      "Epoch: 486 Loss: 0.010061943\n",
      "Epoch: 487 Loss: 0.009820135\n",
      "Epoch: 488 Loss: 0.010398576\n",
      "Epoch: 489 Loss: 0.010020198\n",
      "Epoch: 490 Loss: 0.009797273\n",
      "Epoch: 491 Loss: 0.009949535\n",
      "Epoch: 492 Loss: 0.009865949\n",
      "Epoch: 493 Loss: 0.010099024\n",
      "Epoch: 494 Loss: 0.009867392\n",
      "Epoch: 495 Loss: 0.009603404\n",
      "Epoch: 496 Loss: 0.010608164\n",
      "Epoch: 497 Loss: 0.010920015\n",
      "Epoch: 498 Loss: 0.009537003\n",
      "Epoch: 499 Loss: 0.009961758\n",
      "Epoch: 500 Loss: 0.010704956\n",
      "Epoch: 501 Loss: 0.009986302\n",
      "Epoch: 502 Loss: 0.009470045\n",
      "Epoch: 503 Loss: 0.009224187\n",
      "Epoch: 504 Loss: 0.009547786\n",
      "Epoch: 505 Loss: 0.009355113\n",
      "Epoch: 506 Loss: 0.009019853\n",
      "Epoch: 507 Loss: 0.009066249\n",
      "Epoch: 508 Loss: 0.009026392\n",
      "Epoch: 509 Loss: 0.009147951\n",
      "Epoch: 510 Loss: 0.009027705\n",
      "Epoch: 511 Loss: 0.008744978\n",
      "Epoch: 512 Loss: 0.009619668\n",
      "Epoch: 513 Loss: 0.009736977\n",
      "Epoch: 514 Loss: 0.0088198185\n",
      "Epoch: 515 Loss: 0.009298638\n",
      "Epoch: 516 Loss: 0.009875747\n",
      "Epoch: 517 Loss: 0.009006899\n",
      "Epoch: 518 Loss: 0.008698124\n",
      "Epoch: 519 Loss: 0.008882984\n",
      "Epoch: 520 Loss: 0.008913269\n",
      "Epoch: 521 Loss: 0.00846486\n",
      "Epoch: 522 Loss: 0.008237349\n",
      "Epoch: 523 Loss: 0.008551602\n",
      "Epoch: 524 Loss: 0.008372861\n",
      "Epoch: 525 Loss: 0.008458529\n",
      "Epoch: 526 Loss: 0.008289223\n",
      "Epoch: 527 Loss: 0.008093321\n",
      "Epoch: 528 Loss: 0.008956161\n",
      "Epoch: 529 Loss: 0.008874845\n",
      "Epoch: 530 Loss: 0.008105121\n",
      "Epoch: 531 Loss: 0.008886124\n",
      "Epoch: 532 Loss: 0.009477231\n",
      "Epoch: 533 Loss: 0.008227853\n",
      "Epoch: 534 Loss: 0.008038571\n",
      "Epoch: 535 Loss: 0.0086591635\n",
      "Epoch: 536 Loss: 0.0085752765\n",
      "Epoch: 537 Loss: 0.007972081\n",
      "Epoch: 538 Loss: 0.007734677\n",
      "Epoch: 539 Loss: 0.008146093\n",
      "Epoch: 540 Loss: 0.0077632153\n",
      "Epoch: 541 Loss: 0.007645014\n",
      "Epoch: 542 Loss: 0.0077257142\n",
      "Epoch: 543 Loss: 0.0077000805\n",
      "Epoch: 544 Loss: 0.008278852\n",
      "Epoch: 545 Loss: 0.007886353\n",
      "Epoch: 546 Loss: 0.0075251996\n",
      "Epoch: 547 Loss: 0.008429362\n",
      "Epoch: 548 Loss: 0.008798887\n",
      "Epoch: 549 Loss: 0.007631714\n",
      "Epoch: 550 Loss: 0.007766964\n",
      "Epoch: 551 Loss: 0.008496408\n",
      "Epoch: 552 Loss: 0.007913298\n",
      "Epoch: 553 Loss: 0.007341214\n",
      "Epoch: 554 Loss: 0.007276833\n",
      "Epoch: 555 Loss: 0.007624112\n",
      "Epoch: 556 Loss: 0.00731929\n",
      "Epoch: 557 Loss: 0.0071528\n",
      "Epoch: 558 Loss: 0.0071027433\n",
      "Epoch: 559 Loss: 0.007155284\n",
      "Epoch: 560 Loss: 0.0073618013\n",
      "Epoch: 561 Loss: 0.0072098994\n",
      "Epoch: 562 Loss: 0.0069632223\n",
      "Epoch: 563 Loss: 0.007690894\n",
      "Epoch: 564 Loss: 0.007896333\n",
      "Epoch: 565 Loss: 0.006997072\n",
      "Epoch: 566 Loss: 0.007446597\n",
      "Epoch: 567 Loss: 0.00791032\n",
      "Epoch: 568 Loss: 0.0072414693\n",
      "Epoch: 569 Loss: 0.0069614397\n",
      "Epoch: 570 Loss: 0.0069491705\n",
      "Epoch: 571 Loss: 0.007132799\n",
      "Epoch: 572 Loss: 0.006855588\n",
      "Epoch: 573 Loss: 0.006539586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 574 Loss: 0.0066273795\n",
      "Epoch: 575 Loss: 0.0067479727\n",
      "Epoch: 576 Loss: 0.006708878\n",
      "Epoch: 577 Loss: 0.0065941513\n",
      "Epoch: 578 Loss: 0.006520813\n",
      "Epoch: 579 Loss: 0.0071959198\n",
      "Epoch: 580 Loss: 0.0071055694\n",
      "Epoch: 581 Loss: 0.006521168\n",
      "Epoch: 582 Loss: 0.0070011513\n",
      "Epoch: 583 Loss: 0.007492378\n",
      "Epoch: 584 Loss: 0.0065689175\n",
      "Epoch: 585 Loss: 0.0064419843\n",
      "Epoch: 586 Loss: 0.0067782444\n",
      "Epoch: 587 Loss: 0.006708307\n",
      "Epoch: 588 Loss: 0.0063180067\n",
      "Epoch: 589 Loss: 0.006121772\n",
      "Epoch: 590 Loss: 0.0064362283\n",
      "Epoch: 591 Loss: 0.006265606\n",
      "Epoch: 592 Loss: 0.006265621\n",
      "Epoch: 593 Loss: 0.0061097564\n",
      "Epoch: 594 Loss: 0.0061297375\n",
      "Epoch: 595 Loss: 0.006861941\n",
      "Epoch: 596 Loss: 0.0064985366\n",
      "Epoch: 597 Loss: 0.0060699256\n",
      "Epoch: 598 Loss: 0.00683987\n",
      "Epoch: 599 Loss: 0.00728442\n",
      "Epoch: 600 Loss: 0.0061093657\n",
      "Epoch: 601 Loss: 0.006148728\n",
      "Epoch: 602 Loss: 0.0066691185\n",
      "Epoch: 603 Loss: 0.0064414106\n",
      "Epoch: 604 Loss: 0.005975826\n",
      "Epoch: 605 Loss: 0.0057749464\n",
      "Epoch: 606 Loss: 0.006121673\n",
      "Epoch: 607 Loss: 0.0058514792\n",
      "Epoch: 608 Loss: 0.005800646\n",
      "Epoch: 609 Loss: 0.0057796957\n",
      "Epoch: 610 Loss: 0.005879582\n",
      "Epoch: 611 Loss: 0.0061504673\n",
      "Epoch: 612 Loss: 0.005858428\n",
      "Epoch: 613 Loss: 0.005728837\n",
      "Epoch: 614 Loss: 0.006337133\n",
      "Epoch: 615 Loss: 0.0065884595\n",
      "Epoch: 616 Loss: 0.0056863697\n",
      "Epoch: 617 Loss: 0.0059336186\n",
      "Epoch: 618 Loss: 0.0064772977\n",
      "Epoch: 619 Loss: 0.006013399\n",
      "Epoch: 620 Loss: 0.005742571\n",
      "Epoch: 621 Loss: 0.0055394964\n",
      "Epoch: 622 Loss: 0.005725688\n",
      "Epoch: 623 Loss: 0.00557568\n",
      "Epoch: 624 Loss: 0.0054130824\n",
      "Epoch: 625 Loss: 0.0053989324\n",
      "Epoch: 626 Loss: 0.005492306\n",
      "Epoch: 627 Loss: 0.005535867\n",
      "Epoch: 628 Loss: 0.0053864713\n",
      "Epoch: 629 Loss: 0.005297352\n",
      "Epoch: 630 Loss: 0.0059609334\n",
      "Epoch: 631 Loss: 0.00595734\n",
      "Epoch: 632 Loss: 0.0053396323\n",
      "Epoch: 633 Loss: 0.0057288334\n",
      "Epoch: 634 Loss: 0.006152076\n",
      "Epoch: 635 Loss: 0.0054646535\n",
      "Epoch: 636 Loss: 0.005364577\n",
      "Epoch: 637 Loss: 0.0053757876\n",
      "Epoch: 638 Loss: 0.0053759883\n",
      "Epoch: 639 Loss: 0.0052307476\n",
      "Epoch: 640 Loss: 0.0049981526\n",
      "Epoch: 641 Loss: 0.005287331\n",
      "Epoch: 642 Loss: 0.0051473887\n",
      "Epoch: 643 Loss: 0.0052086613\n",
      "Epoch: 644 Loss: 0.0050407522\n",
      "Epoch: 645 Loss: 0.0050040884\n",
      "Epoch: 646 Loss: 0.0056013754\n",
      "Epoch: 647 Loss: 0.005484798\n",
      "Epoch: 648 Loss: 0.0050112084\n",
      "Epoch: 649 Loss: 0.0055141198\n",
      "Epoch: 650 Loss: 0.0059514646\n",
      "Epoch: 651 Loss: 0.0050859377\n",
      "Epoch: 652 Loss: 0.0050277943\n",
      "Epoch: 653 Loss: 0.005368792\n",
      "Epoch: 654 Loss: 0.0052592494\n",
      "Epoch: 655 Loss: 0.004979569\n",
      "Epoch: 656 Loss: 0.004755561\n",
      "Epoch: 657 Loss: 0.005079893\n",
      "Epoch: 658 Loss: 0.0048651365\n",
      "Epoch: 659 Loss: 0.0047901776\n",
      "Epoch: 660 Loss: 0.0047776722\n",
      "Epoch: 661 Loss: 0.0048257895\n",
      "Epoch: 662 Loss: 0.0052745463\n",
      "Epoch: 663 Loss: 0.0050026467\n",
      "Epoch: 664 Loss: 0.004719745\n",
      "Epoch: 665 Loss: 0.005337494\n",
      "Epoch: 666 Loss: 0.005656049\n",
      "Epoch: 667 Loss: 0.0047793076\n",
      "Epoch: 668 Loss: 0.004932555\n",
      "Epoch: 669 Loss: 0.005339871\n",
      "Epoch: 670 Loss: 0.0050282627\n",
      "Epoch: 671 Loss: 0.004670188\n",
      "Epoch: 672 Loss: 0.0045479103\n",
      "Epoch: 673 Loss: 0.0048058815\n",
      "Epoch: 674 Loss: 0.0046193036\n",
      "Epoch: 675 Loss: 0.004553472\n",
      "Epoch: 676 Loss: 0.0045024375\n",
      "Epoch: 677 Loss: 0.004587859\n",
      "Epoch: 678 Loss: 0.0047504418\n",
      "Epoch: 679 Loss: 0.0045473212\n",
      "Epoch: 680 Loss: 0.004455222\n",
      "Epoch: 681 Loss: 0.0050078817\n",
      "Epoch: 682 Loss: 0.00516492\n",
      "Epoch: 683 Loss: 0.0044705886\n",
      "Epoch: 684 Loss: 0.004778362\n",
      "Epoch: 685 Loss: 0.005162669\n",
      "Epoch: 686 Loss: 0.0046625645\n",
      "Epoch: 687 Loss: 0.0045295525\n",
      "Epoch: 688 Loss: 0.004433942\n",
      "Epoch: 689 Loss: 0.004534122\n",
      "Epoch: 690 Loss: 0.00442932\n",
      "Epoch: 691 Loss: 0.0042043035\n",
      "Epoch: 692 Loss: 0.004292687\n",
      "Epoch: 693 Loss: 0.0044046137\n",
      "Epoch: 694 Loss: 0.004342865\n",
      "Epoch: 695 Loss: 0.004267082\n",
      "Epoch: 696 Loss: 0.004196164\n",
      "Epoch: 697 Loss: 0.004756288\n",
      "Epoch: 698 Loss: 0.004645077\n",
      "Epoch: 699 Loss: 0.0042201867\n",
      "Epoch: 700 Loss: 0.0045865583\n",
      "Epoch: 701 Loss: 0.004976088\n",
      "Epoch: 702 Loss: 0.004297425\n",
      "Epoch: 703 Loss: 0.004236113\n",
      "Epoch: 704 Loss: 0.0043446845\n",
      "Epoch: 705 Loss: 0.004382886\n",
      "Epoch: 706 Loss: 0.004147963\n",
      "Epoch: 707 Loss: 0.004001275\n",
      "Epoch: 708 Loss: 0.0041723154\n",
      "Epoch: 709 Loss: 0.0041117226\n",
      "Epoch: 710 Loss: 0.0041021323\n",
      "Epoch: 711 Loss: 0.0040194527\n",
      "Epoch: 712 Loss: 0.0040107323\n",
      "Epoch: 713 Loss: 0.004614676\n",
      "Epoch: 714 Loss: 0.0043119024\n",
      "Epoch: 715 Loss: 0.0039953883\n",
      "Epoch: 716 Loss: 0.0045623127\n",
      "Epoch: 717 Loss: 0.004845739\n",
      "Epoch: 718 Loss: 0.0040100445\n",
      "Epoch: 719 Loss: 0.0041030953\n",
      "Epoch: 720 Loss: 0.004397142\n",
      "Epoch: 721 Loss: 0.004342933\n",
      "Epoch: 722 Loss: 0.0039710295\n",
      "Epoch: 723 Loss: 0.0038316727\n",
      "Epoch: 724 Loss: 0.004025375\n",
      "Epoch: 725 Loss: 0.0039299913\n",
      "Epoch: 726 Loss: 0.0038392772\n",
      "Epoch: 727 Loss: 0.0038500573\n",
      "Epoch: 728 Loss: 0.0038907954\n",
      "Epoch: 729 Loss: 0.0041827667\n",
      "Epoch: 730 Loss: 0.0039596725\n",
      "Epoch: 731 Loss: 0.0037923683\n",
      "Epoch: 732 Loss: 0.0042693242\n",
      "Epoch: 733 Loss: 0.004451021\n",
      "Epoch: 734 Loss: 0.0037793282\n",
      "Epoch: 735 Loss: 0.0039863084\n",
      "Epoch: 736 Loss: 0.0043817563\n",
      "Epoch: 737 Loss: 0.0040187715\n",
      "Epoch: 738 Loss: 0.0037860393\n",
      "Epoch: 739 Loss: 0.0036748275\n",
      "Epoch: 740 Loss: 0.0038720523\n",
      "Epoch: 741 Loss: 0.0037450644\n",
      "Epoch: 742 Loss: 0.0036453723\n",
      "Epoch: 743 Loss: 0.0036451821\n",
      "Epoch: 744 Loss: 0.003691372\n",
      "Epoch: 745 Loss: 0.0037642133\n",
      "Epoch: 746 Loss: 0.003662691\n",
      "Epoch: 747 Loss: 0.003524026\n",
      "Epoch: 748 Loss: 0.0040533086\n",
      "Epoch: 749 Loss: 0.0040949243\n",
      "Epoch: 750 Loss: 0.0036290572\n",
      "Epoch: 751 Loss: 0.0039035913\n",
      "Epoch: 752 Loss: 0.004192004\n",
      "Epoch: 753 Loss: 0.003680637\n",
      "Epoch: 754 Loss: 0.0036316027\n",
      "Epoch: 755 Loss: 0.0036337615\n",
      "Epoch: 756 Loss: 0.0037360168\n",
      "Epoch: 757 Loss: 0.0035532692\n",
      "Epoch: 758 Loss: 0.0034359654\n",
      "Epoch: 759 Loss: 0.0035075098\n",
      "Epoch: 760 Loss: 0.0034886487\n",
      "Epoch: 761 Loss: 0.0035532864\n",
      "Epoch: 762 Loss: 0.0034432386\n",
      "Epoch: 763 Loss: 0.0034228393\n",
      "Epoch: 764 Loss: 0.0039001608\n",
      "Epoch: 765 Loss: 0.003789151\n",
      "Epoch: 766 Loss: 0.003412214\n",
      "Epoch: 767 Loss: 0.0038078167\n",
      "Epoch: 768 Loss: 0.0040914584\n",
      "Epoch: 769 Loss: 0.0034511043\n",
      "Epoch: 770 Loss: 0.003487672\n",
      "Epoch: 771 Loss: 0.0036793475\n",
      "Epoch: 772 Loss: 0.003654011\n",
      "Epoch: 773 Loss: 0.0034019835\n",
      "Epoch: 774 Loss: 0.003239443\n",
      "Epoch: 775 Loss: 0.0034859832\n",
      "Epoch: 776 Loss: 0.0033794153\n",
      "Epoch: 777 Loss: 0.0033161105\n",
      "Epoch: 778 Loss: 0.0032994533\n",
      "Epoch: 779 Loss: 0.0033097826\n",
      "Epoch: 780 Loss: 0.0037566456\n",
      "Epoch: 781 Loss: 0.003489727\n",
      "Epoch: 782 Loss: 0.003239271\n",
      "Epoch: 783 Loss: 0.0037939493\n",
      "Epoch: 784 Loss: 0.0040362272\n",
      "Epoch: 785 Loss: 0.0032710016\n",
      "Epoch: 786 Loss: 0.00343083\n",
      "Epoch: 787 Loss: 0.0037067777\n",
      "Epoch: 788 Loss: 0.0034670183\n",
      "Epoch: 789 Loss: 0.0032504813\n",
      "Epoch: 790 Loss: 0.0031335084\n",
      "Epoch: 791 Loss: 0.0033409877\n",
      "Epoch: 792 Loss: 0.0032165193\n",
      "Epoch: 793 Loss: 0.0031553095\n",
      "Epoch: 794 Loss: 0.0031397855\n",
      "Epoch: 795 Loss: 0.0032017017\n",
      "Epoch: 796 Loss: 0.003366025\n",
      "Epoch: 797 Loss: 0.0031621915\n",
      "Epoch: 798 Loss: 0.003085676\n",
      "Epoch: 799 Loss: 0.0035953687\n",
      "Epoch: 800 Loss: 0.0036745714\n",
      "Epoch: 801 Loss: 0.0030886887\n",
      "Epoch: 802 Loss: 0.0033711332\n",
      "Epoch: 803 Loss: 0.0036690626\n",
      "Epoch: 804 Loss: 0.0032606476\n",
      "Epoch: 805 Loss: 0.003193534\n",
      "Epoch: 806 Loss: 0.003075081\n",
      "Epoch: 807 Loss: 0.003190964\n",
      "Epoch: 808 Loss: 0.0031293493\n",
      "Epoch: 809 Loss: 0.0029622132\n",
      "Epoch: 810 Loss: 0.0029999793\n",
      "Epoch: 811 Loss: 0.0030729957\n",
      "Epoch: 812 Loss: 0.003074233\n",
      "Epoch: 813 Loss: 0.0030111517\n",
      "Epoch: 814 Loss: 0.002947324\n",
      "Epoch: 815 Loss: 0.003420053\n",
      "Epoch: 816 Loss: 0.0033341127\n",
      "Epoch: 817 Loss: 0.002994911\n",
      "Epoch: 818 Loss: 0.003264043\n",
      "Epoch: 819 Loss: 0.0035625366\n",
      "Epoch: 820 Loss: 0.0030208256\n",
      "Epoch: 821 Loss: 0.0029998494\n",
      "Epoch: 822 Loss: 0.0030652003\n",
      "Epoch: 823 Loss: 0.0031187888\n",
      "Epoch: 824 Loss: 0.0029365427\n",
      "Epoch: 825 Loss: 0.0028231847\n",
      "Epoch: 826 Loss: 0.0029350517\n",
      "Epoch: 827 Loss: 0.002909041\n",
      "Epoch: 828 Loss: 0.002947026\n",
      "Epoch: 829 Loss: 0.0028824569\n",
      "Epoch: 830 Loss: 0.0028386954\n",
      "Epoch: 831 Loss: 0.003326835\n",
      "Epoch: 832 Loss: 0.0031217074\n",
      "Epoch: 833 Loss: 0.0028323978\n",
      "Epoch: 834 Loss: 0.0032593352\n",
      "Epoch: 835 Loss: 0.0034970455\n",
      "Epoch: 836 Loss: 0.0028499747\n",
      "Epoch: 837 Loss: 0.0029389248\n",
      "Epoch: 838 Loss: 0.0031723848\n",
      "Epoch: 839 Loss: 0.0030892382\n",
      "Epoch: 840 Loss: 0.0028438945\n",
      "Epoch: 841 Loss: 0.002697979\n",
      "Epoch: 842 Loss: 0.0029156846\n",
      "Epoch: 843 Loss: 0.0028182482\n",
      "Epoch: 844 Loss: 0.0027700572\n",
      "Epoch: 845 Loss: 0.0027591938\n",
      "Epoch: 846 Loss: 0.0027825113\n",
      "Epoch: 847 Loss: 0.0030388788\n",
      "Epoch: 848 Loss: 0.0028560972\n",
      "Epoch: 849 Loss: 0.0027010408\n",
      "Epoch: 850 Loss: 0.0031378174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 851 Loss: 0.0032806955\n",
      "Epoch: 852 Loss: 0.002733212\n",
      "Epoch: 853 Loss: 0.0029095493\n",
      "Epoch: 854 Loss: 0.003212214\n",
      "Epoch: 855 Loss: 0.0028544292\n",
      "Epoch: 856 Loss: 0.0027478007\n",
      "Epoch: 857 Loss: 0.0026121284\n",
      "Epoch: 858 Loss: 0.0028363806\n",
      "Epoch: 859 Loss: 0.0027186922\n",
      "Epoch: 860 Loss: 0.0026554943\n",
      "Epoch: 861 Loss: 0.0025988596\n",
      "Epoch: 862 Loss: 0.0026800174\n",
      "Epoch: 863 Loss: 0.0027643433\n",
      "Epoch: 864 Loss: 0.0026579718\n",
      "Epoch: 865 Loss: 0.0025605496\n",
      "Epoch: 866 Loss: 0.002993401\n",
      "Epoch: 867 Loss: 0.0030099836\n",
      "Epoch: 868 Loss: 0.0026188355\n",
      "Epoch: 869 Loss: 0.0028590437\n",
      "Epoch: 870 Loss: 0.0030949179\n",
      "Epoch: 871 Loss: 0.0026830586\n",
      "Epoch: 872 Loss: 0.002661589\n",
      "Epoch: 873 Loss: 0.002636749\n",
      "Epoch: 874 Loss: 0.0027091512\n",
      "Epoch: 875 Loss: 0.0026103167\n",
      "Epoch: 876 Loss: 0.0024767641\n",
      "Epoch: 877 Loss: 0.002546498\n",
      "Epoch: 878 Loss: 0.0025584372\n",
      "Epoch: 879 Loss: 0.0025993786\n",
      "Epoch: 880 Loss: 0.0025070934\n",
      "Epoch: 881 Loss: 0.0025100738\n",
      "Epoch: 882 Loss: 0.0028560273\n",
      "Epoch: 883 Loss: 0.0027754554\n",
      "Epoch: 884 Loss: 0.0024585677\n",
      "Epoch: 885 Loss: 0.0027806112\n",
      "Epoch: 886 Loss: 0.0029809223\n",
      "Epoch: 887 Loss: 0.0025067076\n",
      "Epoch: 888 Loss: 0.0025473314\n",
      "Epoch: 889 Loss: 0.0026919248\n",
      "Epoch: 890 Loss: 0.0026374846\n",
      "Epoch: 891 Loss: 0.002480095\n",
      "Epoch: 892 Loss: 0.0023711086\n",
      "Epoch: 893 Loss: 0.0025356614\n",
      "Epoch: 894 Loss: 0.0024734149\n",
      "Epoch: 895 Loss: 0.0024570127\n",
      "Epoch: 896 Loss: 0.0024162496\n",
      "Epoch: 897 Loss: 0.0024095739\n",
      "Epoch: 898 Loss: 0.0028085345\n",
      "Epoch: 899 Loss: 0.0025755279\n",
      "Epoch: 900 Loss: 0.002358422\n",
      "Epoch: 901 Loss: 0.002882695\n",
      "Epoch: 902 Loss: 0.0030414017\n",
      "Epoch: 903 Loss: 0.0023995352\n",
      "Epoch: 904 Loss: 0.0025196045\n",
      "Epoch: 905 Loss: 0.0027548268\n",
      "Epoch: 906 Loss: 0.0025690603\n",
      "Epoch: 907 Loss: 0.0023884443\n",
      "Epoch: 908 Loss: 0.0022781764\n",
      "Epoch: 909 Loss: 0.0024783334\n",
      "Epoch: 910 Loss: 0.0023997887\n",
      "Epoch: 911 Loss: 0.0023516773\n",
      "Epoch: 912 Loss: 0.0023163252\n",
      "Epoch: 913 Loss: 0.002354724\n",
      "Epoch: 914 Loss: 0.0025083385\n",
      "Epoch: 915 Loss: 0.0023441878\n",
      "Epoch: 916 Loss: 0.0022716525\n",
      "Epoch: 917 Loss: 0.0027149161\n",
      "Epoch: 918 Loss: 0.0027502363\n",
      "Epoch: 919 Loss: 0.0022963185\n",
      "Epoch: 920 Loss: 0.0024925007\n",
      "Epoch: 921 Loss: 0.0027436006\n",
      "Epoch: 922 Loss: 0.0023979414\n",
      "Epoch: 923 Loss: 0.0023686076\n",
      "Epoch: 924 Loss: 0.0022548726\n",
      "Epoch: 925 Loss: 0.0023957817\n",
      "Epoch: 926 Loss: 0.002325548\n",
      "Epoch: 927 Loss: 0.002203213\n",
      "Epoch: 928 Loss: 0.0022064045\n",
      "Epoch: 929 Loss: 0.0022648536\n",
      "Epoch: 930 Loss: 0.0023330553\n",
      "Epoch: 931 Loss: 0.00223246\n",
      "Epoch: 932 Loss: 0.0021855372\n",
      "Epoch: 933 Loss: 0.0025508103\n",
      "Epoch: 934 Loss: 0.002514024\n",
      "Epoch: 935 Loss: 0.0021774413\n",
      "Epoch: 936 Loss: 0.002458653\n",
      "Epoch: 937 Loss: 0.0026567301\n",
      "Epoch: 938 Loss: 0.0022644082\n",
      "Epoch: 939 Loss: 0.002241979\n",
      "Epoch: 940 Loss: 0.0022819599\n",
      "Epoch: 941 Loss: 0.0022931667\n",
      "Epoch: 942 Loss: 0.002173434\n",
      "Epoch: 943 Loss: 0.0020854769\n",
      "Epoch: 944 Loss: 0.0022013686\n",
      "Epoch: 945 Loss: 0.0021827563\n",
      "Epoch: 946 Loss: 0.0022202723\n",
      "Epoch: 947 Loss: 0.002148973\n",
      "Epoch: 948 Loss: 0.002098271\n",
      "Epoch: 949 Loss: 0.002480111\n",
      "Epoch: 950 Loss: 0.0023448602\n",
      "Epoch: 951 Loss: 0.0020938804\n",
      "Epoch: 952 Loss: 0.0024433862\n",
      "Epoch: 953 Loss: 0.0026302189\n",
      "Epoch: 954 Loss: 0.002125536\n",
      "Epoch: 955 Loss: 0.0021973827\n",
      "Epoch: 956 Loss: 0.0023651053\n",
      "Epoch: 957 Loss: 0.0022842153\n",
      "Epoch: 958 Loss: 0.002114659\n",
      "Epoch: 959 Loss: 0.0020117608\n",
      "Epoch: 960 Loss: 0.0021909324\n",
      "Epoch: 961 Loss: 0.0021126054\n",
      "Epoch: 962 Loss: 0.0020623666\n",
      "Epoch: 963 Loss: 0.0020673168\n",
      "Epoch: 964 Loss: 0.0020843053\n",
      "Epoch: 965 Loss: 0.0022957937\n",
      "Epoch: 966 Loss: 0.002138565\n",
      "Epoch: 967 Loss: 0.0020111008\n",
      "Epoch: 968 Loss: 0.0023649991\n",
      "Epoch: 969 Loss: 0.0025145933\n",
      "Epoch: 970 Loss: 0.0020412197\n",
      "Epoch: 971 Loss: 0.0021946924\n",
      "Epoch: 972 Loss: 0.0024321177\n",
      "Epoch: 973 Loss: 0.0021597713\n",
      "Epoch: 974 Loss: 0.0020576126\n",
      "Epoch: 975 Loss: 0.001957413\n",
      "Epoch: 976 Loss: 0.0021341557\n",
      "Epoch: 977 Loss: 0.0020341673\n",
      "Epoch: 978 Loss: 0.0019992671\n",
      "Epoch: 979 Loss: 0.0019638008\n",
      "Epoch: 980 Loss: 0.0019945516\n",
      "Epoch: 981 Loss: 0.002108496\n",
      "Epoch: 982 Loss: 0.0020151818\n",
      "Epoch: 983 Loss: 0.0019236859\n",
      "Epoch: 984 Loss: 0.0022677523\n",
      "Epoch: 985 Loss: 0.0022968224\n",
      "Epoch: 986 Loss: 0.0019449091\n",
      "Epoch: 987 Loss: 0.00218029\n",
      "Epoch: 988 Loss: 0.0023615048\n",
      "Epoch: 989 Loss: 0.0020059217\n",
      "Epoch: 990 Loss: 0.0020085296\n",
      "Epoch: 991 Loss: 0.0019800914\n",
      "Epoch: 992 Loss: 0.0020570094\n",
      "Epoch: 993 Loss: 0.001953567\n",
      "Epoch: 994 Loss: 0.0018548553\n",
      "Epoch: 995 Loss: 0.0018995323\n",
      "Epoch: 996 Loss: 0.0019511143\n",
      "Epoch: 997 Loss: 0.0019486094\n",
      "Epoch: 998 Loss: 0.0019030875\n",
      "Epoch: 999 Loss: 0.0018645208\n",
      "Epoch: 1000 Loss: 0.0021827663\n"
     ]
    }
   ],
   "source": [
    "# Setting Batch with Training\n",
    "batch_size = 1261\n",
    "epoch = 1000\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('tf_board', sess.graph)\n",
    "    for i in range(epoch):\n",
    "        batch_data, batch_label = batch(trainlist, batch_size)     \n",
    "        _, loss, summary = sess.run([train_step, Loss, merged], feed_dict = {X: batch_data, Y: batch_label})\n",
    "        print(\"Epoch:\",i+1,\"Loss:\",loss)\n",
    "        if i % 10 == 0:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)\n",
    "        elif i+1 == epoch:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs\\model.ckpt-1000\n",
      "[-17.278006    3.235529    6.8484416  -5.9314322] 0\n",
      "[-10.057173    7.0168886   0.9351513  -4.7276454] 0\n",
      "[-1.0380585 -4.0620213 -8.484668   3.5294194] 0\n",
      "[  0.5391812   14.13793      0.63532877 -14.495421  ] 0\n",
      "[-4.342651  -6.0655394 -0.7866046 13.003101 ] 0\n",
      "[-17.934954  -15.7701025  11.571608    5.2379317] 0\n",
      "[  2.9852672   -0.66685915 -13.561948    11.08674   ] 0\n",
      "[  0.05917977  -9.059642     1.5708605  -12.91371   ] 0\n",
      "[-6.996594  -0.7078459  6.398762  -3.585133 ] 0\n",
      "[-1.9597287 -6.9169884 -3.3610299 -1.63502  ] 0\n",
      "[-14.141863   -4.7289524   2.0295415   4.431017 ] 0\n",
      "[  7.4803243 -11.335563    6.520249    9.691671 ] 0\n",
      "[-2.2283869 -2.68149    9.259859  -7.708418 ] 0\n",
      "[-2.4393115 12.857679   0.5217554 -5.489901 ] 0\n",
      "[-23.165653    5.367371    2.1649225   2.6220615] 0\n",
      "[ -0.66795397   4.922429     3.7625484  -11.746194  ] 0\n",
      "[ -0.19961657  15.461643   -12.570179   -15.195143  ] 0\n",
      "[-2.739008  -9.610336  -3.4827583  1.7071807] 0\n",
      "[  2.2846658 -14.08564     5.2519283   6.20102  ] 0\n",
      "[-6.20617   -7.3080826  4.6541553  7.0376725] 0\n",
      "[-17.621012    12.519396     0.48558608 -11.012248  ] 0\n",
      "[-15.228323    8.7553425   9.458043    3.7708256] 0\n",
      "[-5.6412215   0.01224853  3.0896657  -2.07263   ] 0\n",
      "[ 13.565355   -2.5953722 -16.819777   20.785831 ] 0\n",
      "[-20.810297   10.986465    4.7549458 -19.057869 ] 0\n",
      "[  8.736159  -14.203205   -3.6266096  12.717676 ] 0\n",
      "[-11.100092    2.1919775   2.5191634   1.4914956] 0\n",
      "[-3.2227921  1.1602077  2.4336543  3.4720585] 0\n",
      "[-0.90833455 -7.1192317   1.5455526   1.5913202 ] 0\n",
      "[-16.849752   5.809254   6.904127   8.670778] 0\n",
      "[-4.7431226 18.639502  -9.810149  -7.9972363] 0\n",
      "[ -8.441578    11.390358    -0.99850184 -12.045716  ] 0\n",
      "[ -3.035247  11.641755   4.880555 -18.250118] 0\n",
      "[-5.2286267   8.899697    1.8994352  -0.22487971] 0\n",
      "[-7.0380926 -3.7739534 -1.1689508  6.1496763] 0\n",
      "[-3.8409014  0.2765448  7.846219  -8.638044 ] 0\n",
      "[-3.1118276  1.4122725 -5.68437   -0.4154744] 0\n",
      "[  2.7041967 -21.380293    9.314283   14.805392 ] 0\n",
      "[-10.606175   -5.038548   -2.2935333  12.864615 ] 0\n",
      "[-0.8419578  0.0481465  0.6284284 -4.8830247] 0\n",
      "[-12.234321    -0.36995494   0.5357805   -5.955643  ] 0\n",
      "[ 7.1559846e-04 -1.2216096e+01 -1.0373244e+01  1.2814035e+01] 0\n",
      "[-23.034529    6.9184866  -7.031906   18.771297 ] 0\n",
      "[-11.525158   -3.9198053   7.9348392   0.9083278] 0\n",
      "[-10.7207     -5.3829517  -3.3924325  16.523771 ] 1\n",
      "[  5.993853   -12.649054    -0.93277794   8.739599  ] 1\n",
      "[-2.1790638  3.2959232 -8.0333395  5.674127 ] 1\n",
      "[  5.4996004   5.5062203 -10.020856    6.6993794] 1\n",
      "[-16.122551  -11.757771    6.7276196  14.041929 ] 1\n",
      "[ 18.520935  -8.976714   6.265134 -15.66437 ] 1\n",
      "[-12.805945   -5.5417957   2.7683883  22.040043 ] 1\n",
      "[14.438776   4.939449  -2.5851066 -8.916098 ] 1\n",
      "[-4.457757  -4.1390977  8.364619   1.5888464] 1\n",
      "[-7.3905087 -7.2237573  5.171757  13.034812 ] 1\n",
      "[-4.1942267 -3.54432   -6.0465717  8.231208 ] 1\n",
      "[-17.036514  -9.933128  15.058863  -8.486623] 1\n",
      "[ 9.208585   -9.298059   -0.33469328 -5.3715496 ] 1\n",
      "[  0.45382392  -9.889179   -10.363871    -3.935184  ] 1\n",
      "[-7.024753   -0.17511044  1.6110818   7.7989173 ] 1\n",
      "[-6.883769    0.04168273 -2.521139    4.320653  ] 1\n",
      "[-0.44543564 -9.276717   15.374225   -5.040696  ] 1\n",
      "[-9.420665   -4.307749    0.11585215 13.53816   ] 1\n",
      "[ 4.8921533 -1.0955921 -6.187484  -9.99404  ] 1\n",
      "[ 3.2940967 -2.101931  -6.3007755  0.8866215] 1\n",
      "[ 2.6476266 -6.0089903 -3.990452   2.8507526] 1\n",
      "[ 21.86246   -11.680756   -3.5468836  -1.124327 ] 1\n",
      "[ 9.247208  -3.8813167 -8.273508  18.427877 ] 1\n",
      "[ -7.783076 -14.416522  13.266238   5.701707] 1\n",
      "[  0.13728581 -11.681366    -1.0695167   16.729868  ] 1\n",
      "[-0.22886558  1.6823037   5.5504565  -0.97644806] 1\n",
      "[ -0.86820644  -1.6705457   12.731455   -21.610722  ] 1\n",
      "[  6.002738 -21.076044  13.799638  -6.886178] 1\n",
      "[-5.571563 -5.81725  11.516696  8.593806] 1\n",
      "[  1.9843813 -12.13035    11.098897   -4.9233446] 1\n",
      "[ 1.0786942 -8.159711  -2.9338942 -2.1499212] 1\n",
      "[ 3.3022122   0.78977895 -0.700888   12.372993  ] 1\n",
      "[ 7.8506274   0.42410296 -4.1888576   5.7467713 ] 1\n",
      "[-13.224019   -7.5176935  11.149686   14.304628 ] 1\n",
      "[-17.350227   -9.243865    4.5283184   3.6318505] 1\n",
      "[-1.7083064e-02 -1.6250310e+00 -9.1339865e+00  1.7164454e+01] 1\n",
      "[ -2.5472052 -10.20836    -1.8230946   5.8035483] 1\n",
      "[ 1.432715  -1.1667466 -6.692641  -5.419772 ] 1\n",
      "[-0.6554166 -5.2897015 -2.483229   7.9290495] 1\n",
      "[  1.4588687 -10.092149   -6.3136992  28.074184 ] 1\n",
      "[ 3.9390938  3.6312275 -8.705077   2.5082765] 1\n",
      "[ 6.3604283 -4.7261243 -5.96531    4.164068 ] 1\n",
      "[-1.0273468 -9.113222   4.3728685 13.0215845] 1\n",
      "[ 0.12550072 -5.911172   -3.0161357   5.2403097 ] 1\n",
      "[ 4.6321974 -3.1645408 -7.434961   8.747845 ] 1\n",
      "[ -6.925732  -11.287103   21.25361     0.6574601] 1\n",
      "[  5.758934   -6.8121676 -19.63333     8.639194 ] 1\n",
      "[ 12.32553    10.445915   -2.2273083 -13.863495 ] 2\n",
      "[-13.576689    -1.9730096    0.26297274   3.305779  ] 2\n",
      "[ 4.960276  -9.243996  -5.194075  -1.7610738] 2\n",
      "[ -4.891345    2.0948722 -11.90397     1.1380155] 2\n",
      "[ 7.6031504  3.2034214 -3.815618  -8.986926 ] 2\n",
      "[ -1.7150826  -4.887196  -12.304485    8.360512 ] 2\n",
      "[  7.040585   -17.839474    -3.1005728   -0.39658117] 2\n",
      "[12.487464  -9.719484   2.4865043 14.416911 ] 2\n",
      "[-26.072992   -5.760421    4.0576506  10.582426 ] 2\n",
      "[ -3.893322   14.373516    2.8274148 -19.673891 ] 2\n",
      "[-4.810495 10.466623 -5.315831 -9.02445 ] 2\n",
      "[-10.813561    6.918132    4.910561    0.8411751] 2\n",
      "[ 6.01545   9.082513 -9.077456 -7.403072] 2\n",
      "[15.728433  -8.80517   -5.771652   1.0142794] 2\n",
      "[-0.97880805 -9.251077    2.615417    4.3919916 ] 2\n",
      "[-12.955245     0.29668403  -9.483385    20.950844  ] 2\n",
      "[-18.914118   -1.7027862   6.1447396   6.872958 ] 2\n",
      "[ -4.947783   5.926793  -4.710982 -10.012771] 2\n",
      "[-13.906767   -1.0702949   9.985142   13.424489 ] 2\n",
      "[ 4.6174912  1.5266256 -4.0658646 -5.108652 ] 2\n",
      "[ 12.87131   -14.843922  -10.172681    3.9678433] 2\n",
      "[  8.549065   12.533345   -2.3094785 -17.006294 ] 2\n",
      "[  7.649763    -0.84669375 -10.09616      0.6237719 ] 2\n",
      "[ -2.5005374 -13.640889   -3.6371665   5.32922  ] 2\n",
      "[-16.174578   -6.6648483  11.185489   13.467327 ] 2\n",
      "[-5.9145937 12.608947   4.3411674 -2.6288433] 2\n",
      "[ -6.4950476  -2.787954  -13.395965   11.277997 ] 2\n",
      "[  8.127409   5.629802  -6.029427 -14.740798] 2\n",
      "[-3.7971923  7.0640745  2.243279  -3.4312358] 2\n",
      "[-2.567619   -4.8664303   0.10853191 19.678406  ] 2\n",
      "[ 17.970827    7.582088   -3.7456996 -10.032836 ] 2\n",
      "[-22.897991  -9.145239   8.013299   9.341913] 2\n",
      "[ 5.76248   -7.2143426  5.378813   6.059351 ] 2\n",
      "[-8.547166    0.09770605 -7.5827675  18.173372  ] 2\n",
      "[ -8.052037  13.117982 -10.66392   -2.315594] 2\n",
      "[-15.9742565   18.754772    -0.39558718   4.3849015 ] 2\n",
      "[ -8.84235   11.347009 -15.58551   14.157511] 2\n",
      "[13.203503    1.1265295  -7.427885    0.93884075] 2\n",
      "[-5.1144047  0.7725581  1.4358588  1.7471855] 2\n",
      "[ 0.8457547 16.497044  -4.767096  -8.263053 ] 2\n",
      "[ -3.0168571   4.1953216 -10.052481    1.5617504] 3\n",
      "[-14.942698   -1.5328298   8.638869   -3.024289 ] 3\n",
      "[ 16.679173   -15.435978    -3.3162055   -0.27997577] 3\n",
      "[-8.027195  -8.835581  14.327185   0.8682991] 3\n",
      "[  9.31097   -10.678108    6.0359855   5.8986835] 3\n",
      "[ 9.01615    -7.887927   -1.7007309   0.48967016] 3\n",
      "[ -0.39110714 -18.433252     6.782588    -0.6238686 ] 3\n",
      "[  2.4231336   8.802698  -16.279436    3.802982 ] 3\n",
      "[-9.001833  -3.9376426  6.0411205  0.3913465] 3\n",
      "[ 18.401852  -15.1577835  -6.012761   -5.598986 ] 3\n",
      "[ 12.962732  -11.161514   -9.338124   -0.6922982] 3\n",
      "[ -7.917407  22.435244   4.372516 -14.349352] 3\n",
      "[-15.785363   -2.9502838  14.412002  -15.749306 ] 3\n",
      "[-22.136156    4.6808376  16.650187   -6.1081963] 3\n",
      "[  6.412027    -0.29850245 -11.534764     3.6891143 ] 3\n",
      "[  0.5000603  -6.9336843  -4.7448244 -12.327004 ] 3\n",
      "[  2.709684    4.4730434 -14.972415   -3.334053 ] 3\n",
      "[-1.4957939 -7.437035   4.56946    1.8761772] 3\n",
      "[ 15.806747   -5.1412797 -15.718207   -2.2461135] 3\n",
      "[-8.467362   8.036849  -4.5566363 -7.9579363] 3\n",
      "[-14.305018   12.861408   -1.8258629   1.842808 ] 3\n",
      "[  9.911528  -7.245316   6.361305 -12.303999] 3\n",
      "[ 21.377176  -16.8579     -3.7818606  -2.374139 ] 3\n",
      "[ -8.367067   12.0102825 -19.957136    6.271439 ] 3\n",
      "[  7.168977   20.901129   -3.8392627 -15.286997 ] 3\n",
      "[ -9.818144  12.866768  11.169847 -13.147096] 3\n",
      "[-26.569466   16.642715   -2.2063806   0.0826981] 3\n",
      "[-14.985947   -6.172946   17.05831     6.5577035] 3\n",
      "[-1.0569108 -2.8776655  1.0099481 -3.6553402] 3\n",
      "[ 13.856046   8.316452 -10.685809   8.149204] 3\n",
      "[ 7.991204  -5.0290227 -1.139824  -1.0472832] 3\n",
      "Accuracy: 0.325\n"
     ]
    }
   ],
   "source": [
    "# Print an Accuracy\n",
    "acc = 0\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint('logs')\n",
    "    if checkpoint:\n",
    "        saver.restore(sess, checkpoint)\n",
    "    for i in range(len(testlist)):\n",
    "        batch_data, batch_label = batch(testlist, 1)\n",
    "        logit = sess.run(output, feed_dict = {X:batch_data})\n",
    "        if np.argmax(logit[0]) == batch_label[0]:\n",
    "            acc += 1\n",
    "        else:\n",
    "            print(logit[0], batch_label[0])\n",
    "            \n",
    "    print(\"Accuracy:\", acc/len(testlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
