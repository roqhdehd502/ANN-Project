{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동차 이미지들을 학습 및 테스트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터는 각 480개씩 총 1920개로 구성되어있고,\n",
    "시험 데이터는 각 160개씩 총 480개로 구성되어있습니다.\n",
    "\n",
    "차종은 세단, 쿠페, SUV, 픽업트럭으로 네가지를 구분하였고 종류별 숫자 코드는 다음과 같습니다.\n",
    "0 = Sedan, 1 = Coupe, 2 = SUV, 3 = Pickup-Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Namin Neural Network\n",
    "    ----------\n",
    "    Input Layer: 100*100*3(Input Size)\n",
    "    \n",
    "    1st Layer: Conv1(3EA, 3*3*3, Strides=3, Padding=1),\n",
    "               ReLU1(34*34*3),\n",
    "               Pool1(2*2, Strides=1),\n",
    "               Bn1(33*33*3)\n",
    "    2nd Layer: Conv2(6EA, 6*6*3, Strides=1, Padding=VALID),\n",
    "               ReLU2(28*28*6),\n",
    "               Pool2(2*2, Strides=2),\n",
    "               Bn2(14*14*6)\n",
    "    3rd Layer: Conv3(9EA, 3*3*6, Strides=3, Padding=2),\n",
    "               ReLU3(6*6*9),\n",
    "               Pool3(2*2, Strides=2),\n",
    "               Bn3(3*3*9)\n",
    "    4th Layer: Flatten,\n",
    "               Affine(W=3*3*9, B=9)\n",
    "                \n",
    "    Output Layer: Softmax Cross Entropy(Sedan, Coupe, SUV, Pickup-Truck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to Sedan, Coupe, SUV, PickupTruck Images\n",
    "trainlist, testlist = [], []\n",
    "with open('train.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        trainlist.append([tmp[0], tmp[1]])\n",
    "        \n",
    "with open('test.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        testlist.append([tmp[0], tmp[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "IMG_H = 100\n",
    "IMG_W = 100\n",
    "IMG_C = 3\n",
    "\n",
    "def readimg(path):\n",
    "    img = plt.imread(path)\n",
    "    return img\n",
    "\n",
    "def batch(path, batch_size):\n",
    "    img, label, paths = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        img.append(readimg(path[0][0]))\n",
    "        label.append(int(path[0][1]))\n",
    "        path.append(path.pop(0))\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "num_class = 4 # Sedan, Coupe, SUV, PickupTruck\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C]) # Input Layer, X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C])\n",
    "    Y = tf.placeholder(tf.int32, [None]) # Y = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    with tf.variable_scope('CNN'):\n",
    "        # 1st Layer(Conv1 - relu1 - maxpool1 - bn1) = 33*33*3\n",
    "        conv1 = tf.layers.conv2d(X, 3, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, 2, (1, 1), padding='VALID')\n",
    "        bn1 = tf.compat.v1.layers.batch_normalization(pool1, training=True)\n",
    "        # 2nd Layer(Conv2 - relu2 - maxpool2 - bn2) = 14*14*6\n",
    "        conv2 = tf.layers.conv2d(bn1, 6, 6, (1, 1), padding='VALID', activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, 2, (2, 2), padding='VALID')\n",
    "        bn2 = tf.compat.v1.layers.batch_normalization(pool2, training=True)\n",
    "        # 3rd Layer(Conv3 - relu3 - maxpool3 - bn3) = 3*3*9\n",
    "        conv3 = tf.layers.conv2d(bn2, 9, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling2d(conv3, 2, (2, 2), padding='VALID')\n",
    "        bn3 = tf.compat.v1.layers.batch_normalization(pool3, training=True)\n",
    "        # Fully Connected Layer(Affine)\n",
    "        affine1 = tf.layers.flatten(bn3)\n",
    "        # Output Layer\n",
    "        output = tf.layers.dense(affine1, num_class)\n",
    "        \n",
    "    # Softmax with Loss\n",
    "    with tf.variable_scope('Loss'):\n",
    "        Loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= Y, logits=output))\n",
    "    \n",
    "    # Training with Adam    \n",
    "    train_step = tf.train.AdamOptimizer(0.005).minimize(Loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar('Epoch-Loss', Loss)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size\n",
    "np.sum([np.product(var.shape) for var in g.get_collection('trainable_variables')]).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.631593\n",
      "Epoch: 2 Loss: 1.4582636\n",
      "Epoch: 3 Loss: 1.4053605\n",
      "Epoch: 4 Loss: 1.4826719\n",
      "Epoch: 5 Loss: 1.2624273\n",
      "Epoch: 6 Loss: 2.9279804\n",
      "Epoch: 7 Loss: 2.7565608\n",
      "Epoch: 8 Loss: 2.7209485\n",
      "Epoch: 9 Loss: 2.775003\n",
      "Epoch: 10 Loss: 2.4362888\n",
      "Epoch: 11 Loss: 1.7852826\n",
      "Epoch: 12 Loss: 1.7954918\n",
      "Epoch: 13 Loss: 1.7305063\n",
      "Epoch: 14 Loss: 1.5103219\n",
      "Epoch: 15 Loss: 1.176048\n",
      "Epoch: 16 Loss: 1.0825949\n",
      "Epoch: 17 Loss: 1.1761502\n",
      "Epoch: 18 Loss: 1.110229\n",
      "Epoch: 19 Loss: 0.7596741\n",
      "Epoch: 20 Loss: 0.81031716\n",
      "Epoch: 21 Loss: 2.6192067\n",
      "Epoch: 22 Loss: 2.2232757\n",
      "Epoch: 23 Loss: 2.9230404\n",
      "Epoch: 24 Loss: 2.4288945\n",
      "Epoch: 25 Loss: 2.694233\n",
      "Epoch: 26 Loss: 2.3608837\n",
      "Epoch: 27 Loss: 2.2462497\n",
      "Epoch: 28 Loss: 2.2896233\n",
      "Epoch: 29 Loss: 2.3738642\n",
      "Epoch: 30 Loss: 1.8606273\n",
      "Epoch: 31 Loss: 1.8561676\n",
      "Epoch: 32 Loss: 1.8118513\n",
      "Epoch: 33 Loss: 1.5877088\n",
      "Epoch: 34 Loss: 1.5532078\n",
      "Epoch: 35 Loss: 1.6096509\n",
      "Epoch: 36 Loss: 2.7028482\n",
      "Epoch: 37 Loss: 2.554895\n",
      "Epoch: 38 Loss: 2.6863909\n",
      "Epoch: 39 Loss: 2.3784971\n",
      "Epoch: 40 Loss: 2.5842283\n",
      "Epoch: 41 Loss: 2.3334746\n",
      "Epoch: 42 Loss: 2.330535\n",
      "Epoch: 43 Loss: 2.3815413\n",
      "Epoch: 44 Loss: 2.2131765\n",
      "Epoch: 45 Loss: 1.9682437\n",
      "Epoch: 46 Loss: 1.8571693\n",
      "Epoch: 47 Loss: 1.9191647\n",
      "Epoch: 48 Loss: 1.863912\n",
      "Epoch: 49 Loss: 1.8053784\n",
      "Epoch: 50 Loss: 1.8063895\n",
      "Epoch: 51 Loss: 2.111671\n",
      "Epoch: 52 Loss: 2.157652\n",
      "Epoch: 53 Loss: 1.7852213\n",
      "Epoch: 54 Loss: 1.9386027\n",
      "Epoch: 55 Loss: 1.6869221\n",
      "Epoch: 56 Loss: 2.1132061\n",
      "Epoch: 57 Loss: 1.8126225\n",
      "Epoch: 58 Loss: 1.6910061\n",
      "Epoch: 59 Loss: 1.7563202\n",
      "Epoch: 60 Loss: 1.6471977\n",
      "Epoch: 61 Loss: 1.5088099\n",
      "Epoch: 62 Loss: 1.3649081\n",
      "Epoch: 63 Loss: 1.2434413\n",
      "Epoch: 64 Loss: 1.3592858\n",
      "Epoch: 65 Loss: 1.2940645\n",
      "Epoch: 66 Loss: 2.0278234\n",
      "Epoch: 67 Loss: 1.9499431\n",
      "Epoch: 68 Loss: 1.8921574\n",
      "Epoch: 69 Loss: 1.9584152\n",
      "Epoch: 70 Loss: 1.7563174\n",
      "Epoch: 71 Loss: 1.6776795\n",
      "Epoch: 72 Loss: 1.6770518\n",
      "Epoch: 73 Loss: 1.623204\n",
      "Epoch: 74 Loss: 1.4978154\n",
      "Epoch: 75 Loss: 1.4056008\n",
      "Epoch: 76 Loss: 1.3095006\n",
      "Epoch: 77 Loss: 1.2636275\n",
      "Epoch: 78 Loss: 1.2753762\n",
      "Epoch: 79 Loss: 0.9659319\n",
      "Epoch: 80 Loss: 1.0580611\n",
      "Epoch: 81 Loss: 1.8444331\n",
      "Epoch: 82 Loss: 1.7343998\n",
      "Epoch: 83 Loss: 2.1130126\n",
      "Epoch: 84 Loss: 1.7430452\n",
      "Epoch: 85 Loss: 2.0862353\n",
      "Epoch: 86 Loss: 1.8794513\n",
      "Epoch: 87 Loss: 1.8410658\n",
      "Epoch: 88 Loss: 1.9032536\n",
      "Epoch: 89 Loss: 2.04775\n",
      "Epoch: 90 Loss: 1.703552\n",
      "Epoch: 91 Loss: 1.9178818\n",
      "Epoch: 92 Loss: 1.7760541\n",
      "Epoch: 93 Loss: 1.7854159\n",
      "Epoch: 94 Loss: 1.6485295\n",
      "Epoch: 95 Loss: 1.5742946\n",
      "Epoch: 96 Loss: 1.7796631\n",
      "Epoch: 97 Loss: 1.6742212\n",
      "Epoch: 98 Loss: 1.7114648\n",
      "Epoch: 99 Loss: 1.6582427\n",
      "Epoch: 100 Loss: 1.6733091\n",
      "Epoch: 101 Loss: 1.587336\n",
      "Epoch: 102 Loss: 1.6715975\n",
      "Epoch: 103 Loss: 1.5562952\n",
      "Epoch: 104 Loss: 1.6064849\n",
      "Epoch: 105 Loss: 1.5793835\n",
      "Epoch: 106 Loss: 1.5214194\n",
      "Epoch: 107 Loss: 1.5797881\n",
      "Epoch: 108 Loss: 1.5147918\n",
      "Epoch: 109 Loss: 1.5341432\n",
      "Epoch: 110 Loss: 1.5080354\n",
      "Epoch: 111 Loss: 1.7925408\n",
      "Epoch: 112 Loss: 1.8031651\n",
      "Epoch: 113 Loss: 1.5443616\n",
      "Epoch: 114 Loss: 1.7082789\n",
      "Epoch: 115 Loss: 1.5073934\n",
      "Epoch: 116 Loss: 1.6723614\n",
      "Epoch: 117 Loss: 1.5571307\n",
      "Epoch: 118 Loss: 1.4887959\n",
      "Epoch: 119 Loss: 1.5875646\n",
      "Epoch: 120 Loss: 1.5307102\n",
      "Epoch: 121 Loss: 1.4022548\n",
      "Epoch: 122 Loss: 1.3518482\n",
      "Epoch: 123 Loss: 1.2187195\n",
      "Epoch: 124 Loss: 1.2690814\n",
      "Epoch: 125 Loss: 1.2784364\n",
      "Epoch: 126 Loss: 1.7464542\n",
      "Epoch: 127 Loss: 1.7061838\n",
      "Epoch: 128 Loss: 1.6696677\n",
      "Epoch: 129 Loss: 1.7047279\n",
      "Epoch: 130 Loss: 1.5960579\n",
      "Epoch: 131 Loss: 1.5526521\n",
      "Epoch: 132 Loss: 1.5851994\n",
      "Epoch: 133 Loss: 1.5713995\n",
      "Epoch: 134 Loss: 1.4711432\n",
      "Epoch: 135 Loss: 1.3726141\n",
      "Epoch: 136 Loss: 1.3562922\n",
      "Epoch: 137 Loss: 1.3151448\n",
      "Epoch: 138 Loss: 1.2687795\n",
      "Epoch: 139 Loss: 1.1010783\n",
      "Epoch: 140 Loss: 1.1363968\n",
      "Epoch: 141 Loss: 1.6396623\n",
      "Epoch: 142 Loss: 1.5563798\n",
      "Epoch: 143 Loss: 1.820221\n",
      "Epoch: 144 Loss: 1.5539901\n",
      "Epoch: 145 Loss: 1.7962552\n",
      "Epoch: 146 Loss: 1.6639975\n",
      "Epoch: 147 Loss: 1.6592975\n",
      "Epoch: 148 Loss: 1.6820843\n",
      "Epoch: 149 Loss: 1.8225859\n",
      "Epoch: 150 Loss: 1.6098526\n",
      "Epoch: 151 Loss: 1.704942\n",
      "Epoch: 152 Loss: 1.6132647\n",
      "Epoch: 153 Loss: 1.6463637\n",
      "Epoch: 154 Loss: 1.5646172\n",
      "Epoch: 155 Loss: 1.4848444\n",
      "Epoch: 156 Loss: 1.6096494\n",
      "Epoch: 157 Loss: 1.5196407\n",
      "Epoch: 158 Loss: 1.5623759\n",
      "Epoch: 159 Loss: 1.5223205\n",
      "Epoch: 160 Loss: 1.5415975\n",
      "Epoch: 161 Loss: 1.4526563\n",
      "Epoch: 162 Loss: 1.5991414\n",
      "Epoch: 163 Loss: 1.420949\n",
      "Epoch: 164 Loss: 1.5307794\n",
      "Epoch: 165 Loss: 1.5231797\n",
      "Epoch: 166 Loss: 1.4519074\n",
      "Epoch: 167 Loss: 1.4694766\n",
      "Epoch: 168 Loss: 1.4020689\n",
      "Epoch: 169 Loss: 1.4932232\n",
      "Epoch: 170 Loss: 1.4010044\n",
      "Epoch: 171 Loss: 1.6996245\n",
      "Epoch: 172 Loss: 1.6949465\n",
      "Epoch: 173 Loss: 1.5379782\n",
      "Epoch: 174 Loss: 1.6293497\n",
      "Epoch: 175 Loss: 1.4951879\n",
      "Epoch: 176 Loss: 1.5754137\n",
      "Epoch: 177 Loss: 1.489099\n",
      "Epoch: 178 Loss: 1.458164\n",
      "Epoch: 179 Loss: 1.5654445\n",
      "Epoch: 180 Loss: 1.4825966\n",
      "Epoch: 181 Loss: 1.3705739\n",
      "Epoch: 182 Loss: 1.3598425\n",
      "Epoch: 183 Loss: 1.249356\n",
      "Epoch: 184 Loss: 1.2758253\n",
      "Epoch: 185 Loss: 1.290659\n",
      "Epoch: 186 Loss: 1.5960677\n",
      "Epoch: 187 Loss: 1.5745074\n",
      "Epoch: 188 Loss: 1.592799\n",
      "Epoch: 189 Loss: 1.5927436\n",
      "Epoch: 190 Loss: 1.5385944\n",
      "Epoch: 191 Loss: 1.4914637\n",
      "Epoch: 192 Loss: 1.5424521\n",
      "Epoch: 193 Loss: 1.5723429\n",
      "Epoch: 194 Loss: 1.4630687\n",
      "Epoch: 195 Loss: 1.3623849\n",
      "Epoch: 196 Loss: 1.3832657\n",
      "Epoch: 197 Loss: 1.3408004\n",
      "Epoch: 198 Loss: 1.2675123\n",
      "Epoch: 199 Loss: 1.139304\n",
      "Epoch: 200 Loss: 1.1699173\n",
      "Epoch: 201 Loss: 1.5621793\n",
      "Epoch: 202 Loss: 1.5160477\n",
      "Epoch: 203 Loss: 1.6973104\n",
      "Epoch: 204 Loss: 1.5039692\n",
      "Epoch: 205 Loss: 1.6950158\n",
      "Epoch: 206 Loss: 1.6039698\n",
      "Epoch: 207 Loss: 1.5827011\n",
      "Epoch: 208 Loss: 1.6014037\n",
      "Epoch: 209 Loss: 1.6773078\n",
      "Epoch: 210 Loss: 1.5222445\n",
      "Epoch: 211 Loss: 1.6041036\n",
      "Epoch: 212 Loss: 1.5462846\n",
      "Epoch: 213 Loss: 1.5716512\n",
      "Epoch: 214 Loss: 1.5028996\n",
      "Epoch: 215 Loss: 1.4292824\n",
      "Epoch: 216 Loss: 1.5628102\n",
      "Epoch: 217 Loss: 1.4661864\n",
      "Epoch: 218 Loss: 1.5079556\n",
      "Epoch: 219 Loss: 1.5058739\n",
      "Epoch: 220 Loss: 1.481297\n",
      "Epoch: 221 Loss: 1.4158931\n",
      "Epoch: 222 Loss: 1.5569797\n",
      "Epoch: 223 Loss: 1.3948396\n",
      "Epoch: 224 Loss: 1.4847984\n",
      "Epoch: 225 Loss: 1.4850329\n",
      "Epoch: 226 Loss: 1.4153866\n",
      "Epoch: 227 Loss: 1.4492875\n",
      "Epoch: 228 Loss: 1.3926044\n",
      "Epoch: 229 Loss: 1.4814066\n",
      "Epoch: 230 Loss: 1.39274\n",
      "Epoch: 231 Loss: 1.6592607\n",
      "Epoch: 232 Loss: 1.6939334\n",
      "Epoch: 233 Loss: 1.4924951\n",
      "Epoch: 234 Loss: 1.6085558\n",
      "Epoch: 235 Loss: 1.470818\n",
      "Epoch: 236 Loss: 1.5542448\n",
      "Epoch: 237 Loss: 1.4844064\n",
      "Epoch: 238 Loss: 1.4611037\n",
      "Epoch: 239 Loss: 1.5239879\n",
      "Epoch: 240 Loss: 1.4576559\n",
      "Epoch: 241 Loss: 1.3766329\n",
      "Epoch: 242 Loss: 1.3552508\n",
      "Epoch: 243 Loss: 1.2718382\n",
      "Epoch: 244 Loss: 1.2836645\n",
      "Epoch: 245 Loss: 1.2864988\n",
      "Epoch: 246 Loss: 1.5697657\n",
      "Epoch: 247 Loss: 1.5472689\n",
      "Epoch: 248 Loss: 1.5460479\n",
      "Epoch: 249 Loss: 1.5543087\n",
      "Epoch: 250 Loss: 1.5091165\n",
      "Epoch: 251 Loss: 1.4612398\n",
      "Epoch: 252 Loss: 1.504261\n",
      "Epoch: 253 Loss: 1.5009296\n",
      "Epoch: 254 Loss: 1.4404995\n",
      "Epoch: 255 Loss: 1.344039\n",
      "Epoch: 256 Loss: 1.3545079\n",
      "Epoch: 257 Loss: 1.3275778\n",
      "Epoch: 258 Loss: 1.2683473\n",
      "Epoch: 259 Loss: 1.1420034\n",
      "Epoch: 260 Loss: 1.1676652\n",
      "Epoch: 261 Loss: 1.5210745\n",
      "Epoch: 262 Loss: 1.4970379\n",
      "Epoch: 263 Loss: 1.6893401\n",
      "Epoch: 264 Loss: 1.5071926\n",
      "Epoch: 265 Loss: 1.6857951\n",
      "Epoch: 266 Loss: 1.5794113\n",
      "Epoch: 267 Loss: 1.5657644\n",
      "Epoch: 268 Loss: 1.5864078\n",
      "Epoch: 269 Loss: 1.7054595\n",
      "Epoch: 270 Loss: 1.5265636\n",
      "Epoch: 271 Loss: 1.5954018\n",
      "Epoch: 272 Loss: 1.5439482\n",
      "Epoch: 273 Loss: 1.5880907\n",
      "Epoch: 274 Loss: 1.5256435\n",
      "Epoch: 275 Loss: 1.4389963\n",
      "Epoch: 276 Loss: 1.5212829\n",
      "Epoch: 277 Loss: 1.434572\n",
      "Epoch: 278 Loss: 1.4732766\n",
      "Epoch: 279 Loss: 1.4665899\n",
      "Epoch: 280 Loss: 1.455747\n",
      "Epoch: 281 Loss: 1.3863777\n",
      "Epoch: 282 Loss: 1.5418949\n",
      "Epoch: 283 Loss: 1.3502584\n",
      "Epoch: 284 Loss: 1.4712617\n",
      "Epoch: 285 Loss: 1.4615238\n",
      "Epoch: 286 Loss: 1.3885038\n",
      "Epoch: 287 Loss: 1.4085572\n",
      "Epoch: 288 Loss: 1.354389\n",
      "Epoch: 289 Loss: 1.4599545\n",
      "Epoch: 290 Loss: 1.3492663\n",
      "Epoch: 291 Loss: 1.6049958\n",
      "Epoch: 292 Loss: 1.6303418\n",
      "Epoch: 293 Loss: 1.4872649\n",
      "Epoch: 294 Loss: 1.5637002\n",
      "Epoch: 295 Loss: 1.4657586\n",
      "Epoch: 296 Loss: 1.5046629\n",
      "Epoch: 297 Loss: 1.4513745\n",
      "Epoch: 298 Loss: 1.4369974\n",
      "Epoch: 299 Loss: 1.5228198\n",
      "Epoch: 300 Loss: 1.4358726\n",
      "Epoch: 301 Loss: 1.371043\n",
      "Epoch: 302 Loss: 1.363236\n",
      "Epoch: 303 Loss: 1.2737992\n",
      "Epoch: 304 Loss: 1.2827889\n",
      "Epoch: 305 Loss: 1.290107\n",
      "Epoch: 306 Loss: 1.488022\n",
      "Epoch: 307 Loss: 1.460943\n",
      "Epoch: 308 Loss: 1.5070119\n",
      "Epoch: 309 Loss: 1.5065329\n",
      "Epoch: 310 Loss: 1.4744301\n",
      "Epoch: 311 Loss: 1.4282761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 312 Loss: 1.4865507\n",
      "Epoch: 313 Loss: 1.5235819\n",
      "Epoch: 314 Loss: 1.4288926\n",
      "Epoch: 315 Loss: 1.3336452\n",
      "Epoch: 316 Loss: 1.3632426\n",
      "Epoch: 317 Loss: 1.3408657\n",
      "Epoch: 318 Loss: 1.2621431\n",
      "Epoch: 319 Loss: 1.1549621\n",
      "Epoch: 320 Loss: 1.19023\n",
      "Epoch: 321 Loss: 1.4960024\n",
      "Epoch: 322 Loss: 1.4773941\n",
      "Epoch: 323 Loss: 1.6258179\n",
      "Epoch: 324 Loss: 1.477725\n",
      "Epoch: 325 Loss: 1.617972\n",
      "Epoch: 326 Loss: 1.547442\n",
      "Epoch: 327 Loss: 1.5179245\n",
      "Epoch: 328 Loss: 1.5432594\n",
      "Epoch: 329 Loss: 1.6203239\n",
      "Epoch: 330 Loss: 1.4786117\n",
      "Epoch: 331 Loss: 1.5365031\n",
      "Epoch: 332 Loss: 1.5083761\n",
      "Epoch: 333 Loss: 1.5354925\n",
      "Epoch: 334 Loss: 1.487242\n",
      "Epoch: 335 Loss: 1.4093521\n",
      "Epoch: 336 Loss: 1.506464\n",
      "Epoch: 337 Loss: 1.4201272\n",
      "Epoch: 338 Loss: 1.4515096\n",
      "Epoch: 339 Loss: 1.4628547\n",
      "Epoch: 340 Loss: 1.425905\n",
      "Epoch: 341 Loss: 1.3792734\n",
      "Epoch: 342 Loss: 1.5262915\n",
      "Epoch: 343 Loss: 1.3636243\n",
      "Epoch: 344 Loss: 1.460383\n",
      "Epoch: 345 Loss: 1.4443798\n",
      "Epoch: 346 Loss: 1.370183\n",
      "Epoch: 347 Loss: 1.4031246\n",
      "Epoch: 348 Loss: 1.3559666\n",
      "Epoch: 349 Loss: 1.4569325\n",
      "Epoch: 350 Loss: 1.35351\n",
      "Epoch: 351 Loss: 1.5900784\n",
      "Epoch: 352 Loss: 1.6393516\n",
      "Epoch: 353 Loss: 1.467238\n",
      "Epoch: 354 Loss: 1.5575407\n",
      "Epoch: 355 Loss: 1.4475935\n",
      "Epoch: 356 Loss: 1.5083568\n",
      "Epoch: 357 Loss: 1.4527496\n",
      "Epoch: 358 Loss: 1.4467026\n",
      "Epoch: 359 Loss: 1.5040557\n",
      "Epoch: 360 Loss: 1.4240971\n",
      "Epoch: 361 Loss: 1.3788173\n",
      "Epoch: 362 Loss: 1.3740035\n",
      "Epoch: 363 Loss: 1.2924217\n",
      "Epoch: 364 Loss: 1.2977245\n",
      "Epoch: 365 Loss: 1.2996079\n",
      "Epoch: 366 Loss: 1.4643234\n",
      "Epoch: 367 Loss: 1.4373887\n",
      "Epoch: 368 Loss: 1.4705902\n",
      "Epoch: 369 Loss: 1.477468\n",
      "Epoch: 370 Loss: 1.4576304\n",
      "Epoch: 371 Loss: 1.3955791\n",
      "Epoch: 372 Loss: 1.459661\n",
      "Epoch: 373 Loss: 1.4795551\n",
      "Epoch: 374 Loss: 1.4173921\n",
      "Epoch: 375 Loss: 1.3271582\n",
      "Epoch: 376 Loss: 1.3480179\n",
      "Epoch: 377 Loss: 1.3298837\n",
      "Epoch: 378 Loss: 1.2659526\n",
      "Epoch: 379 Loss: 1.1571078\n",
      "Epoch: 380 Loss: 1.198658\n",
      "Epoch: 381 Loss: 1.4656975\n",
      "Epoch: 382 Loss: 1.4584144\n",
      "Epoch: 383 Loss: 1.610183\n",
      "Epoch: 384 Loss: 1.4734356\n",
      "Epoch: 385 Loss: 1.5967886\n",
      "Epoch: 386 Loss: 1.5305068\n",
      "Epoch: 387 Loss: 1.5025575\n",
      "Epoch: 388 Loss: 1.5283592\n",
      "Epoch: 389 Loss: 1.6396617\n",
      "Epoch: 390 Loss: 1.4811788\n",
      "Epoch: 391 Loss: 1.5284395\n",
      "Epoch: 392 Loss: 1.5001106\n",
      "Epoch: 393 Loss: 1.5471311\n",
      "Epoch: 394 Loss: 1.501615\n",
      "Epoch: 395 Loss: 1.4132621\n",
      "Epoch: 396 Loss: 1.4700584\n",
      "Epoch: 397 Loss: 1.3980865\n",
      "Epoch: 398 Loss: 1.4346886\n",
      "Epoch: 399 Loss: 1.4283633\n",
      "Epoch: 400 Loss: 1.4084756\n",
      "Epoch: 401 Loss: 1.3625178\n",
      "Epoch: 402 Loss: 1.5201634\n",
      "Epoch: 403 Loss: 1.3327833\n",
      "Epoch: 404 Loss: 1.4516047\n",
      "Epoch: 405 Loss: 1.4199808\n",
      "Epoch: 406 Loss: 1.3558843\n",
      "Epoch: 407 Loss: 1.373879\n",
      "Epoch: 408 Loss: 1.3299904\n",
      "Epoch: 409 Loss: 1.4425814\n",
      "Epoch: 410 Loss: 1.3182476\n",
      "Epoch: 411 Loss: 1.5444579\n",
      "Epoch: 412 Loss: 1.5828419\n",
      "Epoch: 413 Loss: 1.4695766\n",
      "Epoch: 414 Loss: 1.5288686\n",
      "Epoch: 415 Loss: 1.4515061\n",
      "Epoch: 416 Loss: 1.4814258\n",
      "Epoch: 417 Loss: 1.4446726\n",
      "Epoch: 418 Loss: 1.435873\n",
      "Epoch: 419 Loss: 1.5073457\n",
      "Epoch: 420 Loss: 1.4247758\n",
      "Epoch: 421 Loss: 1.3724501\n",
      "Epoch: 422 Loss: 1.3785431\n",
      "Epoch: 423 Loss: 1.308534\n",
      "Epoch: 424 Loss: 1.3004843\n",
      "Epoch: 425 Loss: 1.3090527\n",
      "Epoch: 426 Loss: 1.4058244\n",
      "Epoch: 427 Loss: 1.3629854\n",
      "Epoch: 428 Loss: 1.4468004\n",
      "Epoch: 429 Loss: 1.4362845\n",
      "Epoch: 430 Loss: 1.4314826\n",
      "Epoch: 431 Loss: 1.3611374\n",
      "Epoch: 432 Loss: 1.4392467\n",
      "Epoch: 433 Loss: 1.4895828\n",
      "Epoch: 434 Loss: 1.3938204\n",
      "Epoch: 435 Loss: 1.3050025\n",
      "Epoch: 436 Loss: 1.3361497\n",
      "Epoch: 437 Loss: 1.3190358\n",
      "Epoch: 438 Loss: 1.2553084\n",
      "Epoch: 439 Loss: 1.1601522\n",
      "Epoch: 440 Loss: 1.201198\n",
      "Epoch: 441 Loss: 1.4530869\n",
      "Epoch: 442 Loss: 1.4376848\n",
      "Epoch: 443 Loss: 1.5574948\n",
      "Epoch: 444 Loss: 1.4462025\n",
      "Epoch: 445 Loss: 1.5584311\n",
      "Epoch: 446 Loss: 1.5008781\n",
      "Epoch: 447 Loss: 1.4626483\n",
      "Epoch: 448 Loss: 1.4884508\n",
      "Epoch: 449 Loss: 1.5763376\n",
      "Epoch: 450 Loss: 1.4280288\n",
      "Epoch: 451 Loss: 1.4808148\n",
      "Epoch: 452 Loss: 1.4816794\n",
      "Epoch: 453 Loss: 1.4928691\n",
      "Epoch: 454 Loss: 1.4687846\n",
      "Epoch: 455 Loss: 1.380518\n",
      "Epoch: 456 Loss: 1.4661937\n",
      "Epoch: 457 Loss: 1.3926406\n",
      "Epoch: 458 Loss: 1.4194047\n",
      "Epoch: 459 Loss: 1.4295765\n",
      "Epoch: 460 Loss: 1.3915826\n",
      "Epoch: 461 Loss: 1.3589544\n",
      "Epoch: 462 Loss: 1.5012407\n",
      "Epoch: 463 Loss: 1.3355873\n",
      "Epoch: 464 Loss: 1.4436083\n",
      "Epoch: 465 Loss: 1.4174174\n",
      "Epoch: 466 Loss: 1.3440694\n",
      "Epoch: 467 Loss: 1.3705211\n",
      "Epoch: 468 Loss: 1.3357582\n",
      "Epoch: 469 Loss: 1.4378614\n",
      "Epoch: 470 Loss: 1.3199822\n",
      "Epoch: 471 Loss: 1.5426402\n",
      "Epoch: 472 Loss: 1.5981511\n",
      "Epoch: 473 Loss: 1.4537716\n",
      "Epoch: 474 Loss: 1.5441078\n",
      "Epoch: 475 Loss: 1.4432247\n",
      "Epoch: 476 Loss: 1.4866138\n",
      "Epoch: 477 Loss: 1.4536154\n",
      "Epoch: 478 Loss: 1.4516205\n",
      "Epoch: 479 Loss: 1.5005617\n",
      "Epoch: 480 Loss: 1.4263133\n",
      "Epoch: 481 Loss: 1.3869919\n",
      "Epoch: 482 Loss: 1.383136\n",
      "Epoch: 483 Loss: 1.3347554\n",
      "Epoch: 484 Loss: 1.3230515\n",
      "Epoch: 485 Loss: 1.3210167\n",
      "Epoch: 486 Loss: 1.3970921\n",
      "Epoch: 487 Loss: 1.3611908\n",
      "Epoch: 488 Loss: 1.4393185\n",
      "Epoch: 489 Loss: 1.4131293\n",
      "Epoch: 490 Loss: 1.4204428\n",
      "Epoch: 491 Loss: 1.3333797\n",
      "Epoch: 492 Loss: 1.4167929\n",
      "Epoch: 493 Loss: 1.4645199\n",
      "Epoch: 494 Loss: 1.3835647\n",
      "Epoch: 495 Loss: 1.3041004\n",
      "Epoch: 496 Loss: 1.318501\n",
      "Epoch: 497 Loss: 1.3095125\n",
      "Epoch: 498 Loss: 1.2585719\n",
      "Epoch: 499 Loss: 1.1520085\n",
      "Epoch: 500 Loss: 1.1969712\n",
      "Epoch: 501 Loss: 1.436687\n",
      "Epoch: 502 Loss: 1.4239116\n",
      "Epoch: 503 Loss: 1.5398635\n",
      "Epoch: 504 Loss: 1.4510084\n",
      "Epoch: 505 Loss: 1.5465283\n",
      "Epoch: 506 Loss: 1.4913592\n",
      "Epoch: 507 Loss: 1.4555492\n",
      "Epoch: 508 Loss: 1.4847096\n",
      "Epoch: 509 Loss: 1.6104131\n",
      "Epoch: 510 Loss: 1.4435831\n",
      "Epoch: 511 Loss: 1.4816043\n",
      "Epoch: 512 Loss: 1.4732611\n",
      "Epoch: 513 Loss: 1.5079699\n",
      "Epoch: 514 Loss: 1.484133\n",
      "Epoch: 515 Loss: 1.3834169\n",
      "Epoch: 516 Loss: 1.4373238\n",
      "Epoch: 517 Loss: 1.3739964\n",
      "Epoch: 518 Loss: 1.4153397\n",
      "Epoch: 519 Loss: 1.3989906\n",
      "Epoch: 520 Loss: 1.3748136\n",
      "Epoch: 521 Loss: 1.3418194\n",
      "Epoch: 522 Loss: 1.5070758\n",
      "Epoch: 523 Loss: 1.3092301\n",
      "Epoch: 524 Loss: 1.4480377\n",
      "Epoch: 525 Loss: 1.3867048\n",
      "Epoch: 526 Loss: 1.3314183\n",
      "Epoch: 527 Loss: 1.349133\n",
      "Epoch: 528 Loss: 1.3295202\n",
      "Epoch: 529 Loss: 1.4199463\n",
      "Epoch: 530 Loss: 1.3051496\n",
      "Epoch: 531 Loss: 1.5134323\n",
      "Epoch: 532 Loss: 1.5574467\n",
      "Epoch: 533 Loss: 1.4548931\n",
      "Epoch: 534 Loss: 1.5213833\n",
      "Epoch: 535 Loss: 1.441011\n",
      "Epoch: 536 Loss: 1.4772958\n",
      "Epoch: 537 Loss: 1.4342883\n",
      "Epoch: 538 Loss: 1.4323025\n",
      "Epoch: 539 Loss: 1.5114692\n",
      "Epoch: 540 Loss: 1.4135448\n",
      "Epoch: 541 Loss: 1.3773365\n",
      "Epoch: 542 Loss: 1.3856056\n",
      "Epoch: 543 Loss: 1.307661\n",
      "Epoch: 544 Loss: 1.3075235\n",
      "Epoch: 545 Loss: 1.3077457\n",
      "Epoch: 546 Loss: 1.3585224\n",
      "Epoch: 547 Loss: 1.3205221\n",
      "Epoch: 548 Loss: 1.4187691\n",
      "Epoch: 549 Loss: 1.4043064\n",
      "Epoch: 550 Loss: 1.3873863\n",
      "Epoch: 551 Loss: 1.3085265\n",
      "Epoch: 552 Loss: 1.3904827\n",
      "Epoch: 553 Loss: 1.4942603\n",
      "Epoch: 554 Loss: 1.3733696\n",
      "Epoch: 555 Loss: 1.2977796\n",
      "Epoch: 556 Loss: 1.3295946\n",
      "Epoch: 557 Loss: 1.2943113\n",
      "Epoch: 558 Loss: 1.2505985\n",
      "Epoch: 559 Loss: 1.1492668\n",
      "Epoch: 560 Loss: 1.1798737\n",
      "Epoch: 561 Loss: 1.462912\n",
      "Epoch: 562 Loss: 1.4241017\n",
      "Epoch: 563 Loss: 1.5044518\n",
      "Epoch: 564 Loss: 1.4493897\n",
      "Epoch: 565 Loss: 1.5207725\n",
      "Epoch: 566 Loss: 1.4826071\n",
      "Epoch: 567 Loss: 1.4434028\n",
      "Epoch: 568 Loss: 1.457622\n",
      "Epoch: 569 Loss: 1.5582584\n",
      "Epoch: 570 Loss: 1.3952363\n",
      "Epoch: 571 Loss: 1.4660255\n",
      "Epoch: 572 Loss: 1.4661978\n",
      "Epoch: 573 Loss: 1.4445077\n",
      "Epoch: 574 Loss: 1.4402066\n",
      "Epoch: 575 Loss: 1.3514483\n",
      "Epoch: 576 Loss: 1.426948\n",
      "Epoch: 577 Loss: 1.3823086\n",
      "Epoch: 578 Loss: 1.4051604\n",
      "Epoch: 579 Loss: 1.4146364\n",
      "Epoch: 580 Loss: 1.3647624\n",
      "Epoch: 581 Loss: 1.3410754\n",
      "Epoch: 582 Loss: 1.4879692\n",
      "Epoch: 583 Loss: 1.314975\n",
      "Epoch: 584 Loss: 1.4408613\n",
      "Epoch: 585 Loss: 1.3982742\n",
      "Epoch: 586 Loss: 1.3206446\n",
      "Epoch: 587 Loss: 1.342134\n",
      "Epoch: 588 Loss: 1.3338814\n",
      "Epoch: 589 Loss: 1.421741\n",
      "Epoch: 590 Loss: 1.3043073\n",
      "Epoch: 591 Loss: 1.5043069\n",
      "Epoch: 592 Loss: 1.5904756\n",
      "Epoch: 593 Loss: 1.4374154\n",
      "Epoch: 594 Loss: 1.5297855\n",
      "Epoch: 595 Loss: 1.4351754\n",
      "Epoch: 596 Loss: 1.4761326\n",
      "Epoch: 597 Loss: 1.4456829\n",
      "Epoch: 598 Loss: 1.4282143\n",
      "Epoch: 599 Loss: 1.4918623\n",
      "Epoch: 600 Loss: 1.4103227\n",
      "Epoch: 601 Loss: 1.3924911\n",
      "Epoch: 602 Loss: 1.3941873\n",
      "Epoch: 603 Loss: 1.3089417\n",
      "Epoch: 604 Loss: 1.3323939\n",
      "Epoch: 605 Loss: 1.3356668\n",
      "Epoch: 606 Loss: 1.3609395\n",
      "Epoch: 607 Loss: 1.3467733\n",
      "Epoch: 608 Loss: 1.432909\n",
      "Epoch: 609 Loss: 1.3960767\n",
      "Epoch: 610 Loss: 1.3916168\n",
      "Epoch: 611 Loss: 1.2871275\n",
      "Epoch: 612 Loss: 1.400025\n",
      "Epoch: 613 Loss: 1.4652281\n",
      "Epoch: 614 Loss: 1.3768573\n",
      "Epoch: 615 Loss: 1.2958491\n",
      "Epoch: 616 Loss: 1.3300853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 617 Loss: 1.3089665\n",
      "Epoch: 618 Loss: 1.2669182\n",
      "Epoch: 619 Loss: 1.1559304\n",
      "Epoch: 620 Loss: 1.1743021\n",
      "Epoch: 621 Loss: 1.4338845\n",
      "Epoch: 622 Loss: 1.4142257\n",
      "Epoch: 623 Loss: 1.5142689\n",
      "Epoch: 624 Loss: 1.4429355\n",
      "Epoch: 625 Loss: 1.5205086\n",
      "Epoch: 626 Loss: 1.4758408\n",
      "Epoch: 627 Loss: 1.4297085\n",
      "Epoch: 628 Loss: 1.4583211\n",
      "Epoch: 629 Loss: 1.5933996\n",
      "Epoch: 630 Loss: 1.4024162\n",
      "Epoch: 631 Loss: 1.4799466\n",
      "Epoch: 632 Loss: 1.4624243\n",
      "Epoch: 633 Loss: 1.4707102\n",
      "Epoch: 634 Loss: 1.4603125\n",
      "Epoch: 635 Loss: 1.3683522\n",
      "Epoch: 636 Loss: 1.4076896\n",
      "Epoch: 637 Loss: 1.3595269\n",
      "Epoch: 638 Loss: 1.391895\n",
      "Epoch: 639 Loss: 1.3799579\n",
      "Epoch: 640 Loss: 1.3388453\n",
      "Epoch: 641 Loss: 1.3109713\n",
      "Epoch: 642 Loss: 1.481111\n",
      "Epoch: 643 Loss: 1.285934\n",
      "Epoch: 644 Loss: 1.4377489\n",
      "Epoch: 645 Loss: 1.3673991\n",
      "Epoch: 646 Loss: 1.3118596\n",
      "Epoch: 647 Loss: 1.315045\n",
      "Epoch: 648 Loss: 1.3126951\n",
      "Epoch: 649 Loss: 1.3897561\n",
      "Epoch: 650 Loss: 1.2907052\n",
      "Epoch: 651 Loss: 1.4870887\n",
      "Epoch: 652 Loss: 1.5472314\n",
      "Epoch: 653 Loss: 1.4394007\n",
      "Epoch: 654 Loss: 1.5112517\n",
      "Epoch: 655 Loss: 1.4551058\n",
      "Epoch: 656 Loss: 1.4673494\n",
      "Epoch: 657 Loss: 1.4240532\n",
      "Epoch: 658 Loss: 1.4374292\n",
      "Epoch: 659 Loss: 1.4948647\n",
      "Epoch: 660 Loss: 1.4006538\n",
      "Epoch: 661 Loss: 1.3751979\n",
      "Epoch: 662 Loss: 1.3822534\n",
      "Epoch: 663 Loss: 1.2968171\n",
      "Epoch: 664 Loss: 1.3150079\n",
      "Epoch: 665 Loss: 1.3319888\n",
      "Epoch: 666 Loss: 1.3284043\n",
      "Epoch: 667 Loss: 1.3088791\n",
      "Epoch: 668 Loss: 1.4120305\n",
      "Epoch: 669 Loss: 1.3797377\n",
      "Epoch: 670 Loss: 1.3532841\n",
      "Epoch: 671 Loss: 1.2900052\n",
      "Epoch: 672 Loss: 1.3880727\n",
      "Epoch: 673 Loss: 1.4776473\n",
      "Epoch: 674 Loss: 1.3605392\n",
      "Epoch: 675 Loss: 1.2860332\n",
      "Epoch: 676 Loss: 1.3082702\n",
      "Epoch: 677 Loss: 1.2938468\n",
      "Epoch: 678 Loss: 1.2705178\n",
      "Epoch: 679 Loss: 1.1569507\n",
      "Epoch: 680 Loss: 1.1691597\n",
      "Epoch: 681 Loss: 1.4280751\n",
      "Epoch: 682 Loss: 1.4157443\n",
      "Epoch: 683 Loss: 1.4798082\n",
      "Epoch: 684 Loss: 1.4315107\n",
      "Epoch: 685 Loss: 1.4778416\n",
      "Epoch: 686 Loss: 1.466032\n",
      "Epoch: 687 Loss: 1.4039581\n",
      "Epoch: 688 Loss: 1.4339797\n",
      "Epoch: 689 Loss: 1.5400783\n",
      "Epoch: 690 Loss: 1.3582783\n",
      "Epoch: 691 Loss: 1.4506425\n",
      "Epoch: 692 Loss: 1.4612432\n",
      "Epoch: 693 Loss: 1.4290135\n",
      "Epoch: 694 Loss: 1.4322555\n",
      "Epoch: 695 Loss: 1.3410122\n",
      "Epoch: 696 Loss: 1.3960932\n",
      "Epoch: 697 Loss: 1.3615395\n",
      "Epoch: 698 Loss: 1.3866122\n",
      "Epoch: 699 Loss: 1.3810549\n",
      "Epoch: 700 Loss: 1.3266442\n",
      "Epoch: 701 Loss: 1.3184981\n",
      "Epoch: 702 Loss: 1.4559578\n",
      "Epoch: 703 Loss: 1.2924498\n",
      "Epoch: 704 Loss: 1.4292725\n",
      "Epoch: 705 Loss: 1.3848883\n",
      "Epoch: 706 Loss: 1.298209\n",
      "Epoch: 707 Loss: 1.317215\n",
      "Epoch: 708 Loss: 1.3127093\n",
      "Epoch: 709 Loss: 1.3860979\n",
      "Epoch: 710 Loss: 1.2609187\n",
      "Epoch: 711 Loss: 1.5131685\n",
      "Epoch: 712 Loss: 1.5827688\n",
      "Epoch: 713 Loss: 1.4390831\n",
      "Epoch: 714 Loss: 1.5235604\n",
      "Epoch: 715 Loss: 1.4476404\n",
      "Epoch: 716 Loss: 1.471703\n",
      "Epoch: 717 Loss: 1.4470167\n",
      "Epoch: 718 Loss: 1.436669\n",
      "Epoch: 719 Loss: 1.4880904\n",
      "Epoch: 720 Loss: 1.4043608\n",
      "Epoch: 721 Loss: 1.3847587\n",
      "Epoch: 722 Loss: 1.392503\n",
      "Epoch: 723 Loss: 1.3175223\n",
      "Epoch: 724 Loss: 1.3355467\n",
      "Epoch: 725 Loss: 1.3309765\n",
      "Epoch: 726 Loss: 1.3534551\n",
      "Epoch: 727 Loss: 1.318579\n",
      "Epoch: 728 Loss: 1.4584634\n",
      "Epoch: 729 Loss: 1.3745086\n",
      "Epoch: 730 Loss: 1.3560784\n",
      "Epoch: 731 Loss: 1.2696438\n",
      "Epoch: 732 Loss: 1.3928294\n",
      "Epoch: 733 Loss: 1.4665523\n",
      "Epoch: 734 Loss: 1.3633536\n",
      "Epoch: 735 Loss: 1.3046997\n",
      "Epoch: 736 Loss: 1.3120618\n",
      "Epoch: 737 Loss: 1.3045266\n",
      "Epoch: 738 Loss: 1.2685053\n",
      "Epoch: 739 Loss: 1.1549785\n",
      "Epoch: 740 Loss: 1.1674588\n",
      "Epoch: 741 Loss: 1.4385941\n",
      "Epoch: 742 Loss: 1.4372216\n",
      "Epoch: 743 Loss: 1.5097518\n",
      "Epoch: 744 Loss: 1.4006708\n",
      "Epoch: 745 Loss: 1.473943\n",
      "Epoch: 746 Loss: 1.4457011\n",
      "Epoch: 747 Loss: 1.404619\n",
      "Epoch: 748 Loss: 1.431623\n",
      "Epoch: 749 Loss: 1.5759804\n",
      "Epoch: 750 Loss: 1.3826598\n",
      "Epoch: 751 Loss: 1.4766972\n",
      "Epoch: 752 Loss: 1.4215215\n",
      "Epoch: 753 Loss: 1.423311\n",
      "Epoch: 754 Loss: 1.4343437\n",
      "Epoch: 755 Loss: 1.3124903\n",
      "Epoch: 756 Loss: 1.4695354\n",
      "Epoch: 757 Loss: 1.3604434\n",
      "Epoch: 758 Loss: 1.353364\n",
      "Epoch: 759 Loss: 1.379161\n",
      "Epoch: 760 Loss: 1.339264\n",
      "Epoch: 761 Loss: 1.2936642\n",
      "Epoch: 762 Loss: 1.4832082\n",
      "Epoch: 763 Loss: 1.3033745\n",
      "Epoch: 764 Loss: 1.3719825\n",
      "Epoch: 765 Loss: 1.261693\n",
      "Epoch: 766 Loss: 1.3152554\n",
      "Epoch: 767 Loss: 1.2936673\n",
      "Epoch: 768 Loss: 1.2794572\n",
      "Epoch: 769 Loss: 1.3002088\n",
      "Epoch: 770 Loss: 1.2514821\n",
      "Epoch: 771 Loss: 1.545287\n",
      "Epoch: 772 Loss: 1.58582\n",
      "Epoch: 773 Loss: 1.4747939\n",
      "Epoch: 774 Loss: 1.5136145\n",
      "Epoch: 775 Loss: 1.4538238\n",
      "Epoch: 776 Loss: 1.5189756\n",
      "Epoch: 777 Loss: 1.4586878\n",
      "Epoch: 778 Loss: 1.4630456\n",
      "Epoch: 779 Loss: 1.5015533\n",
      "Epoch: 780 Loss: 1.4127548\n",
      "Epoch: 781 Loss: 1.3584653\n",
      "Epoch: 782 Loss: 1.3832095\n",
      "Epoch: 783 Loss: 1.2406085\n",
      "Epoch: 784 Loss: 1.2724693\n",
      "Epoch: 785 Loss: 1.2954242\n",
      "Epoch: 786 Loss: 1.3183612\n",
      "Epoch: 787 Loss: 1.2779522\n",
      "Epoch: 788 Loss: 1.4038748\n",
      "Epoch: 789 Loss: 1.3815291\n",
      "Epoch: 790 Loss: 1.3295941\n",
      "Epoch: 791 Loss: 1.2581232\n",
      "Epoch: 792 Loss: 1.3168194\n",
      "Epoch: 793 Loss: 1.527457\n",
      "Epoch: 794 Loss: 1.3484186\n",
      "Epoch: 795 Loss: 1.2295055\n",
      "Epoch: 796 Loss: 1.2574977\n",
      "Epoch: 797 Loss: 1.1857904\n",
      "Epoch: 798 Loss: 1.1692127\n",
      "Epoch: 799 Loss: 1.0612459\n",
      "Epoch: 800 Loss: 1.071145\n",
      "Epoch: 801 Loss: 1.5241687\n",
      "Epoch: 802 Loss: 1.5399303\n",
      "Epoch: 803 Loss: 1.5558794\n",
      "Epoch: 804 Loss: 1.5176114\n",
      "Epoch: 805 Loss: 1.6002421\n",
      "Epoch: 806 Loss: 1.5090476\n",
      "Epoch: 807 Loss: 1.4623823\n",
      "Epoch: 808 Loss: 1.5273967\n",
      "Epoch: 809 Loss: 1.5765688\n",
      "Epoch: 810 Loss: 1.3958733\n",
      "Epoch: 811 Loss: 1.4526366\n",
      "Epoch: 812 Loss: 1.5200214\n",
      "Epoch: 813 Loss: 1.4448029\n",
      "Epoch: 814 Loss: 1.4219866\n",
      "Epoch: 815 Loss: 1.3669891\n",
      "Epoch: 816 Loss: 1.4595059\n",
      "Epoch: 817 Loss: 1.3798223\n",
      "Epoch: 818 Loss: 1.3753809\n",
      "Epoch: 819 Loss: 1.4452956\n",
      "Epoch: 820 Loss: 1.4267378\n",
      "Epoch: 821 Loss: 1.3517824\n",
      "Epoch: 822 Loss: 1.4297569\n",
      "Epoch: 823 Loss: 1.2274611\n",
      "Epoch: 824 Loss: 1.4030908\n",
      "Epoch: 825 Loss: 1.3777258\n",
      "Epoch: 826 Loss: 1.2839942\n",
      "Epoch: 827 Loss: 1.3047975\n",
      "Epoch: 828 Loss: 1.2725059\n",
      "Epoch: 829 Loss: 1.3142221\n",
      "Epoch: 830 Loss: 1.2521439\n",
      "Epoch: 831 Loss: 1.5857339\n",
      "Epoch: 832 Loss: 1.6518039\n",
      "Epoch: 833 Loss: 1.5288724\n",
      "Epoch: 834 Loss: 1.6364143\n",
      "Epoch: 835 Loss: 1.4835764\n",
      "Epoch: 836 Loss: 1.5382786\n",
      "Epoch: 837 Loss: 1.5617914\n",
      "Epoch: 838 Loss: 1.5104018\n",
      "Epoch: 839 Loss: 1.5220377\n",
      "Epoch: 840 Loss: 1.4350436\n",
      "Epoch: 841 Loss: 1.408256\n",
      "Epoch: 842 Loss: 1.4202175\n",
      "Epoch: 843 Loss: 1.3348587\n",
      "Epoch: 844 Loss: 1.3518155\n",
      "Epoch: 845 Loss: 1.2957772\n",
      "Epoch: 846 Loss: 1.5810823\n",
      "Epoch: 847 Loss: 1.5250243\n",
      "Epoch: 848 Loss: 1.6100981\n",
      "Epoch: 849 Loss: 1.6107728\n",
      "Epoch: 850 Loss: 1.5550958\n",
      "Epoch: 851 Loss: 1.4713578\n",
      "Epoch: 852 Loss: 1.5649117\n",
      "Epoch: 853 Loss: 1.5646961\n",
      "Epoch: 854 Loss: 1.4858227\n",
      "Epoch: 855 Loss: 1.4260552\n",
      "Epoch: 856 Loss: 1.4038013\n",
      "Epoch: 857 Loss: 1.4025962\n",
      "Epoch: 858 Loss: 1.3952763\n",
      "Epoch: 859 Loss: 1.235565\n",
      "Epoch: 860 Loss: 1.2470664\n",
      "Epoch: 861 Loss: 1.4847565\n",
      "Epoch: 862 Loss: 1.5042605\n",
      "Epoch: 863 Loss: 1.5799348\n",
      "Epoch: 864 Loss: 1.4962434\n",
      "Epoch: 865 Loss: 1.5963216\n",
      "Epoch: 866 Loss: 1.5019891\n",
      "Epoch: 867 Loss: 1.452296\n",
      "Epoch: 868 Loss: 1.4592602\n",
      "Epoch: 869 Loss: 1.5622808\n",
      "Epoch: 870 Loss: 1.353046\n",
      "Epoch: 871 Loss: 1.4337993\n",
      "Epoch: 872 Loss: 1.4682579\n",
      "Epoch: 873 Loss: 1.4645025\n",
      "Epoch: 874 Loss: 1.4147683\n",
      "Epoch: 875 Loss: 1.3298532\n",
      "Epoch: 876 Loss: 1.3855457\n",
      "Epoch: 877 Loss: 1.3549927\n",
      "Epoch: 878 Loss: 1.3287199\n",
      "Epoch: 879 Loss: 1.347914\n",
      "Epoch: 880 Loss: 1.3633642\n",
      "Epoch: 881 Loss: 1.3232012\n",
      "Epoch: 882 Loss: 1.3986256\n",
      "Epoch: 883 Loss: 1.2122238\n",
      "Epoch: 884 Loss: 1.396971\n",
      "Epoch: 885 Loss: 1.3515961\n",
      "Epoch: 886 Loss: 1.2375684\n",
      "Epoch: 887 Loss: 1.2583861\n",
      "Epoch: 888 Loss: 1.2293758\n",
      "Epoch: 889 Loss: 1.2286844\n",
      "Epoch: 890 Loss: 1.1327155\n",
      "Epoch: 891 Loss: 1.6363134\n",
      "Epoch: 892 Loss: 1.5775337\n",
      "Epoch: 893 Loss: 1.5643958\n",
      "Epoch: 894 Loss: 1.6510057\n",
      "Epoch: 895 Loss: 1.5744984\n",
      "Epoch: 896 Loss: 1.5379426\n",
      "Epoch: 897 Loss: 1.5106685\n",
      "Epoch: 898 Loss: 1.5294521\n",
      "Epoch: 899 Loss: 1.6397517\n",
      "Epoch: 900 Loss: 1.4626713\n",
      "Epoch: 901 Loss: 1.4587071\n",
      "Epoch: 902 Loss: 1.5141459\n",
      "Epoch: 903 Loss: 1.3672156\n",
      "Epoch: 904 Loss: 1.357995\n",
      "Epoch: 905 Loss: 1.3168492\n",
      "Epoch: 906 Loss: 1.5876623\n",
      "Epoch: 907 Loss: 1.4490316\n",
      "Epoch: 908 Loss: 1.6034636\n",
      "Epoch: 909 Loss: 1.542162\n",
      "Epoch: 910 Loss: 1.4867944\n",
      "Epoch: 911 Loss: 1.4081371\n",
      "Epoch: 912 Loss: 1.4481052\n",
      "Epoch: 913 Loss: 1.5130048\n",
      "Epoch: 914 Loss: 1.3952069\n",
      "Epoch: 915 Loss: 1.3482987\n",
      "Epoch: 916 Loss: 1.4271477\n",
      "Epoch: 917 Loss: 1.3903356\n",
      "Epoch: 918 Loss: 1.2849289\n",
      "Epoch: 919 Loss: 1.1942438\n",
      "Epoch: 920 Loss: 1.2437384\n",
      "Epoch: 921 Loss: 1.4294746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 922 Loss: 1.4327697\n",
      "Epoch: 923 Loss: 1.3659828\n",
      "Epoch: 924 Loss: 1.4128721\n",
      "Epoch: 925 Loss: 1.4358959\n",
      "Epoch: 926 Loss: 1.4282188\n",
      "Epoch: 927 Loss: 1.3597469\n",
      "Epoch: 928 Loss: 1.3730984\n",
      "Epoch: 929 Loss: 1.3866339\n",
      "Epoch: 930 Loss: 1.3068924\n",
      "Epoch: 931 Loss: 1.373078\n",
      "Epoch: 932 Loss: 1.4031185\n",
      "Epoch: 933 Loss: 1.3418512\n",
      "Epoch: 934 Loss: 1.3035254\n",
      "Epoch: 935 Loss: 1.2633023\n",
      "Epoch: 936 Loss: 1.4923573\n",
      "Epoch: 937 Loss: 1.4925152\n",
      "Epoch: 938 Loss: 1.4510636\n",
      "Epoch: 939 Loss: 1.4953983\n",
      "Epoch: 940 Loss: 1.3991747\n",
      "Epoch: 941 Loss: 1.4226688\n",
      "Epoch: 942 Loss: 1.4953146\n",
      "Epoch: 943 Loss: 1.3507348\n",
      "Epoch: 944 Loss: 1.4705598\n",
      "Epoch: 945 Loss: 1.4575803\n",
      "Epoch: 946 Loss: 1.3596855\n",
      "Epoch: 947 Loss: 1.3847039\n",
      "Epoch: 948 Loss: 1.3348219\n",
      "Epoch: 949 Loss: 1.395719\n",
      "Epoch: 950 Loss: 1.292139\n",
      "Epoch: 951 Loss: 1.6222067\n",
      "Epoch: 952 Loss: 1.6820779\n",
      "Epoch: 953 Loss: 1.512229\n",
      "Epoch: 954 Loss: 1.5940727\n",
      "Epoch: 955 Loss: 1.4537311\n",
      "Epoch: 956 Loss: 1.5360022\n",
      "Epoch: 957 Loss: 1.4917037\n",
      "Epoch: 958 Loss: 1.5057709\n",
      "Epoch: 959 Loss: 1.5333512\n",
      "Epoch: 960 Loss: 1.4787917\n",
      "Epoch: 961 Loss: 1.4457836\n",
      "Epoch: 962 Loss: 1.4519018\n",
      "Epoch: 963 Loss: 1.318908\n",
      "Epoch: 964 Loss: 1.3617446\n",
      "Epoch: 965 Loss: 1.3461827\n",
      "Epoch: 966 Loss: 1.3649156\n",
      "Epoch: 967 Loss: 1.3678328\n",
      "Epoch: 968 Loss: 1.4374146\n",
      "Epoch: 969 Loss: 1.4249343\n",
      "Epoch: 970 Loss: 1.3886267\n",
      "Epoch: 971 Loss: 1.4018292\n",
      "Epoch: 972 Loss: 1.4146497\n",
      "Epoch: 973 Loss: 1.4005177\n",
      "Epoch: 974 Loss: 1.3969133\n",
      "Epoch: 975 Loss: 1.3717811\n",
      "Epoch: 976 Loss: 1.3602822\n",
      "Epoch: 977 Loss: 1.3279316\n",
      "Epoch: 978 Loss: 1.3692265\n",
      "Epoch: 979 Loss: 1.2330964\n",
      "Epoch: 980 Loss: 1.2061497\n",
      "Epoch: 981 Loss: 1.4229586\n",
      "Epoch: 982 Loss: 1.4415655\n",
      "Epoch: 983 Loss: 1.5361292\n",
      "Epoch: 984 Loss: 1.4627173\n",
      "Epoch: 985 Loss: 1.4926007\n",
      "Epoch: 986 Loss: 1.5036967\n",
      "Epoch: 987 Loss: 1.4634814\n",
      "Epoch: 988 Loss: 1.4373491\n",
      "Epoch: 989 Loss: 1.5189921\n",
      "Epoch: 990 Loss: 1.3914549\n",
      "Epoch: 991 Loss: 1.4746759\n",
      "Epoch: 992 Loss: 1.453534\n",
      "Epoch: 993 Loss: 1.461345\n",
      "Epoch: 994 Loss: 1.3965319\n",
      "Epoch: 995 Loss: 1.360685\n",
      "Epoch: 996 Loss: 1.3972464\n",
      "Epoch: 997 Loss: 1.3932252\n",
      "Epoch: 998 Loss: 1.3796215\n",
      "Epoch: 999 Loss: 1.4169276\n",
      "Epoch: 1000 Loss: 1.3249905\n"
     ]
    }
   ],
   "source": [
    "# Setting Batch with Training\n",
    "batch_size = 32\n",
    "epoch = 1000\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('tf_board', sess.graph)\n",
    "    for i in range(epoch):\n",
    "        batch_data, batch_label = batch(trainlist, batch_size)     \n",
    "        _, loss, summary = sess.run([train_step, Loss, merged], feed_dict = {X: batch_data, Y: batch_label})\n",
    "        print(\"Epoch:\",i+1,\"Loss:\",loss)\n",
    "        if i % 10 == 0:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)\n",
    "        elif i+1 == epoch:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs\\model.ckpt-1000\n",
      "[ 0.03118621 -0.7509834  -0.3411302   0.15451182] 0\n",
      "[-0.29599497 -0.00972899  0.69436216 -0.34930193] 0\n",
      "[ 0.20180067  0.685225    0.6394979  -0.24031797] 0\n",
      "[ 0.3289079   0.87433904  0.48748899 -0.33216542] 0\n",
      "[ 0.11306528  0.06574048 -0.15619108  0.61677486] 0\n",
      "[ 0.12965593 -0.67751515 -0.2887019   0.49423605] 0\n",
      "[-0.29129517 -0.87265456 -0.6146739   0.27174616] 0\n",
      "[-0.31972718  0.1364482   0.00375311 -0.13819927] 0\n",
      "[ 0.0067321   0.32318392 -0.36040044 -0.51187325] 0\n",
      "[ 0.04719917  0.14003897 -0.19405986 -0.14264196] 0\n",
      "[ 0.30392346  0.63925725  0.3851918  -0.41005743] 0\n",
      "[ 0.19096169  0.6999493   0.3182152  -0.38203782] 0\n",
      "[0.33381504 0.6921789  0.47539198 0.67109245] 0\n",
      "[ 0.11297365  0.717613    0.43210894 -0.33377308] 0\n",
      "[ 0.11597645  0.7560584   0.54772615 -0.16456857] 0\n",
      "[ 0.45049363  1.091221    0.18170224 -0.7843885 ] 0\n",
      "[-0.42932796 -0.8593974  -0.5069361  -0.3514501 ] 0\n",
      "[ 0.0151442  -0.34857088 -0.51285434  0.20732044] 0\n",
      "[-0.106853   -0.25246027 -0.12668109 -0.02932567] 0\n",
      "[-0.14187925  0.4113908   0.4025234  -0.38408095] 0\n",
      "[ 0.22763461  0.23893052 -0.3983503   0.42806417] 0\n",
      "[0.3430553  0.60812646 0.6996796  0.16026278] 0\n",
      "[ 0.05437959 -0.39714295  0.10650909  0.5225921 ] 0\n",
      "[ 0.23160754 -0.7519223  -0.22746605  0.67064774] 0\n",
      "[-0.45181534 -0.69680524 -0.6391453   0.06488572] 0\n",
      "[ 0.26748428 -0.16032842  0.1074716   0.90891445] 0\n",
      "[ 0.10594616 -0.18693286 -0.32754618  0.3893137 ] 0\n",
      "[-0.26707712  0.47290567 -0.20237038 -0.3688293 ] 0\n",
      "[-0.3623323  -0.4335587  -0.3554749   0.16072433] 0\n",
      "[ 0.10800656 -0.03279844  0.59470403  0.4351774 ] 0\n",
      "[ 0.1757998  -0.29540387 -0.4251333   0.54366887] 0\n",
      "[-0.42386198 -0.5103197  -0.70286345  0.03300013] 0\n",
      "[-0.5604425  -0.5958569  -0.4180429  -0.23670685] 0\n",
      "[ 0.22954108  0.8131967   0.21305685 -0.80251503] 0\n",
      "[-0.2876676  -0.48210305  0.38275248  0.13720573] 0\n",
      "[ 0.2540618   0.54381007  0.06864387 -0.6285533 ] 0\n",
      "[ 0.11581246 -0.67032474 -0.40059298  0.44500202] 0\n",
      "[-0.34151575 -0.49320185 -0.31006336 -0.3840018 ] 0\n",
      "[ 0.0641832  -0.623105   -0.41849822  0.18022613] 0\n",
      "[-0.05226296 -0.06408089  0.02293034 -0.6467552 ] 0\n",
      "[-0.28047672  0.41238716  0.6384323   0.03408501] 0\n",
      "[-0.22568955 -0.8190516  -0.26419324  0.44844526] 0\n",
      "[-0.2828663  -1.1810492   0.13990636  0.6940394 ] 0\n",
      "[-0.01875734 -0.27647406 -0.49676156  0.5025669 ] 0\n",
      "[ 0.14760505 -0.7793758  -0.3489203   0.37330598] 0\n",
      "[-0.14224134 -0.37681237 -0.33157325  0.52859   ] 0\n",
      "[ 0.3106758  -1.1194029   0.37782234  0.55519086] 0\n",
      "[ 0.24521731 -1.2454342   0.42773515  0.3002715 ] 0\n",
      "[-0.3518506  -1.1742848  -0.56974906  0.04310684] 0\n",
      "[-0.06000496 -0.78803754 -0.08745188  0.7480673 ] 0\n",
      "[ 0.17626674  0.67699337  0.30886424 -0.4365158 ] 0\n",
      "[-0.19592285 -1.414407   -0.1541577   0.01029383] 0\n",
      "[ 0.02148065 -0.73927444 -0.24889037  0.27878678] 0\n",
      "[-0.72851765 -0.39689022 -0.7030442  -0.23397726] 0\n",
      "[ 0.38988227 -0.05492224  0.3411547   0.4032603 ] 0\n",
      "[-0.03611493  0.6160949   0.42070723 -0.2679757 ] 0\n",
      "[-0.12874946 -0.90840113 -0.5557501   0.20452042] 0\n",
      "[-0.10497992 -0.5758419  -0.37415946  0.2861917 ] 0\n",
      "[-0.08165865 -0.5474729  -0.48871565  0.01419027] 0\n",
      "[ 0.20379496  0.8782604   0.53591377 -0.46099323] 0\n",
      "[ 0.04501321  0.29310384 -0.8343778  -0.82745147] 0\n",
      "[ 0.36800864 -0.4405843   0.16917978  0.5145488 ] 0\n",
      "[ 0.06648872 -0.02413302  0.1693088   0.27569026] 0\n",
      "[-0.05990635  0.2671994   0.3744892  -0.12348519] 0\n",
      "[-0.5405864  -0.06545424  0.4789908   0.03648555] 0\n",
      "[ 0.1806966  -0.28306216 -0.06995114  0.60306334] 0\n",
      "[ 0.1855126   0.8466245   0.45089257 -0.5223981 ] 0\n",
      "[-0.18603101 -0.33040842  0.21185039 -0.26139688] 0\n",
      "[0.17018186 0.3378384  0.604707   0.3085078 ] 0\n",
      "[ 0.04102595 -0.55226773  0.25929254 -0.61859   ] 0\n",
      "[-0.24843095 -0.19712976 -0.08016686 -0.36839545] 0\n",
      "[ 0.06974564  0.32627633 -0.40269858  0.6492082 ] 0\n",
      "[ 0.20143957 -0.02755613 -0.03207774  0.25497574] 0\n",
      "[-0.3171768  -0.7225466   0.43139285  0.04910803] 0\n",
      "[ 0.38345423  0.6810421   0.0910956  -0.615564  ] 0\n",
      "[ 0.460114   -0.40509832  0.3963433   0.7218519 ] 0\n",
      "[-0.34869027 -0.98788315 -0.4263587  -0.09019747] 0\n",
      "[ 0.06622505 -0.70052004 -0.00226496  0.4368469 ] 0\n",
      "[ 0.04878788  0.70126975 -0.4219585  -0.5429318 ] 0\n",
      "[-0.23292528 -1.2331601  -0.16563709  0.59072036] 0\n",
      "[ 0.01991105 -0.8763282  -0.19172315  0.40068763] 0\n",
      "[-0.04308239 -0.36307222 -0.1546362   0.63038397] 0\n",
      "[ 0.28397653  0.46955064 -0.10388371 -0.60950065] 0\n",
      "[0.187261   0.03066592 0.6091122  0.61859715] 0\n",
      "[-0.18842025  0.5643389   0.36193377 -0.44132513] 0\n",
      "[ 0.25246412  0.5333592   0.36474264 -0.24069975] 0\n",
      "[-0.5134306  -0.9368132  -0.28424513 -0.50110745] 0\n",
      "[-0.08417242 -0.31105006  0.24653839  0.65976894] 0\n",
      "[ 0.30378848 -0.79995984 -0.3658504   0.54839444] 0\n",
      "[ 0.07237398 -0.42641562 -0.44684094  0.5320631 ] 0\n",
      "[ 0.01657756  0.7974344   0.53027326 -0.24789   ] 0\n",
      "[-0.34677878 -0.42095327 -0.5649445  -0.18454418] 0\n",
      "[-0.30578864 -0.9091154  -0.52620786  0.17190816] 0\n",
      "[ 0.0749293  -0.40030158 -0.21144181  0.46788383] 0\n",
      "[-0.58820945 -0.8866381  -0.53785753 -0.1284871 ] 0\n",
      "[ 0.22397429 -0.1051295  -0.3023518   0.3207538 ] 0\n",
      "[-0.12229551 -0.6900831  -0.36165047  0.45846647] 0\n",
      "[-0.08042737 -0.80722415 -0.53816     0.27032667] 0\n",
      "[-0.27998868 -0.74603873 -0.59986556  0.21445827] 0\n",
      "[ 0.01706291 -0.7900803  -0.45410174  0.31896365] 0\n",
      "[ 0.36541334  0.842822    0.4658345  -0.5136632 ] 0\n",
      "[-0.03600461 -0.25135648 -0.503073    0.45046276] 0\n",
      "[ 0.08049745 -0.27719876 -0.22690742  0.32734972] 0\n",
      "[-0.12802573  0.11264679 -0.00801726  0.36263055] 0\n",
      "[ 0.23688014  0.67281294  0.61231154 -0.19213484] 0\n",
      "[0.63670665 0.02399201 0.4396963  0.6863047 ] 0\n",
      "[-0.02960892  1.1293889   0.43767577 -0.05677272] 0\n",
      "[-0.00561126  0.13701129  0.43335587 -0.20398867] 0\n",
      "[ 0.0552744   0.8286158   0.5259479  -0.20179823] 0\n",
      "[-0.42545283 -1.2800714  -0.44304317  0.5106205 ] 0\n",
      "[-0.4055756  -0.9706676  -0.01960669 -0.33636767] 0\n",
      "[-0.34234992 -0.9402368  -0.4101888   0.28203106] 0\n",
      "[-0.12324924 -0.47219175 -0.53935075  0.4750772 ] 0\n",
      "[ 0.3552312  -0.0920579  -0.26546627  0.49518073] 0\n",
      "[ 0.12539607  0.7085848   0.00180103 -0.46478617] 0\n",
      "[ 0.29031765  0.6331938  -0.0297476  -0.23060748] 0\n",
      "[ 0.14093782 -0.6328725  -0.31792197  0.25906515] 0\n",
      "[-0.11692589 -0.7614718  -0.5454927   0.25979424] 0\n",
      "[-0.46034253 -1.0582774  -0.6017708   0.13186027] 0\n",
      "[-0.66767395 -0.6622798  -0.59161603 -0.42318916] 0\n",
      "[-0.2326617  -0.06975284  0.2506439   0.34593666] 0\n",
      "[-0.10674708 -0.97397864 -0.49549037  0.3042578 ] 0\n",
      "[-0.0648477  -0.7670982  -0.38066     0.28319162] 0\n",
      "[-0.0668867  -0.67192256 -0.3591836   0.49650502] 1\n",
      "[-0.00986635 -0.8061555  -0.41993362  0.3468067 ] 1\n",
      "[-0.4145894  -0.49945375  0.23293827  0.16173695] 1\n",
      "[-0.24136165 -0.51855993 -0.17954704 -0.16870369] 1\n",
      "[-0.6483255  -1.2458442  -0.55642354  0.03572969] 1\n",
      "[0.26589027 0.22403044 0.74341744 0.33310312] 1\n",
      "[ 0.00596097 -0.7193352  -0.4610598   0.35892665] 1\n",
      "[-0.17232168 -0.7443772  -0.43275738 -0.07383531] 1\n",
      "[0.15443723 0.4547388  0.71648985 0.08043529] 1\n",
      "[ 0.16068669 -0.36850205 -0.15967163  0.5577489 ] 1\n",
      "[-0.3779204  -0.38849083  0.4877919   0.89401984] 1\n",
      "[-0.18338713 -0.96585315 -0.49136716 -0.16462533] 1\n",
      "[0.40592912 0.35189047 0.73128283 0.69871134] 1\n",
      "[ 0.08429805 -0.49575388 -0.21486314  0.607821  ] 1\n",
      "[ 0.73256975  0.7207963   0.08500008 -0.18579876] 1\n",
      "[-0.16927797 -0.69643104 -0.5192289  -0.32634526] 1\n",
      "[-0.0181247  -0.68432164 -0.21326831  0.6437437 ] 1\n",
      "[-0.31414232 -0.8415073  -0.67615944  0.02462558] 1\n",
      "[-0.0857106  -0.33726516 -0.3217262  -0.33748543] 1\n",
      "[ 0.27777287 -0.66617197 -0.2860286   0.53775436] 1\n",
      "[0.16975258 0.490616   0.6102603  0.20996712] 1\n",
      "[-0.22325055 -0.7170577  -0.16485539 -0.3194564 ] 1\n",
      "[-0.06901816  0.3851295   0.64679784  0.24388038] 1\n",
      "[-0.34701902 -0.9613213  -0.12690431  0.43637556] 1\n",
      "[ 0.03095545 -0.21727785 -0.44835627  0.4601798 ] 1\n",
      "[ 0.1700492  -0.07489575 -0.3334923   0.35008895] 1\n",
      "[-0.16721593 -0.6382014  -0.42298663  0.58148134] 1\n",
      "[-0.1627088  -0.80831546 -0.5396313   0.44661856] 1\n",
      "[-0.14893511 -0.758043   -0.5547897   0.4659986 ] 1\n",
      "[-0.4340526  -0.5875551  -0.73094857 -0.1558539 ] 1\n",
      "[-0.08517604 -0.09797792 -0.2509811  -0.41759658] 1\n",
      "[ 0.19181368 -0.26741967 -0.18923336  0.49173725] 1\n",
      "[-0.293011  -0.6923903 -0.522649  -0.4597726] 1\n",
      "[ 0.00894134 -0.5675801  -0.37626415  0.2150997 ] 1\n",
      "[-0.48067757 -0.68862754  0.12049471  0.03021653] 1\n",
      "[-0.55225766 -0.42281777 -0.5109613   0.12644593] 1\n",
      "[-0.42623016 -0.7436921  -0.08880442  0.06595704] 1\n",
      "[ 0.31092793 -0.24331897 -0.2479792   0.13192488] 1\n",
      "[0.0800123  0.35816804 0.76259226 0.11401071] 1\n",
      "[ 0.41344777 -0.29725048 -0.2452737  -0.17644289] 1\n",
      "[-0.253622   -0.666475    0.08185448  0.49474865] 1\n",
      "[-0.64366615 -1.11788    -0.5002886   0.21513946] 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3856552   0.5681018   0.6409953  -0.32014582] 1\n",
      "[ 0.44908938 -0.33811733 -0.20125535  0.17717104] 1\n",
      "[ 0.31505796 -0.0890376   0.15105163 -0.01600879] 1\n",
      "[-0.4032459  -1.3947339  -0.33602485  0.52493703] 1\n",
      "[ 0.33543706 -0.25665864 -0.12358163 -0.5863059 ] 1\n",
      "[ 0.11301622  0.59806585  0.8039702  -0.0832848 ] 1\n",
      "[ 0.32072717 -0.0213178   0.37491727  0.8169246 ] 1\n",
      "[-0.2421089  -0.5553005  -0.55685484 -0.48717433] 1\n",
      "[ 0.01600491 -0.6271948  -0.354168    0.22078146] 1\n",
      "[0.05194828 0.31597835 0.6708177  0.2545954 ] 1\n",
      "[ 0.19741166  0.46816036  0.5666187  -0.6526624 ] 1\n",
      "[-0.12256848 -0.16565062  0.13269629  0.00279833] 1\n",
      "[ 0.07931025 -0.31539422 -0.36157972 -0.3178647 ] 1\n",
      "[-0.18391307 -0.5814933  -0.23616894  0.68584853] 1\n",
      "[-0.0898267  -0.02919852  0.01486174  0.8555411 ] 1\n",
      "[-0.22691594 -0.82083476 -0.53681546  0.2919857 ] 1\n",
      "[-0.26882705 -0.8100487  -0.3004893   0.19256686] 1\n",
      "[ 0.30403033  0.13402796 -0.09068889  0.33954734] 1\n",
      "[ 0.1148923  -0.6086484   0.09132384 -0.18497354] 1\n",
      "[-0.27105966 -0.7516904  -0.704707   -0.15355888] 1\n",
      "[-0.33099434 -0.29953635 -0.10582259 -0.43844563] 1\n",
      "[-0.3611364  -0.4866436  -0.13197288 -0.26123524] 1\n",
      "[ 0.3181387  -0.23219375 -0.14281595  0.22744994] 1\n",
      "[-0.30069962 -0.7768509   0.24717616 -0.24152166] 1\n",
      "[-0.34829387 -0.8783731  -0.42197347 -0.08156762] 1\n",
      "[-0.22729059 -0.49731284 -0.0840134  -0.03255595] 1\n",
      "[-0.1565684  -0.79538095  0.30927628  0.14161725] 1\n",
      "[ 0.29945233 -0.8155353   0.22489278  0.17748703] 1\n",
      "[-0.32016367 -0.27024856 -0.19095846 -0.5511446 ] 1\n",
      "[ 0.18246372 -0.47161412  0.02376801  0.03149727] 1\n",
      "[-0.07857923 -0.79835963 -0.46141958  0.2140568 ] 1\n",
      "[-0.27428958 -0.57330716 -0.6020042  -0.39697075] 1\n",
      "[ 0.19888137 -0.3750542  -0.18718258  0.16632436] 1\n",
      "[ 0.0589036  -0.41070676 -0.3654588   0.46649218] 1\n",
      "[-0.46276563 -1.579873    0.12877066  0.2202947 ] 1\n",
      "[-0.21416388 -1.1929246  -0.32872516 -0.12530498] 1\n",
      "[-0.19041231 -0.56434757 -0.4352697   0.49984854] 1\n",
      "[-0.22636825 -0.58318716 -0.18795064 -0.28280362] 1\n",
      "[ 0.5305587  -0.13306412  0.22568043  0.05544201] 1\n",
      "[ 0.4612171  -0.05592571  0.14265089  0.09410153] 1\n",
      "[ 0.5684239   0.37777472  0.13402368 -0.6137687 ] 1\n",
      "[ 0.28032413 -0.16222946  0.69530904  0.44073582] 1\n",
      "[-0.3454485  -0.83249885 -0.5718527   0.50597805] 1\n",
      "[-0.44094673 -0.5693866  -0.36501372  0.06024487] 1\n",
      "[0.01217164 0.34941068 0.5718224  0.12195577] 1\n",
      "[-0.22742768 -1.0750396  -0.15520033  0.5462318 ] 1\n",
      "[ 0.2440375  -0.00310304 -0.23467147 -0.03001939] 1\n",
      "[-0.5393002  -0.93318516 -0.24403946 -0.08811944] 1\n",
      "[-0.13813595 -0.80633557 -0.00438634 -0.15191962] 1\n",
      "[-0.40948516 -0.6321186  -0.4445278  -0.31163034] 1\n",
      "[-0.2595433  -0.7970557  -0.41746312 -0.22677791] 1\n",
      "[ 0.18387704  0.04845896 -0.34710264  0.01473394] 1\n",
      "[-0.01901343 -0.5316728  -0.5847078   0.2842344 ] 1\n",
      "[-0.16652036 -0.03064807  0.8016632   0.37973905] 1\n",
      "[-0.06015902 -0.56155694 -0.4921211   0.46785545] 1\n",
      "[ 0.00741857 -0.8930777   0.30257952  0.5934028 ] 1\n",
      "[-0.7140933  -0.5669304  -0.47294587 -0.08144966] 1\n",
      "[ 0.19969852 -0.44608742 -0.03519637  0.8917258 ] 1\n",
      "[ 0.48172423 -0.1036787   0.11658286  0.39884079] 1\n",
      "[ 0.32437378 -0.86743724 -0.11285204  0.30496788] 1\n",
      "[ 0.23473625 -0.2044837  -0.53220344  0.23383524] 1\n",
      "[ 0.06117613  0.34415364  0.3720678  -0.39603162] 1\n",
      "[ 0.21721198 -0.61464685 -0.12623486  0.703444  ] 1\n",
      "[ 0.19598196 -0.59637743  0.37023365  0.73598224] 1\n",
      "[ 0.01987653 -0.71169    -0.2888592   0.5508052 ] 1\n",
      "[ 0.19265838 -0.29790008 -0.37137198  0.1880769 ] 1\n",
      "[ 0.05741949  0.33285448  0.72419876 -0.03905118] 1\n",
      "[-0.3747038  -0.97330827 -0.7429314  -0.26145384] 1\n",
      "[-0.39881963 -0.32254136  0.19295378  0.10623895] 1\n",
      "[ 0.0499084  -0.92754877 -0.3406499   0.2200296 ] 1\n",
      "[ 0.44482267 -0.00866234 -0.3515789  -0.3390328 ] 2\n",
      "[ 0.53522694 -0.28863645 -0.12506968  0.3148306 ] 2\n",
      "[ 0.1660796  -0.46963885 -0.09008964  0.08393842] 2\n",
      "[-0.14444003 -0.74405783 -0.46573925 -0.05865464] 2\n",
      "[ 0.55040216  0.41784793 -0.39925146 -0.70904577] 2\n",
      "[-0.3042363   0.09694946 -0.10031775 -0.26081708] 2\n",
      "[-0.23037095 -1.0140059  -0.6676724   0.01026531] 2\n",
      "[-0.83378214 -0.66670734 -0.36895245  0.19036739] 2\n",
      "[ 0.04305231 -0.28592378 -0.15214026  0.39712495] 2\n",
      "[ 0.33885175 -0.14039153  0.3528577   0.86436594] 2\n",
      "[-0.09838176 -1.0756404   0.15881626  0.47516483] 2\n",
      "[0.35779187 0.6157492  0.56334156 0.00977641] 2\n",
      "[-0.22501568 -0.58596665 -0.6663119  -0.4474002 ] 2\n",
      "[ 0.06609365 -0.36564544 -0.23362742  0.23476936] 2\n",
      "[ 0.56833583  0.84890175  0.17432515 -0.6568528 ] 2\n",
      "[ 0.42334425  0.40477943 -0.4328819   0.5898882 ] 2\n",
      "[-0.21613248 -0.17846216 -0.026721    0.5650602 ] 2\n",
      "[-0.1318389   0.6318036   0.6187676   0.00305553] 2\n",
      "[-0.13287234 -0.15240745 -0.6527767   0.08034255] 2\n",
      "[-0.34359604 -0.64535904 -0.41092694 -0.37474537] 2\n",
      "[ 0.8709581  -0.05506263  0.4109996   0.646746  ] 2\n",
      "[-0.32243958 -0.79863405 -0.42545277  0.3144362 ] 2\n",
      "[-0.04398114 -0.49653482 -0.28899795  0.53945166] 2\n",
      "[-0.27961218 -0.6474414  -0.46475542 -0.31123602] 2\n",
      "[-0.25489733 -0.34307587 -0.33331156 -0.5702345 ] 2\n",
      "[-0.17042293 -0.18084788 -0.302406    0.07455355] 2\n",
      "[0.53140897 0.07419372 0.18098958 0.6507901 ] 2\n",
      "[ 0.10726638  0.30608112 -0.6553184  -0.49131745] 2\n",
      "[ 0.30071044  0.5436126   0.39211088 -0.04557284] 2\n",
      "[ 0.10239729  0.53314227 -0.10073902 -0.6504894 ] 2\n",
      "[ 0.11313785 -0.50179064  0.00888081  1.0111676 ] 2\n",
      "[-0.18382342  0.2818123  -0.04621077 -0.59132564] 2\n",
      "[-0.13038622 -1.0374848  -0.18012354 -0.30569518] 2\n",
      "[-0.19072452 -0.8770026  -0.62195015  0.01063728] 2\n",
      "[-0.07508221 -0.10748613 -0.36835903 -0.21707094] 2\n",
      "[-0.11653358 -0.319313   -0.34956312 -0.4558581 ] 2\n",
      "[ 0.0912165  -0.5564164  -0.36926657  0.43041056] 2\n",
      "[-0.08002498 -0.5041977  -0.17557654  0.53018403] 2\n",
      "[-0.41543895 -0.2963928  -0.4990934  -0.0565799 ] 2\n",
      "[-0.62079775 -0.7332696  -0.39893037  0.16904517] 2\n",
      "[-0.360197   -0.8820335  -0.6546655   0.21535783] 2\n",
      "[-0.50308645 -1.2210708  -0.48064435  0.39447212] 2\n",
      "[ 0.23962985  0.49968877  0.36852074 -0.32231173] 2\n",
      "[ 0.15832192 -0.0669526   0.43037492  0.9188098 ] 2\n",
      "[-0.46240607 -1.0641168  -0.5687583  -0.17231894] 2\n",
      "[-0.40318426 -0.07084718  0.00799904  0.4185928 ] 2\n",
      "[ 0.5989062  -0.20540142  0.2807327   0.46051466] 2\n",
      "[ 0.30278054  0.58025867 -0.72020376 -0.74836445] 2\n",
      "[ 0.08744179 -0.19260004 -0.43441188 -0.41117978] 2\n",
      "[-0.31609324  0.07799795 -0.11923552 -0.6989033 ] 2\n",
      "[ 0.18075104 -0.7416979  -0.3811044   0.59520954] 2\n",
      "[ 0.32415026  0.04625035  0.30397862 -0.7470018 ] 2\n",
      "[-0.08108657  0.31690192 -0.22691804 -0.32510468] 2\n",
      "[ 0.05577611 -0.4529595  -0.47680885  0.2394266 ] 2\n",
      "[-0.57226187 -0.7841194  -0.4356724   0.31456554] 2\n",
      "[-0.06658181 -0.6220072  -0.04632886  0.6615883 ] 2\n",
      "[ 0.00776305 -0.32055402 -0.5466062   0.64920944] 2\n",
      "[ 0.42755085 -0.2926472  -0.3517534   0.2729569 ] 2\n",
      "[-0.18601456 -0.5126203  -0.7359283  -0.2593273 ] 2\n",
      "[ 0.14759961 -0.52982914 -0.38974804 -0.30729645] 2\n",
      "[-0.04167674 -0.331384   -0.4119429  -0.43084472] 2\n",
      "[ 0.02379248 -0.47760084 -0.22174819  0.6864191 ] 2\n",
      "[-0.3165687  -0.07560372 -0.29740697 -0.45591307] 2\n",
      "[-0.19111304 -1.2505349  -0.27802002  0.05734137] 2\n",
      "[ 0.15032898 -0.3765289  -0.2667334   0.12377681] 2\n",
      "[-0.01988463 -0.64369303  0.03319123  1.1550685 ] 2\n",
      "[-0.02052056 -0.41763818 -0.27759755 -0.43812698] 2\n",
      "[-0.07460788  0.03663955 -0.3667764  -0.19381905] 2\n",
      "[ 0.13678192 -0.8006121   0.07358451  0.8104162 ] 2\n",
      "[-0.52533424 -1.2891833   0.04212049  0.24435611] 2\n",
      "[ 0.520798   -0.13617752  0.35374188  0.7408277 ] 2\n",
      "[0.27080613 0.05038538 0.43515646 0.49988753] 2\n",
      "[-0.35482636 -1.0581563  -0.3703243   0.3363223 ] 2\n",
      "[ 0.23215534 -0.851441    0.10569379  0.7237064 ] 2\n",
      "[ 0.18108983  0.3658895  -0.06888494 -0.33251262] 2\n",
      "[-0.5459865  -0.9888171  -0.67169505 -0.23557027] 2\n",
      "[ 0.29670084  0.8318902   0.5327139  -0.19866091] 2\n",
      "[-0.02994715 -0.61777896 -0.14410228  0.6910134 ] 2\n",
      "[-4.8070800e-02 -7.4386275e-01  2.8000772e-04  9.8320925e-01] 2\n",
      "[-0.03027708 -0.22167368 -0.09075463  0.5689405 ] 2\n",
      "[ 0.06853549  0.6334474   0.5322112  -0.14102462] 2\n",
      "[ 0.15098786  0.48832312 -0.44341522 -0.8848812 ] 2\n",
      "[ 0.27079996 -0.29236796  0.18536036  0.63052815] 2\n",
      "[-0.35199642 -0.58375895 -0.73989934 -0.02929136] 2\n",
      "[ 0.92311007  1.0519178   0.50577426 -0.14967372] 2\n",
      "[-0.00749735 -0.54806805 -0.32158118  0.57034737] 2\n",
      "[-0.35151342 -0.20606154 -0.22228205 -0.21853308] 2\n",
      "[-0.22327076 -0.888025   -0.3101744   0.4862646 ] 2\n",
      "[-0.19727492 -0.8323384  -0.5176151   0.405227  ] 2\n",
      "[ 0.1968105  -0.2800424  -0.24502562  0.07559048] 2\n",
      "[ 0.29002437  0.05887799 -0.00895302  0.6425754 ] 2\n",
      "[-0.1170764   0.1956897   0.03307246 -0.28625464] 2\n",
      "[ 0.53262824 -0.66971684  0.41077995  1.034922  ] 2\n",
      "[ 0.17347464 -0.09156208  0.43621916  0.90827465] 2\n",
      "[ 0.11008315 -0.12262315 -0.3071288   0.14319067] 2\n",
      "[-0.08393104 -0.817561   -0.42367637  0.23614822] 2\n",
      "[-0.24193309 -0.9459984  -0.5717041   0.41525638] 2\n",
      "[ 0.04774533 -0.08503984 -0.461034   -0.08250111] 2\n",
      "[ 0.03903963 -0.77294546 -0.14431943  0.86693144] 2\n",
      "[-0.03530921 -1.0306414  -0.36955363 -0.8082055 ] 2\n",
      "[ 0.41928637 -0.15850721 -0.1778952   0.69176143] 2\n",
      "[ 0.2167622  -0.8496309   0.47944856  0.6910013 ] 2\n",
      "[ 0.14608473  0.7243961   0.48152828 -0.3010409 ] 2\n",
      "[-0.05055102 -1.2831333  -0.05371927  0.5542445 ] 2\n",
      "[-0.10183635  0.24090391 -0.06576367 -0.62259036] 2\n",
      "[-0.24464574 -0.74281716 -0.43413615 -0.09205399] 2\n",
      "[ 0.42790163 -0.8751675  -0.44333196  0.07842936] 2\n",
      "[-0.36000612 -1.3501656  -0.3181651   0.03729425] 2\n",
      "[-0.00268505 -0.070402   -0.55663675 -0.35142744] 2\n",
      "[ 0.22046645  0.7471483   0.05496539 -0.59071743] 2\n",
      "[-0.2191827  -0.5507146  -0.15071328  0.38824743] 2\n",
      "[-0.14074622 -1.0166953  -0.48209894  0.0779542 ] 2\n",
      "[ 0.10054858 -0.43878657 -0.36555004  0.541335  ] 2\n",
      "[ 0.20083486  0.6397041   0.12359104 -0.68885267] 2\n",
      "[-0.02638138 -0.53165    -0.28998953  0.5278484 ] 2\n",
      "[ 0.18444782  0.33780408 -0.13021034 -0.3048371 ] 2\n",
      "[ 0.1662487  -0.07561401 -0.37127185  0.50484556] 2\n",
      "[ 0.1849422  -0.51260877 -0.13155067  0.6977382 ] 2\n",
      "[ 0.02625959 -0.6019549  -0.5443376   0.01629569] 2\n",
      "[-0.10606127 -0.43694106 -0.4544956   0.0925477 ] 2\n",
      "[-0.02750294  0.578326   -0.2522535  -0.40823966] 2\n",
      "[-0.23238365 -0.6331735   0.3095923   0.60311896] 2\n",
      "[ 0.37323445 -0.2925315   0.32886016  0.92868257] 2\n",
      "[ 0.10541337 -0.7144229  -0.352601    0.2586143 ] 2\n",
      "[-0.3971247  -0.83565044 -0.13422598  0.34975386] 2\n",
      "[ 0.0849627  -0.38620034 -0.15010002  0.57421774] 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08276757  0.40322903  0.04915206 -0.51972246] 2\n",
      "[-0.90814924 -0.9806684  -0.08757046  0.13031684] 2\n",
      "[ 0.23662598 -0.2917684  -0.36121696 -0.8179521 ] 2\n",
      "[ 0.06787337 -0.7849353   0.27668476  0.62794554] 2\n",
      "[-0.01147721  0.01033514 -0.54999596 -0.70806   ] 2\n",
      "[-0.22332428 -0.71679205 -0.34136587  0.45304012] 2\n",
      "[ 0.2132685  -0.68641853  0.02905499 -0.5314749 ] 2\n",
      "[-0.12626365 -0.56962913 -0.15532036  0.2058293 ] 2\n",
      "[ 0.87666094  0.8593758  -0.3983562  -0.47931367] 2\n",
      "[-0.36470813 -0.340728   -0.4178657  -0.59267765] 3\n",
      "[-0.05232818 -0.8578982  -0.9302488  -0.35284507] 3\n",
      "[ 0.40465346 -0.31241325 -0.15193711  0.34487772] 3\n",
      "[0.5446491  0.11296168 0.8495943  0.46318066] 3\n",
      "[-0.33643493  0.6204553  -0.27358547 -0.14254066] 3\n",
      "[-0.14221889  0.5271014  -0.02905449  0.13252376] 3\n",
      "[ 0.6975113  -0.05748967  0.14385308  0.11003192] 3\n",
      "[ 0.19550796  0.5721322   0.5123073  -0.18507224] 3\n",
      "[ 0.1706574  -0.00851092  0.62260944  0.40801603] 3\n",
      "[ 0.07551252  0.6190822   0.47617114 -0.59123325] 3\n",
      "[ 0.22825173 -0.46215895  0.03188664 -0.17202246] 3\n",
      "[-0.11292336  0.32433546 -0.34861118  0.04400696] 3\n",
      "[ 0.5585225   0.22297716 -0.04296806  0.05704981] 3\n",
      "[0.12148551 0.17450495 0.5860625  0.35672736] 3\n",
      "[-0.14505033  0.59292847 -0.731715   -0.6986527 ] 3\n",
      "[-0.3344905  -1.1873522  -0.32639143 -0.36521983] 3\n",
      "[ 0.16043933 -0.4453778  -0.6264984  -0.04287572] 3\n",
      "[ 0.50975376 -0.28068063  0.07938753  0.06827167] 3\n",
      "[-0.3265424  -0.47923353  0.32239407  0.23398666] 3\n",
      "[ 0.07600551 -1.0938689   0.05812977 -0.4665407 ] 3\n",
      "[-0.0625633  -0.10256684 -0.7390299  -0.52892613] 3\n",
      "[ 0.2959964   0.2867627   0.3506233  -0.01431303] 3\n",
      "[ 0.22244607 -0.7852144   0.32512718  0.0279388 ] 3\n",
      "[ 0.03678582 -0.33518195 -0.11688429  0.01417851] 3\n",
      "[ 0.13008346  0.6317394   0.07323064 -0.64628005] 3\n",
      "[ 0.29081377  0.594071    0.13273609 -0.60356855] 3\n",
      "[-0.09085383 -0.40260693 -0.39595574 -0.41098344] 3\n",
      "[ 0.586523   -0.1587083   0.60989934  0.58625937] 3\n",
      "[-0.48193255 -0.7514839  -0.83993036 -0.582654  ] 3\n",
      "[ 0.6233393   0.94542825  0.19090717 -0.13935345] 3\n",
      "[-0.19313501 -1.0828321  -0.2855289  -0.3795948 ] 3\n",
      "[ 0.26320145 -0.36167032  0.07303061  0.24209692] 3\n",
      "[-0.01116726  1.067272   -0.63530093 -0.5683893 ] 3\n",
      "[0.841951   0.31560186 0.05062412 0.02565499] 3\n",
      "[-0.11708289 -0.30565676 -0.21908541 -0.39623627] 3\n",
      "[-0.2596633  -0.49023965 -0.44074368 -0.34675452] 3\n",
      "[-0.34664872 -1.1447656  -0.35386962 -0.39590883] 3\n",
      "[-0.32207236 -0.4118818  -0.5645662  -0.6821214 ] 3\n",
      "[0.3898335  0.10538931 0.25404233 0.19710873] 3\n",
      "[-0.19141142 -0.530032   -0.18598229 -0.36005944] 3\n",
      "[0.5696708  0.05189331 0.09894748 0.03924498] 3\n",
      "[ 0.34956262  0.6688159   0.0805217  -0.47538823] 3\n",
      "[ 0.0195239   0.7868003  -0.45195192 -0.932556  ] 3\n",
      "[ 0.1159593  -0.49077642 -0.6215158  -0.760745  ] 3\n",
      "[ 0.10643086  0.25744158 -0.11261506 -0.5201573 ] 3\n",
      "[ 0.28596568 -0.7041443  -0.4021929  -0.08565401] 3\n",
      "[-0.03145603 -0.29446232  0.04422921 -0.24033363] 3\n",
      "[-0.03393119  0.06073255  0.0604388  -0.545856  ] 3\n",
      "[ 0.56655645  0.18204437 -0.16767128 -0.10999851] 3\n",
      "[ 0.15500739 -0.5754245  -0.20420334  0.05509332] 3\n",
      "[-0.39882466 -0.9477895  -0.38612682 -0.46821487] 3\n",
      "[-0.26232156 -0.99777323 -0.51081556 -0.43530995] 3\n",
      "[-0.14810377 -0.59756976 -0.11782309 -0.23200068] 3\n",
      "[-0.10021985  0.4717658   0.7629231  -0.01524942] 3\n",
      "Accuracy: 0.3375\n"
     ]
    }
   ],
   "source": [
    "# Print an Accuracy\n",
    "acc = 0\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint('logs')\n",
    "    if checkpoint:\n",
    "        saver.restore(sess, checkpoint)\n",
    "    for i in range(len(testlist)):\n",
    "        batch_data, batch_label = batch(testlist, 1)\n",
    "        logit = sess.run(output, feed_dict = {X:batch_data})\n",
    "        if np.argmax(logit[0]) == batch_label[0]:\n",
    "            acc += 1\n",
    "        else:\n",
    "            print(logit[0], batch_label[0])\n",
    "            \n",
    "    print(\"Accuracy:\", acc/len(testlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
